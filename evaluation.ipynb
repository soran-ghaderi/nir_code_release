{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa2af50060d9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from datasets import load_from_disk, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "import configs\n",
    "from experimental.controller.memory_manager import MemoryManager\n",
    "from generator.crv_generator import CRVGenerator\n",
    "from generator.text_generator import TextGenerator\n",
    "\n",
    "from utils import set_seed, logger\n",
    "from utils import extract_test_cases, extract_functions, extract_sections, add_parsed_functions_to_dataset\n",
    "\n",
    "from utils.loading_model import CustomTransformerLoader\n",
    "import torch\n",
    "\n",
    "# from rich import print\n",
    "from rich.console import Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696654b1-279c-46ec-ac35-841112a788ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and console\n",
    "console = Console()\n",
    "logger = logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55dd0d2-6866-42bb-b9dc-02ab954746f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "model_urls = {\n",
    "    \"llama31\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}\n",
    "model_path = model_urls[\"llama31\"]\n",
    "tokenizer_path = model_path\n",
    "hf_token = \"your token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de6a1f3-a3fb-40a1-9d97-ce339294e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Loading the Model</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[92m──────────────────────────────────────────────── \u001B[0m\u001B[1;31mLoading the Model\u001B[0m\u001B[92m ────────────────────────────────────────────────\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, use_auth_token=hf_token)\n",
    "\n",
    "console.rule(\"[bold red]Loading the Model\")\n",
    "\n",
    "loader = CustomTransformerLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5913df29-187a-4fc9-b839-5beb932b7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3220: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bdb658ec884a71b9f441de7f8e4751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":warning: model type:  <class 'moc_layers.LlamaForCausalLM'>\n",
      "config.hidden_size:  32\n",
      "config._attn_implementation:  eager\n"
     ]
    }
   ],
   "source": [
    "# mp.set_start_method('spawn')\n",
    "model, tokenizer = loader.load_model(\n",
    "    model_path=model_path, tokenizer_path=tokenizer_path, hf_token=hf_token\n",
    ")\n",
    "\n",
    "crv_layers = configs.CRV_LAYERS\n",
    "\n",
    "print(\":warning: model type: \", type(model))\n",
    "print(\"config.hidden_size: \", config.num_hidden_layers)\n",
    "print(\"config._attn_implementation: \", config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10f01675-9f5c-4d04-8e47-9269d3474364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLLaMACRVFramework:\n",
    "    def __init__(self, model, tokenizer, layer_idx = 10):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_generator = TextGenerator(model, tokenizer)\n",
    "        self.crv_generator = CRVGenerator(model, tokenizer, max_length=configs.MAX_LENGTH)\n",
    "        self.memory_manager = MemoryManager(model, max_memories=5)\n",
    "        self.layer_idx = layer_idx\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.text_generator = TextGenerator(model, tokenizer, device=self.device)\n",
    "\n",
    "    def generate_thought_trajectories(self, input_query, context=None, test_cases=None, max_new_tokens=1000, alt_text=None):\n",
    "        prompt_template = f\"\"\"\n",
    "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        Enable code_interpreter tool.<|eot_id|>\n",
    "        \\n\\n\n",
    "        {context if not context is None else alt_text}\n",
    "        \n",
    "        \\n\\n{input_query}.\n",
    "        \n",
    "        \\n\\nYour outputs must follow this structure and make sure you open and close the tags accurately:\n",
    "\n",
    "        Identify the core components of this problem.\n",
    "        1. Identify potential edge cases and tricky parts.\n",
    "        2. Write 2 short test cases for the edge cases and tricky parts.\n",
    "        \n",
    "        <chain_of_thoughts>\n",
    "        1. you must consider the edge cases according to the problem statement.\n",
    "        2. Begin with a <thinking> section.\n",
    "        3. Inside the thinking section:\n",
    "           a. Write the topic name of the query, the name of the algorithm if necessary.\n",
    "           b. Draft an answer as an expert.\n",
    "           b. Briefly analyze the question and outline your approach.\n",
    "           c. Present a clear plan of steps to solve the problem.\n",
    "           d. Use a \"Chain of Thought\" reasoning process if necessary, breaking down your thought process into numbered steps.\n",
    "        4. Include a <reflection> section for each idea where you:\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        5. Be sure to close all reflection sections.\n",
    "        6. Close the thinking section with </thinking>.\n",
    "        7. Provide your final answer in an <output> section.        \n",
    "        </chain_of_thoughts>\n",
    "\n",
    "        <chain_of_thought_selection>\n",
    "        you must consider the edge cases according to the problem statement and select the most promising chain of thought that solves the edge cases (not necessarily the simplest nor the standard approach).\n",
    "        </chain_of_thought_selection>\n",
    "\n",
    "        <solution>\n",
    "        1. As a Python expert, generate the Python code and make sure it solves the edge cases while keeping it efficient.\n",
    "        2. the internal steps must produce the required output.\n",
    "        </solution>\n",
    "\n",
    "        Include a <reflection> section for the selected solution where if it is not correct, modify or if necessary, rewrite the solution and pay attention to the input problem.\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights according to the problem. you must consider the edge cases according to the problem. Make sure it is not overcomplicated.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        4. Be sure to close all reflection sections.\n",
    "        \n",
    "        <context_generation>\n",
    "        1. Rewrite the problem.\n",
    "        2. Rewrite the edge cases and tricky parts in one short sentence\n",
    "        2. Generate a very accurate and minimal Python code/pseudocode for the final solution. Ensure that the final solution is minimal and accurate.\n",
    "        </context_generation>\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"\"\"\n",
    "\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            prompt_template,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        # print(\"generated_thought trajectory: \", generated_text)\n",
    "\n",
    "        return generated_text\n",
    "    \n",
    "    def extract_hidden_states(self, context):\n",
    "        best_crv, seq_length = self.crv_generator.generate_crvs(\n",
    "            context, crv_layers=crv_layers, max_length=configs.MAX_LENGTH\n",
    "        )\n",
    "        return best_crv, seq_length  # Return the hidden state and its len\n",
    "\n",
    "    def generate_crv(self, hidden_states, seq_length):\n",
    "        # return torch.mean(hidden_states, dim=1)\n",
    "        return hidden_states, seq_length\n",
    "        \n",
    "    def final_generation(self, original_query, test_cases, crv, seq_length, max_new_tokens=250):\n",
    "\n",
    "        query=f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\n{original_query}.\\nYour code should pass the following tests:{test_cases}\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\"\"\"\n",
    "        # Combine original query and CRV\n",
    "        self.memory_manager.add_memory(\n",
    "        crv, seq_length, layer_idx=self.layer_idx, crv_layers=crv_layers\n",
    "    )\n",
    "\n",
    "        # model.model.set_post_concat_crv(True)\n",
    "        self.memory_manager.set_concat_positions(0, start_pos=0, end_pos=seq_length)\n",
    "        if isinstance(self.layer_idx, int):\n",
    "            self.memory_manager.apply_memory_to_model(0)\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            query,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        return generated_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f10ce6-8cea-4da8-b689-8d68acffe825",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_text = '''<|start_header_id|>user<|end_header_id|>You are an expert Python programmer designed to provide standard, accurate,and fully working codes, and here is your task:\\n\n",
    "        \\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n res = tuple(set(test_tup1) & set(test_tup2))\\n return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n result = False\\n for i in range(2,int(math.sqrt(n)) + 1):\\n if n % i == 0:\\n result = True\\n return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n largest_nums = hq.nlargest(n, nums)\\n return largest_nums\\n```<|eot_id|>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2060ece-69fc-42e4-bd65-9c708f7e2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def move_to_cpu(model):\n",
    "    model = model.cpu()\n",
    "    clear_gpu_memory()\n",
    "    return model\n",
    "\n",
    "def move_to_gpu(model):\n",
    "    if torch.cuda.is_available():\n",
    "        return model.cuda()\n",
    "    return model\n",
    "\n",
    "def check_memory(threshold=0.8):\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated()\n",
    "        memory_reserved = torch.cuda.memory_reserved()\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory\n",
    "        # print(memory_allocated,memory_reserved,memory_total)\n",
    "        memory_usage = (memory_allocated + memory_reserved) / memory_total\n",
    "        \n",
    "        if memory_usage > threshold:\n",
    "            print(memory_allocated,memory_reserved,memory_total, memory_usage)\n",
    "        return True \n",
    "    return False\n",
    "check_memory()\n",
    "clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfb0ab3b-b2a3-4653-b87c-4d130362d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def save_checkpoint(layer_idx: int, instance_index: int, results: List[Dict], checkpoint_dir: str = \"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_layer_{layer_idx}_instance_{instance_index}.json\")\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        \"layer_idx\": layer_idx,\n",
    "        \"instance_index\": instance_index,\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    with open(checkpoint_path, \"w\") as f:\n",
    "        json.dump(checkpoint_data, f)\n",
    "    \n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_dir: str = \"checkpoints\") -> Dict:\n",
    "\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "\n",
    "        checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_\")])\n",
    "        \n",
    "        if not checkpoint_files:\n",
    "            return None\n",
    "        \n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "        \n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        \n",
    "        checkpoint_data[\"results\"] = defaultdict(list, checkpoint_data[\"results\"])\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "        return checkpoint_data\n",
    "    else: return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba3ff791-3d11-4830-b45b-9fd96cea0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import traceback\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset: Dataset, layer_indices: List[int], checkpoint_dir: str = \"checkpoints\", max_retries: int = 3) -> pd.DataFrame:\n",
    "    pr_flag = False\n",
    "    checkpoint = load_checkpoint(checkpoint_dir)\n",
    "    max_instances = 0\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    if checkpoint:\n",
    "        start_layer_idx = checkpoint[\"layer_idx\"]\n",
    "        start_instance_index = checkpoint[\"instance_index\"] + 1\n",
    "        results = defaultdict(list, checkpoint[\"results\"])\n",
    "        max_instances = len(results['instance_id'])\n",
    "    else:\n",
    "        start_layer_idx = layer_indices[0]\n",
    "        start_instance_index = 0\n",
    "        results = defaultdict(list)\n",
    "    print(\"layer_indices: \", layer_indices)\n",
    "    for layer_idx in tqdm(layer_indices[layer_indices.index(start_layer_idx):], desc=\"Processing layer indices\"):\n",
    "        framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx=layer_idx)\n",
    "        \n",
    "        if start_instance_index >= dataset_size:\n",
    "            print(f\"All instances processed for layer {layer_idx}. Moving to next layer.\")\n",
    "            start_instance_index = 0\n",
    "            continue\n",
    "        \n",
    "        for i, instance in enumerate(tqdm(dataset.skip(start_instance_index), desc=f\"Processing instances for layer {layer_idx}\", total=dataset_size-start_instance_index)):\n",
    "            if not pr_flag:\n",
    "                print(f\"start_layer_idx: {start_layer_idx}\\nstart_instance_index: {start_instance_index}\\ni: {i}\\nlayer_idx: {layer_idx}\")\n",
    "            try:    \n",
    "                if (start_instance_index + i + 1) % 5 == 0:\n",
    "                    save_checkpoint(layer_idx, start_instance_index + i, dict(results), checkpoint_dir)\n",
    "                \n",
    "                if i % 20 == 0:\n",
    "                    clear_gpu_memory()\n",
    "                    \n",
    "                if check_memory():\n",
    "                    clear_gpu_memory()\n",
    "                \n",
    "                query = instance['query'][0] if instance['query'] else ''\n",
    "                context = instance['context'][0] if instance['context'] else '' \n",
    "                test_cases = '\\n'.join(extract_test_cases(instance['input_final_prompts'][0])[-1])\n",
    "                tmp = \"Proposed solution context: \"\n",
    "\n",
    "                for attempt in range(max_retries):\n",
    "                    try:\n",
    "                        trajectories_and_context = framework.generate_thought_trajectories(query, context, test_cases, max_new_tokens=1000, alt_text=alt_text)\n",
    "                        context_expansion = extract_sections(trajectories_and_context, \"context_generation\")\n",
    "                        context_expansion = f\"Proposed solution context: {context_expansion}{extract_sections(trajectories_and_context, 'solution')}\"\n",
    "                        \n",
    "                        hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "                        crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "                        \n",
    "                        final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "                        extracted_functions = extract_functions(final_output)\n",
    "                        \n",
    "                        # generation_successful = True\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Generation failed for layer {layer_idx}, instance {i}, attempt {attempt + 1}: {str(e)}\\nquery: {query}\")\n",
    "                        if attempt == max_retries - 1:\n",
    "                            logger.error(f\"All retries failed for layer {layer_idx}, instance {i}. Error: {traceback.format_exc()}\")\n",
    "            \n",
    "\n",
    "                # trajectories_and_context = framework.generate_thought_trajectories(query, context, test_cases, max_new_tokens=1000, alt_text=alt_text)\n",
    "                # context_expansion = extract_sections(trajectories_and_context, \"context_generation\")\n",
    "                # context_expansion = tmp + context_expansion + extract_sections(trajectories_and_context, \"solution\")\n",
    "                \n",
    "                # hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "                # crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "                \n",
    "                # final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "                # extracted_functions = extract_functions(final_output)\n",
    "                \n",
    "                current_instance = i + start_instance_index\n",
    "                if current_instance >= max_instances:\n",
    "                    results['instance_id'].append(current_instance)\n",
    "                    results['query'].append(query)\n",
    "                    results['context'].append(context)\n",
    "                    results['test_cases'].append(test_cases)\n",
    "                    max_instances = current_instance + 1\n",
    "\n",
    "                results[f'final_output_{layer_idx}'].append(final_output)\n",
    "                results[f'extracted_functions_{layer_idx}'].append(extracted_functions)\n",
    "                results[f'trajectories_and_context_{layer_idx}'].append(trajectories_and_context)\n",
    "                results[f'context_expansion_{layer_idx}'].append(context_expansion)\n",
    "\n",
    "                if not pr_flag:\n",
    "                    print(f\"results dict for instance i: {results}\")\n",
    "                    pr_flag = True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {start_instance_index + i}: {str(e)}\")\n",
    "                save_checkpoint(layer_idx, start_instance_index + i, dict(results), checkpoint_dir)\n",
    "                raise\n",
    "\n",
    "        start_instance_index = 0\n",
    "\n",
    "    # Ensure all arrays have the same length\n",
    "    max_length = max(len(v) for v in results.values())\n",
    "    for key in results:\n",
    "        results[key] = results[key] + [None] * (max_length - len(results[key]))\n",
    "\n",
    "    df = pd.DataFrame(dict(results))  # Convert defaultdict to regular dict\n",
    "    if 'instance_id' in df.columns:\n",
    "        df.set_index('instance_id', inplace=True)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    # df = pd.DataFrame(results)\n",
    "    # df.set_index('instance_id', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86716f98-f9ae-4a0d-bfe6-90aa7e6acebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from checkpoints/checkpoint_layer_1_instance_9.json\n",
      "Resuming from example 9\n",
      "Loaded checkpoint from checkpoints/checkpoint_layer_1_instance_9.json\n",
      "layer_indices:  [1, 10, 23, 'orig']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d469a36e70d49feb9c57c4811b92931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing layer indices:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5003d3d54e41b1a647be273fe17ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing instances for layer 1:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_layer_idx: 1\n",
      "start_instance_index: 10\n",
      "i: 0\n",
      "layer_idx: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "ERROR:__main__:All retries failed for layer 1, instance 0. Error: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_303594/4165325061.py\", line 240, in evaluate_model\n",
      "    final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_303594/3985563958.py\", line 102, in final_generation\n",
      "    generated_text = self.text_generator.generate_text(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/generator/text_generator.py\", line 140, in generate_text\n",
      "    outputs = self.model.generate(**generate_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2024, in generate\n",
      "    result = self._sample(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2982, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/accelerate/hooks.py\", line 169, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/moc_layers.py\", line 1777, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/moc_layers.py\", line 1495, in forward\n",
      "    post_cat_attention_mask = self.create_post_cat_attention_mask(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sg23454/PycharmProjects/moc/moc_layers.py\", line 1591, in create_post_cat_attention_mask\n",
      "    device = original_mask.device\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'device'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example 10: cannot access local variable 'final_output' where it is not associated with a value\n",
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_10.json\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'final_output' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 24\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResuming from example \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Evaluate model\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# new_results_df = evaluate_model(model, tokenizer, loaded_dataset, layer_indices)\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m new_results_df \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloaded_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_indices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_results_df info:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(new_results_df\u001B[38;5;241m.\u001B[39minfo())\n",
      "Cell \u001B[0;32mIn[10], line 269\u001B[0m, in \u001B[0;36mevaluate_model\u001B[0;34m(model, tokenizer, dataset, layer_indices, checkpoint_dir, max_retries)\u001B[0m\n\u001B[1;32m    266\u001B[0m     results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_cases\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(test_cases)\n\u001B[1;32m    267\u001B[0m     max_instances \u001B[38;5;241m=\u001B[39m current_instance \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 269\u001B[0m results[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinal_output_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(\u001B[43mfinal_output\u001B[49m)\n\u001B[1;32m    270\u001B[0m results[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mextracted_functions_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(extracted_functions)\n\u001B[1;32m    271\u001B[0m results[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrajectories_and_context_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(trajectories_and_context)\n",
      "\u001B[0;31mUnboundLocalError\u001B[0m: cannot access local variable 'final_output' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "subset_name = \"processed_Meta-Llama-3.1-8B-Instruct-evals__mbpp__details\"\n",
    "loaded_dataset = load_from_disk(f\"data/{subset_name}\")\n",
    "\n",
    "num_examples=250\n",
    "if num_examples is not None:\n",
    "    loaded_dataset = loaded_dataset.select(range(num_examples))\n",
    "\n",
    "# Define layer indices to evaluate\n",
    "layer_indices = [1, 10, 23, 'orig']\n",
    "# layer_indices = [32, 'orig']\n",
    " \n",
    "checkpoint = load_checkpoint()\n",
    "start_index = 0\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "if checkpoint:\n",
    "    start_index = checkpoint[\"instance_index\"]\n",
    "    results_df = checkpoint[\"results\"]\n",
    "    print(f\"Resuming from example {start_index}\")\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "# new_results_df = evaluate_model(model, tokenizer, loaded_dataset, layer_indices)\n",
    "new_results_df = evaluate_model(model, tokenizer, loaded_dataset, layer_indices)\n",
    "\n",
    "print(\"new_results_df info:\")\n",
    "print(new_results_df.info())\n",
    "print(\"\\nnew_results_df shape:\", new_results_df.shape)\n",
    "print(\"\\nnew_results_df columns:\", new_results_df.columns)\n",
    "print(\"\\nnew_results_df head:\")\n",
    "print(new_results_df.head())\n",
    "\n",
    "# Combine previous results with new results\n",
    "results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "# Add parsed functions to the dataset\n",
    "updated_dataset = add_parsed_functions_to_dataset(loaded_dataset, results_df, layer_indices)\n",
    "\n",
    "print(f\"Type of updated_dataset: {type(updated_dataset)}\")\n",
    "print(f\"Number of rows in updated_dataset: {len(updated_dataset)}\")\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_dataset.save_to_disk(f\"data/{subset_name}_results\")\n",
    "\n",
    "# os.remove(os.path.join(\"checkpoints\", \"checkpoint.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a97c8d5-83bc-4538-9033-a16db8b03482",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128aecd5-f94d-4fb9-917f-1fe901f028c9",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60e384e-9f0d-46a9-b3da-17e344ff6202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 2 rows of the dataset:\n",
      "Row 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRow \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m \u001B[43mexample\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m():\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m()  \u001B[38;5;66;03m# Add a blank line between rows\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 2 rows of the dataset:\")\n",
    "for i, example in enumerate(results_df):\n",
    "    if i < 2:\n",
    "        print(f\"Row {i + 1}:\")\n",
    "        for key, value in example.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()  # Add a blank line between rows\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "552b68bbbe5973fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc304549-af46-4f55-8b9f-20b6f63edf1d",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ee130-7498-4889-a337-2f6d119fc84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
