{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb713faa-acd6-4baf-8fe2-75c3a4c7c1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa2af50060d9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from datasets import load_from_disk, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import configs\n",
    "from controller.memory_manager import MemoryManager\n",
    "from data_processor.data_loader import GSM8KDataset\n",
    "from generator.crv_generator import CRVGenerator\n",
    "from generator.text_generator import TextGenerator\n",
    "\n",
    "from retrieve.cosine_similarity import CRVRetriever\n",
    "from retrieve.dnc import DNMemory\n",
    "from utils import set_seed, logger\n",
    "from utils import extract_test_cases, extract_functions, extract_sections, add_parsed_functions_to_dataset\n",
    "\n",
    "from utils.loading_model import CustomTransformerLoader\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "# from rich import print\n",
    "from rich.console import Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "696654b1-279c-46ec-ac35-841112a788ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and console\n",
    "console = Console()\n",
    "logger = logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b55dd0d2-6866-42bb-b9dc-02ab954746f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "model_urls = {\n",
    "    \"llama31\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}\n",
    "model_path = model_urls[\"llama31\"]\n",
    "tokenizer_path = model_path\n",
    "hf_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de6a1f3-a3fb-40a1-9d97-ce339294e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Loading the Model</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────────────────────────────────────── \u001b[0m\u001b[1;31mLoading the Model\u001b[0m\u001b[92m ────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, use_auth_token=hf_token)\n",
    "\n",
    "console.rule(\"[bold red]Loading the Model\")\n",
    "\n",
    "loader = CustomTransformerLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5913df29-187a-4fc9-b839-5beb932b7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3220: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9547739f96ff4669a4519156cfc18074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":warning: model type:  <class 'moc_layers.LlamaForCausalLM'>\n",
      "config.hidden_size:  32\n",
      "config._attn_implementation:  eager\n"
     ]
    }
   ],
   "source": [
    "# mp.set_start_method('spawn')\n",
    "model, tokenizer = loader.load_model(\n",
    "    model_path=model_path, tokenizer_path=tokenizer_path, hf_token=hf_token\n",
    ")\n",
    "\n",
    "crv_layers = configs.CRV_LAYERS\n",
    "\n",
    "print(\":warning: model type: \", type(model))\n",
    "print(\"config.hidden_size: \", config.num_hidden_layers)\n",
    "print(\"config._attn_implementation: \", config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10f01675-9f5c-4d04-8e47-9269d3474364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLLaMACRVFramework:\n",
    "    def __init__(self, model, tokenizer, layer_idx = 10):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_generator = TextGenerator(model, tokenizer)\n",
    "        self.crv_generator = CRVGenerator(model, tokenizer, max_length=configs.MAX_LENGTH)\n",
    "        self.memory_manager = MemoryManager(model, max_memories=5)\n",
    "        self.layer_idx = layer_idx\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.text_generator = TextGenerator(model, tokenizer, device=self.device)\n",
    "\n",
    "    def generate_thought_trajectories(self, input_query, context=None, test_cases=None, max_new_tokens=1000, alt_text=None):\n",
    "        prompt_template = f\"\"\"\n",
    "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        Enable code_interpreter tool.<|eot_id|>\n",
    "        \\n\\n\n",
    "        {context if not context is None else alt_text}\n",
    "        \n",
    "        \\n\\n{input_query}.\n",
    "        \n",
    "        \\n\\nYour outputs must follow this structure and make sure you open and close the tags accurately:\n",
    "\n",
    "        Identify the core components of this problem.\n",
    "        1. Identify potential edge cases and tricky parts.\n",
    "        2. Write 2 short test cases for the edge cases and tricky parts.\n",
    "        \n",
    "        <chain_of_thoughts>\n",
    "        1. you must consider the edge cases according to the problem statement.\n",
    "        2. Begin with a <thinking> section.\n",
    "        3. Inside the thinking section:\n",
    "           a. Write the topic name of the query, the name of the algorithm if necessary.\n",
    "           b. Draft an answer as an expert.\n",
    "           b. Briefly analyze the question and outline your approach.\n",
    "           c. Present a clear plan of steps to solve the problem.\n",
    "           d. Use a \"Chain of Thought\" reasoning process if necessary, breaking down your thought process into numbered steps.\n",
    "        4. Include a <reflection> section for each idea where you:\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        5. Be sure to close all reflection sections.\n",
    "        6. Close the thinking section with </thinking>.\n",
    "        7. Provide your final answer in an <output> section.        \n",
    "        </chain_of_thoughts>\n",
    "\n",
    "        <chain_of_thought_selection>\n",
    "        you must consider the edge cases according to the problem statement and select the most promising chain of thought that solves the edge cases (not necessarily the simplest nor the standard approach).\n",
    "        </chain_of_thought_selection>\n",
    "\n",
    "        <solution>\n",
    "        1. As a Python expert, generate the Python code and make sure it solves the edge cases while keeping it efficient.\n",
    "        2. the internal steps must produce the required output.\n",
    "        </solution>\n",
    "\n",
    "        Include a <reflection> section for the selected solution where if it is not correct, modify or if necessary, rewrite the solution and pay attention to the input problem.\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights according to the problem. you must consider the edge cases according to the problem. Make sure it is not overcomplicated.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        4. Be sure to close all reflection sections.\n",
    "        \n",
    "        <context_generation>\n",
    "        1. Rewrite the problem.\n",
    "        2. Rewrite the edge cases and tricky parts in one short sentence\n",
    "        2. Generate a very accurate and minimal Python code/pseudocode for the final solution. Ensure that the final solution is minimal and accurate.\n",
    "        </context_generation>\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"\"\"\n",
    "\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            prompt_template,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        # print(\"generated_thought trajectory: \", generated_text)\n",
    "\n",
    "        return generated_text\n",
    "    \n",
    "    def extract_hidden_states(self, context):\n",
    "        best_crv, seq_length = self.crv_generator.generate_crvs(\n",
    "            context, crv_layers=crv_layers, max_length=configs.MAX_LENGTH\n",
    "        )\n",
    "        return best_crv, seq_length  # Return the hidden state and its len\n",
    "\n",
    "    def generate_crv(self, hidden_states, seq_length):\n",
    "        # return torch.mean(hidden_states, dim=1)\n",
    "        return hidden_states, seq_length\n",
    "        \n",
    "    def final_generation(self, original_query, test_cases, crv, seq_length, max_new_tokens=250):\n",
    "\n",
    "        query=f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\n{original_query}.\\nYour code should pass the following tests:{test_cases}\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\"\"\"\n",
    "        # Combine original query and CRV\n",
    "        self.memory_manager.add_memory(\n",
    "        crv, seq_length, layer_idx=self.layer_idx, crv_layers=crv_layers\n",
    "    )\n",
    "\n",
    "        # model.model.set_post_concat_crv(True)\n",
    "        self.memory_manager.set_concat_positions(0, start_pos=0, end_pos=seq_length)\n",
    "        if isinstance(self.layer_idx, int):\n",
    "            self.memory_manager.apply_memory_to_model(0)\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            query,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        return generated_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64f10ce6-8cea-4da8-b689-8d68acffe825",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_text = '''<|start_header_id|>user<|end_header_id|>You are an expert Python programmer designed to provide standard, accurate,and fully working codes, and here is your task:\\n\n",
    "        \\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n res = tuple(set(test_tup1) & set(test_tup2))\\n return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n result = False\\n for i in range(2,int(math.sqrt(n)) + 1):\\n if n % i == 0:\\n result = True\\n return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n largest_nums = hq.nlargest(n, nums)\\n return largest_nums\\n```<|eot_id|>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba3ff791-3d11-4830-b45b-9fd96cea0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset: Dataset, layer_indices: List[int], num_examples: int = -1) -> pd.DataFrame:\n",
    "    results = []\n",
    "    \n",
    "    for layer_idx in tqdm(layer_indices, desc=\"Processing layer indices\"):\n",
    "        framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx=layer_idx)\n",
    "        \n",
    "        for i, instance in enumerate(tqdm(dataset, desc=f\"Processing instances for layer {layer_idx}\")):\n",
    "            # print('instance: ', instance)\n",
    "            query = instance['query'][0] if instance['query'] else ''\n",
    "            context = instance['context'][0] if instance['context'] else '' \n",
    "            test_cases = '\\n'.join(extract_test_cases(instance['input_final_prompts'][0])[-1])\n",
    "            tmp = \"Proposed solution context: \"\n",
    "            trajectories_and_context = framework.generate_thought_trajectories(query, context, test_cases, max_new_tokens=1000, alt_text=alt_text)\n",
    "            context_expansion = extract_sections(trajectories_and_context, \"context_generation\")\n",
    "            context_expansion = tmp + context_expansion + extract_sections(trajectories_and_context, \"solution\")\n",
    "            # print('context_expansion: ', context_expansion)\n",
    "            # print(\"\\end of context ----\")\n",
    "            hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "            crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "            \n",
    "            final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "            # print(\"final output: \", final_output)\n",
    "            extracted_functions = extract_functions(final_output)\n",
    "            \n",
    "            result = {\n",
    "                'layer_idx': layer_idx,\n",
    "                'instance_id': i,  # This should be the index in the dataset\n",
    "                'query': query,\n",
    "                'context': context,\n",
    "                'test_cases': test_cases,\n",
    "                'final_output': final_output,\n",
    "                'extracted_functions': extracted_functions\n",
    "            }\n",
    "            # print(f'result {i}: ', result)\n",
    "            results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86716f98-f9ae-4a0d-bfe6-90aa7e6acebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f54c6a337c2428f9ca9bf3935089fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing layer indices:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed04cbc1c654b3599cdcae84c4c811c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing instances for layer 32:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb8cd0bbf494c4a9d04c57905193655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing instances for layer orig:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of updated_dataset: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Number of rows in updated_dataset: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fa3e2de8ba433894bc193a4eb3a1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subset_name = \"processed_Meta-Llama-3.1-8B-Instruct-evals__mbpp__details\"\n",
    "loaded_dataset = load_from_disk(f\"data/{subset_name}\")\n",
    "\n",
    "num_examples=None\n",
    "if not num_examples is None:\n",
    "    loaded_dataset.select(range(num_examples))\n",
    "\n",
    "# Define layer indices to evaluate\n",
    "layer_indices = [1, 10, 15, 20, 32, 'orig']\n",
    "# layer_indices = [32, 'orig']\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "results_df = evaluate_model(model, tokenizer, loaded_dataset, layer_indices, num_examples=num_examples)\n",
    "\n",
    "# Add parsed functions to the dataset\n",
    "updated_dataset = add_parsed_functions_to_dataset(loaded_dataset, results_df, layer_indices)\n",
    "\n",
    "print(f\"Type of updated_dataset: {type(updated_dataset)}\")\n",
    "print(f\"Number of rows in updated_dataset: {len(updated_dataset)}\")\n",
    "\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_dataset.save_to_disk(f\"data/{subset_name}_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c60e384e-9f0d-46a9-b3da-17e344ff6202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 2 rows of the dataset:\n",
      "Row 1:\n",
      "  task_type: Generative\n",
      "  task_name: mbpp_chat\n",
      "  subtask_name: None\n",
      "  input_question: Write a function to check if given tuple is distinct or not.\n",
      "  input_choice_list: None\n",
      "  input_final_prompts: ['<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to check if given tuple is distinct or not.\\nYour code should pass the following tests:\\nassert check_distinct((1, 4, 5, 6, 1, 4)) == False\\nassert check_distinct((1, 4, 5, 6)) == True\\nassert check_distinct((2, 3, 4, 5, 6)) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python']\n",
      "  input_correct_responses: None\n",
      "  output_prediction_text: ['\\ndef check_distinct(tup):\\n  return len(tup) == len(set(tup))\\n```']\n",
      "  output_parsed_answer: \n",
      "def check_distinct(tup):\n",
      "  return len(tup) == len(set(tup))\n",
      "\n",
      "  output_choice_completions: None\n",
      "  output_choice_negative_log_likelihoods: None\n",
      "  output_metrics: {'compiles@1': 1.0, 'pass@1': 1.0}\n",
      "  is_correct: True\n",
      "  input_question_hash: 5f95e606207366d0b090735637ee25085e3a99edfc4272637bfa3816b4743eb3\n",
      "  input_final_prompts_hash: ['9c7412dfa641dda248586db1638287547113760c3daa78ca2bc27cebfb3535be']\n",
      "  benchmark_label: MBPP\n",
      "  eval_config: {'max_gen_len': '256', 'max_prompt_len': '4500', 'num_few_shot': '3', 'num_generations': '1', 'prompt_fn': \"functools.partial(<function jinja_dialog_format at 0x7faafad1c790>, template={'prompt': 'You are an expert Python programmer, and here is your task:\\\\n{{ text }}\\\\nYour code should pass the following tests:\\\\n{{ prompt_tests }}', 'answer': '\\\\n```python\\\\n{{ code }}\\\\n```\\\\n', 'gen_prefix': '\\\\n```python\\\\n'}, append_gen_prefix=True)\", 'return_logprobs': 'false', 'seed': '42', 'temperature': '0.0', 'top_k': '0', 'top_p': '0'}\n",
      "  context: ['<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>']\n",
      "  query: ['<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to check if given tuple is distinct or not.\\nYour code should pass the following tests:\\nassert check_distinct((1, 4, 5, 6, 1, 4)) == False\\nassert check_distinct((1, 4, 5, 6)) == True\\nassert check_distinct((2, 3, 4, 5, 6)) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python']\n",
      "  final_output_layer_32: \n",
      "def check_distinct(tup):\n",
      "    \"\"\"\n",
      "    This function checks if all elements in a given tuple are distinct.\n",
      "    \n",
      "    Args:\n",
      "    tup (tuple): The input tuple to be checked.\n",
      "    \n",
      "    Returns:\n",
      "    bool: True if all elements in the tuple are distinct, False otherwise.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Convert the tuple to a set, which automatically removes duplicates\n",
      "    # If the lengths of the tuple and the set are equal, then all elements were distinct\n",
      "    return len(tup) == len(set(tup))\n",
      "```\n",
      "  final_output_layer_orig: \n",
      "def check_distinct(tup):\n",
      "    \"\"\"\n",
      "    This function checks if all elements in a given tuple are distinct.\n",
      "    \n",
      "    Args:\n",
      "    tup (tuple): The input tuple to be checked.\n",
      "    \n",
      "    Returns:\n",
      "    bool: True if all elements in the tuple are distinct, False otherwise.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Convert the tuple to a set, which automatically removes duplicates\n",
      "    # If the lengths of the tuple and the set are equal, then all elements were distinct\n",
      "    return len(tup) == len(set(tup))\n",
      "```\n",
      "  extracted_functions_layer_32: def check_distinct(tup):\n",
      "    return len(tup) == len(set(tup))\n",
      "```\n",
      "  extracted_functions_layer_orig: def check_distinct(tup):\n",
      "    return len(tup) == len(set(tup))\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 2 rows of the dataset:\")\n",
    "for i, example in enumerate(updated_dataset):\n",
    "    if i < 2:\n",
    "        print(f\"Row {i + 1}:\")\n",
    "        for key, value in example.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()  # Add a blank line between rows\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc304549-af46-4f55-8b9f-20b6f63edf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():    \n",
    "    # Print column names\n",
    "    print(\"\\nColumns in the updated dataset:\")\n",
    "    print(updated_dataset.column_names)\n",
    "    # loaded_dataset2 = load_from_disk(\"data/processed_meta_llama_dataset_with_results\")\n",
    "    loaded_dataset2 = load_dataset(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct-evals\",\n",
    "    name=\"Meta-Llama-3.1-8B-Instruct-evals__mbpp__details\",\n",
    "    split=\"latest\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Print selected columns from the first 5 rows of the updated dataset\n",
    "    print(\"First 5 rows of the updated dataset (selected columns):\")\n",
    "    for i, example in enumerate(loaded_dataset2.select(range(num_examples))):\n",
    "        print(f\"\\nExample {i + 1}:\")\n",
    "        # Print original columns\n",
    "        print(f\"input_correct_responses: {example['input_correct_responses']}...\")\n",
    "        # print(f\"extracted_functions_layer_15: {example['extracted_functions_layer_15']}...\")\n",
    "        \n",
    "        # Print new columns for each layer\n",
    "        for layer in layer_indices:\n",
    "            print(f\"\\nLayer {layer}:\")\n",
    "            final_output = example.get(f'final_output_layer_{layer}')\n",
    "            extracted_functions = example.get(f'extracted_functions_layer_{layer}')\n",
    "            \n",
    "            if final_output:\n",
    "                print(f\"Final Output: {final_output[:100]}...\")\n",
    "            else:\n",
    "                print(\"Final Output: None\")\n",
    "            \n",
    "            if extracted_functions:\n",
    "                print(f\"Extracted Functions: {extracted_functions[:100]}...\")\n",
    "            else:\n",
    "                print(\"Extracted Functions: None\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ee130-7498-4889-a337-2f6d119fc84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
