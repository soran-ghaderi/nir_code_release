{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa2af50060d9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from datasets import load_from_disk, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import configs\n",
    "from controller.memory_manager import MemoryManager\n",
    "from data_processor.data_loader import GSM8KDataset\n",
    "from generator.crv_generator import CRVGenerator\n",
    "from generator.text_generator import TextGenerator\n",
    "\n",
    "from retrieve.cosine_similarity import CRVRetriever\n",
    "from retrieve.dnc import DNMemory\n",
    "from utils import set_seed, logger\n",
    "from utils import extract_test_cases, extract_functions, extract_sections, add_parsed_functions_to_dataset\n",
    "\n",
    "from utils.loading_model import CustomTransformerLoader\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "# from rich import print\n",
    "from rich.console import Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696654b1-279c-46ec-ac35-841112a788ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and console\n",
    "console = Console()\n",
    "logger = logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55dd0d2-6866-42bb-b9dc-02ab954746f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "model_urls = {\n",
    "    \"llama31\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}\n",
    "model_path = model_urls[\"llama31\"]\n",
    "tokenizer_path = model_path\n",
    "hf_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de6a1f3-a3fb-40a1-9d97-ce339294e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Loading the Model</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────────────────────────────────────── \u001b[0m\u001b[1;31mLoading the Model\u001b[0m\u001b[92m ────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, use_auth_token=hf_token)\n",
    "\n",
    "console.rule(\"[bold red]Loading the Model\")\n",
    "\n",
    "loader = CustomTransformerLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5913df29-187a-4fc9-b839-5beb932b7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3220: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53776ede97234a1382e45685d0813d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":warning: model type:  <class 'moc_layers.LlamaForCausalLM'>\n",
      "config.hidden_size:  32\n",
      "config._attn_implementation:  eager\n"
     ]
    }
   ],
   "source": [
    "# mp.set_start_method('spawn')\n",
    "model, tokenizer = loader.load_model(\n",
    "    model_path=model_path, tokenizer_path=tokenizer_path, hf_token=hf_token\n",
    ")\n",
    "\n",
    "crv_layers = configs.CRV_LAYERS\n",
    "\n",
    "print(\":warning: model type: \", type(model))\n",
    "print(\"config.hidden_size: \", config.num_hidden_layers)\n",
    "print(\"config._attn_implementation: \", config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10f01675-9f5c-4d04-8e47-9269d3474364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLLaMACRVFramework:\n",
    "    def __init__(self, model, tokenizer, layer_idx = 10):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_generator = TextGenerator(model, tokenizer)\n",
    "        self.crv_generator = CRVGenerator(model, tokenizer, max_length=configs.MAX_LENGTH)\n",
    "        self.memory_manager = MemoryManager(model, max_memories=5)\n",
    "        self.layer_idx = layer_idx\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.text_generator = TextGenerator(model, tokenizer, device=self.device)\n",
    "\n",
    "    def generate_thought_trajectories(self, input_query, context=None, test_cases=None, max_new_tokens=1000, alt_text=None):\n",
    "        prompt_template = f\"\"\"\n",
    "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        Enable code_interpreter tool.<|eot_id|>\n",
    "        \\n\\n\n",
    "        {context if not context is None else alt_text}\n",
    "        \n",
    "        \\n\\n{input_query}.\n",
    "        \n",
    "        \\n\\nYour outputs must follow this structure and make sure you open and close the tags accurately:\n",
    "\n",
    "        Identify the core components of this problem.\n",
    "        1. Identify potential edge cases and tricky parts.\n",
    "        2. Write 2 short test cases for the edge cases and tricky parts.\n",
    "        \n",
    "        <chain_of_thoughts>\n",
    "        1. you must consider the edge cases according to the problem statement.\n",
    "        2. Begin with a <thinking> section.\n",
    "        3. Inside the thinking section:\n",
    "           a. Write the topic name of the query, the name of the algorithm if necessary.\n",
    "           b. Draft an answer as an expert.\n",
    "           b. Briefly analyze the question and outline your approach.\n",
    "           c. Present a clear plan of steps to solve the problem.\n",
    "           d. Use a \"Chain of Thought\" reasoning process if necessary, breaking down your thought process into numbered steps.\n",
    "        4. Include a <reflection> section for each idea where you:\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        5. Be sure to close all reflection sections.\n",
    "        6. Close the thinking section with </thinking>.\n",
    "        7. Provide your final answer in an <output> section.        \n",
    "        </chain_of_thoughts>\n",
    "\n",
    "        <chain_of_thought_selection>\n",
    "        you must consider the edge cases according to the problem statement and select the most promising chain of thought that solves the edge cases (not necessarily the simplest nor the standard approach).\n",
    "        </chain_of_thought_selection>\n",
    "\n",
    "        <solution>\n",
    "        1. As a Python expert, generate the Python code and make sure it solves the edge cases while keeping it efficient.\n",
    "        2. the internal steps must produce the required output.\n",
    "        </solution>\n",
    "\n",
    "        Include a <reflection> section for the selected solution where if it is not correct, modify or if necessary, rewrite the solution and pay attention to the input problem.\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights according to the problem. you must consider the edge cases according to the problem. Make sure it is not overcomplicated.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        4. Be sure to close all reflection sections.\n",
    "        \n",
    "        <context_generation>\n",
    "        1. Rewrite the problem.\n",
    "        2. Rewrite the edge cases and tricky parts in one short sentence\n",
    "        2. Generate a very accurate and minimal Python code/pseudocode for the final solution. Ensure that the final solution is minimal and accurate.\n",
    "        </context_generation>\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"\"\"\n",
    "\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            prompt_template,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        # print(\"generated_thought trajectory: \", generated_text)\n",
    "\n",
    "        return generated_text\n",
    "    \n",
    "    def extract_hidden_states(self, context):\n",
    "        best_crv, seq_length = self.crv_generator.generate_crvs(\n",
    "            context, crv_layers=crv_layers, max_length=configs.MAX_LENGTH\n",
    "        )\n",
    "        return best_crv, seq_length  # Return the hidden state and its len\n",
    "\n",
    "    def generate_crv(self, hidden_states, seq_length):\n",
    "        # return torch.mean(hidden_states, dim=1)\n",
    "        return hidden_states, seq_length\n",
    "        \n",
    "    def final_generation(self, original_query, test_cases, crv, seq_length, max_new_tokens=250):\n",
    "\n",
    "        query=f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\n{original_query}.\\nYour code should pass the following tests:{test_cases}\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\"\"\"\n",
    "        # Combine original query and CRV\n",
    "        self.memory_manager.add_memory(\n",
    "        crv, seq_length, layer_idx=self.layer_idx, crv_layers=crv_layers\n",
    "    )\n",
    "\n",
    "        # model.model.set_post_concat_crv(True)\n",
    "        self.memory_manager.set_concat_positions(0, start_pos=0, end_pos=seq_length)\n",
    "        if isinstance(self.layer_idx, int):\n",
    "            self.memory_manager.apply_memory_to_model(0)\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            query,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        return generated_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f10ce6-8cea-4da8-b689-8d68acffe825",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_text = '''<|start_header_id|>user<|end_header_id|>You are an expert Python programmer designed to provide standard, accurate,and fully working codes, and here is your task:\\n\n",
    "        \\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n res = tuple(set(test_tup1) & set(test_tup2))\\n return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n result = False\\n for i in range(2,int(math.sqrt(n)) + 1):\\n if n % i == 0:\\n result = True\\n return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n largest_nums = hq.nlargest(n, nums)\\n return largest_nums\\n```<|eot_id|>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2060ece-69fc-42e4-bd65-9c708f7e2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def move_to_cpu(model):\n",
    "    model = model.cpu()\n",
    "    clear_gpu_memory()\n",
    "    return model\n",
    "\n",
    "def move_to_gpu(model):\n",
    "    if torch.cuda.is_available():\n",
    "        return model.cuda()\n",
    "    return model\n",
    "\n",
    "def check_memory(threshold=0.8):\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated()\n",
    "        memory_reserved = torch.cuda.memory_reserved()\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory\n",
    "        # print(memory_allocated,memory_reserved,memory_total)\n",
    "        memory_usage = (memory_allocated + memory_reserved) / memory_total\n",
    "        \n",
    "        if memory_usage > threshold:\n",
    "            print(memory_allocated,memory_reserved,memory_total, memory_usage)\n",
    "        return True \n",
    "    return False\n",
    "check_memory()\n",
    "clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb0ab3b-b2a3-4653-b87c-4d130362d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def save_checkpoint(layer_idx: int, instance_index: int, results: List[Dict], checkpoint_dir: str = \"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_layer_{layer_idx}_instance_{instance_index}.json\")\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        \"layer_idx\": layer_idx,\n",
    "        \"instance_index\": instance_index,\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    with open(checkpoint_path, \"w\") as f:\n",
    "        json.dump(checkpoint_data, f)\n",
    "    \n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_dir: str = \"checkpoints\") -> Dict:\n",
    "\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "\n",
    "        checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_\")])\n",
    "        \n",
    "        if not checkpoint_files:\n",
    "            return None\n",
    "        \n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "        \n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        \n",
    "        checkpoint_data[\"results\"] = defaultdict(list, checkpoint_data[\"results\"])\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "        return checkpoint_data\n",
    "    else: return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba3ff791-3d11-4830-b45b-9fd96cea0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# def evaluate_model(model, tokenizer, dataset: Dataset, layer_indices: List[int], checkpoint_dir: str = \"checkpoints\") -> pd.DataFrame:\n",
    "#     pr_flag = False\n",
    "#     checkpoint = load_checkpoint(checkpoint_dir)\n",
    "\n",
    "#     if checkpoint:\n",
    "#         start_layer_idx = checkpoint[\"layer_idx\"]\n",
    "#         start_instance_index = checkpoint[\"instance_index\"] + 1\n",
    "#         results = defaultdict(list, checkpoint[\"results\"])\n",
    "\n",
    "#     else:\n",
    "#         start_layer_idx = layer_indices[0]\n",
    "#         start_instance_index = 0\n",
    "#         results = defaultdict(list)\n",
    "\n",
    "        \n",
    "#     for layer_idx in tqdm(layer_indices[layer_indices.index(start_layer_idx):], desc=\"Processing layer indices\"):\n",
    "#         framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx=layer_idx)\n",
    "        \n",
    "#         for i, instance in enumerate(tqdm(dataset.skip(start_instance_index), desc=f\"Processing instances for layer {layer_idx}\")):\n",
    "#             # if i < start_index:\n",
    "#             #     print('continue')\n",
    "#             #     continue\n",
    "\n",
    "#             if not pr_flag:\n",
    "#                 print(f\"start_layer_idx: {start_layer_idx}\\nstart_instance_index: {start_instance_index}\\ni: {i}\\nlayer_idx: {layer_idx}\")\n",
    "#             try:    \n",
    "#             # print('instance: ', instance)\n",
    "#                 # if i % 5 == 0:\n",
    "#                 #     # clear_gpu_memory()\n",
    "#                 #     save_checkpoint(current_index, pd.DataFrame(results))\n",
    "#                 # Save checkpoint every 5 instances\n",
    "#                 if (start_instance_index + i + 1) % 5 == 0:\n",
    "#                     save_checkpoint(layer_idx, start_instance_index + i, dict(results), checkpoint_dir)\n",
    "                \n",
    "\n",
    "#                 if i % 20 == 0:\n",
    "#                     clear_gpu_memory()\n",
    "                    \n",
    "#                 if check_memory():\n",
    "#                     clear_gpu_memory()\n",
    "                \n",
    "#                 query = instance['query'][0] if instance['query'] else ''\n",
    "#                 context = instance['context'][0] if instance['context'] else '' \n",
    "#                 test_cases = '\\n'.join(extract_test_cases(instance['input_final_prompts'][0])[-1])\n",
    "#                 tmp = \"Proposed solution context: \"\n",
    "#                 trajectories_and_context = framework.generate_thought_trajectories(query, context, test_cases, max_new_tokens=1000, alt_text=alt_text)\n",
    "#                 context_expansion = extract_sections(trajectories_and_context, \"context_generation\")\n",
    "#                 context_expansion = tmp + context_expansion + extract_sections(trajectories_and_context, \"solution\")\n",
    "#                 # print('context_expansion: ', context_expansion)\n",
    "#                 # print(\"\\end of context ----\")\n",
    "#                 hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "#                 crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "                \n",
    "#                 final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "#                 # print(\"final output: \", final_output)\n",
    "#                 extracted_functions = extract_functions(final_output)\n",
    "                \n",
    "#                 # result = {\n",
    "#                 #     'layer_idx': layer_idx,\n",
    "#                 #     'instance_id': i,  # This should be the index in the dataset\n",
    "#                 #     'query': query,\n",
    "#                 #     'context': context,\n",
    "#                 #     'test_cases': test_cases,\n",
    "#                 #     'final_output': final_output,\n",
    "#                 #     'extracted_functions': extracted_functions\n",
    "#                 # }\n",
    "#                 # Update results\n",
    "#                 if i + start_instance_index >= len(results['instance_id']):\n",
    "#                     results['instance_id'].append(i + start_instance_index)\n",
    "#                     results['query'].append(query)\n",
    "#                     results['context'].append(context)\n",
    "#                     results['test_cases'].append(test_cases)\n",
    "                \n",
    "#                 results[f'final_output_{layer_idx}'].append(final_output)\n",
    "#                 results[f'extracted_functions_{layer_idx}'].append(extracted_functions)\n",
    "#                 results[f'trajectories_and_context_{layer_idx}'].append(trajectories_and_context)\n",
    "#                 results[f'context_expansion_{layer_idx}'].append(context_expansion)\n",
    "\n",
    "#                 if not pr_flag:\n",
    "#                     print(f\"results dict for instance i: {results}\")\n",
    "#                     pr_flag = True\n",
    "\n",
    "                \n",
    "#                 current_index = i\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing example {start_instance_index + i}: {str(e)}\")\n",
    "#                 # Save checkpoint on error\n",
    "#                 # save_checkpoint(current_index, pd.DataFrame(results))\n",
    "#                 save_checkpoint(layer_idx, start_instance_index + i, dict(results), checkpoint_dir)\n",
    "\n",
    "#                 raise\n",
    "    \n",
    "#     return pd.DataFrame(results), current_index\n",
    "\n",
    "\n",
    "# def evaluate_model(model, tokenizer, dataset: Dataset, layer_indices: List[int], checkpoint_dir: str = \"checkpoints\") -> pd.DataFrame:\n",
    "#     pr_flag = False\n",
    "#     checkpoint = load_checkpoint(checkpoint_dir)\n",
    "#     max_instances = 0\n",
    "#     dataset_size = len(dataset)\n",
    "\n",
    "#     if checkpoint:\n",
    "#         start_layer_idx = checkpoint[\"layer_idx\"]\n",
    "#         start_instance_index = checkpoint[\"instance_index\"] + 1\n",
    "#         results = defaultdict(list, checkpoint[\"results\"])\n",
    "#         max_instances = len(results['instance_id'])\n",
    "#     else:\n",
    "#         start_layer_idx = layer_indices[0]\n",
    "#         start_instance_index = 0\n",
    "#         results = defaultdict(list)\n",
    "\n",
    "#     for layer_idx in tqdm(layer_indices[layer_indices.index(start_layer_idx):], desc=\"Processing layer indices\"):\n",
    "#         framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx=layer_idx)\n",
    "\n",
    "#         if start_instance_index >= dataset_size:\n",
    "#             print(f\"All instances processed for layer {layer_idx}. Moving to next layer.\")\n",
    "#             start_instance_index = 0\n",
    "#             continue\n",
    "\n",
    "#         for i, instance in enumerate(tqdm(dataset.skip(start_instance_index), desc=f\"Processing instances for layer {layer_idx}\"), total=dataset_size-start_instance_index):\n",
    "#             if not pr_flag:\n",
    "#                 print(f\"start_layer_idx: {start_layer_idx}\\nstart_instance_index: {start_instance_index}\\ni: {i}\\nlayer_idx: {layer_idx}\")\n",
    "#             try:    \n",
    "#                 if (start_instance_index + i + 1) % 5 == 0:\n",
    "#                     save_checkpoint(layer_idx, start_instance_index + i, dict(results), checkpoint_dir)\n",
    "                \n",
    "#                 if i % 20 == 0:\n",
    "#                     clear_gpu_memory()\n",
    "                    \n",
    "#                 if check_memory():\n",
    "#                     clear_gpu_memory()\n",
    "                \n",
    "#                 query = instance['query'][0] if instance['query'] else ''\n",
    "#                 context = instance['context'][0] if instance['context'] else '' \n",
    "#                 test_cases = '\\n'.join(extract_test_cases(instance['input_final_prompts'][0])[-1])\n",
    "#                 tmp = \"Proposed solution context: \"\n",
    "#                 trajectories_and_context = framework.generate_thought_trajectories(query, context, test_cases, max_new_tokens=1000, alt_text=alt_text)\n",
    "#                 context_expansion = extract_sections(trajectories_and_context, \"context_generation\")\n",
    "#                 context_expansion = tmp + context_expansion + extract_sections(trajectories_and_context, \"solution\")\n",
    "#                 hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "#                 crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "                \n",
    "#                 final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "#                 extracted_functions = extract_functions(final_output)\n",
    "                \n",
    "#                 current_instance = i + start_instance_index\n",
    "#                 if current_instance >= max_instances:\n",
    "#                     results['instance_id'].append(current_instance)\n",
    "#                     results['query'].append(query)\n",
    "#                     results['context'].append(context)\n",
    "#                     results['test_cases'].append(test_cases)\n",
    "#                     max_instances = current_instance + 1\n",
    "\n",
    "#                 results[f'final_output_{layer_idx}'].append(final_output)\n",
    "#                 results[f'extracted_functions_{layer_idx}'].append(extracted_functions)\n",
    "#                 results[f'trajectories_and_context_{layer_idx}'].append(trajectories_and_context)\n",
    "#                 results[f'context_expansion_{layer_idx}'].append(context_expansion)\n",
    "\n",
    "#                 if not pr_flag:\n",
    "#                     print(f\"results dict for instance i: {results}\")\n",
    "#                     pr_flag = True\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing example {start_instance_index + i}: {str(e)}\")\n",
    "#                 save_checkpoint(layer_idx, start_instance_index + i, dict(results), checkpoint_dir)\n",
    "#                 raise\n",
    "\n",
    "#         # Pad shorter arrays with None values\n",
    "#         for key, value in results.items():\n",
    "#             if len(value) < max_instances:\n",
    "#                 results[key].extend([None] * (max_instances - len(value)))\n",
    "\n",
    "#     max_length = max(len(v) for v in results.values())\n",
    "#     for key in results:\n",
    "#     results[key] = results[key] + [None] * (max_length - len(results[key]))\n",
    "\n",
    "#     # Convert results to DataFrame\n",
    "#     df = pd.DataFrame(results)\n",
    "#     df.set_index('instance_id', inplace=True)\n",
    "#     return df\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset: Dataset, layer_indices: List[int], checkpoint_dir: str = \"checkpoints\", max_retries: int = 3) -> pd.DataFrame:\n",
    "    pr_flag = False\n",
    "    checkpoint = load_checkpoint(checkpoint_dir)\n",
    "    max_instances = 0\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    if checkpoint:\n",
    "        start_layer_idx = checkpoint[\"layer_idx\"]\n",
    "        start_instance_index = checkpoint[\"instance_index\"] + 1\n",
    "        results = defaultdict(list, checkpoint[\"results\"])\n",
    "        max_instances = len(results['instance_id'])\n",
    "    else:\n",
    "        start_layer_idx = layer_indices[0]\n",
    "        start_instance_index = 0\n",
    "        results = defaultdict(list)\n",
    "    print(\"layer_indices: \", layer_indices)\n",
    "    for layer_idx in tqdm(layer_indices[layer_indices.index(start_layer_idx):], desc=\"Processing layer indices\"):\n",
    "        framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx=layer_idx)\n",
    "        \n",
    "        if start_instance_index >= dataset_size:\n",
    "            print(f\"All instances processed for layer {layer_idx}. Moving to next layer.\")\n",
    "            start_instance_index = 0\n",
    "            continue\n",
    "        \n",
    "        for i, instance in enumerate(tqdm(dataset.skip(start_instance_index), desc=f\"Processing instances for layer {layer_idx}\", total=dataset_size-start_instance_index)):\n",
    "            if not pr_flag:\n",
    "                print(f\"start_layer_idx: {start_layer_idx}\\nstart_instance_index: {start_instance_index}\\ni: {i}\\nlayer_idx: {layer_idx}\")\n",
    "            try:    \n",
    "                if (start_instance_index + i + 1) % 5 == 0:\n",
    "                    save_checkpoint(layer_idx, start_instance_index + i, dict(results), checkpoint_dir)\n",
    "                \n",
    "                if i % 20 == 0:\n",
    "                    clear_gpu_memory()\n",
    "                    \n",
    "                if check_memory():\n",
    "                    clear_gpu_memory()\n",
    "                \n",
    "                query = instance['query'][0] if instance['query'] else ''\n",
    "                context = instance['context'][0] if instance['context'] else '' \n",
    "                test_cases = '\\n'.join(extract_test_cases(instance['input_final_prompts'][0])[-1])\n",
    "                tmp = \"Proposed solution context: \"\n",
    "\n",
    "                for attempt in range(max_retries):\n",
    "                    try:\n",
    "                        trajectories_and_context = framework.generate_thought_trajectories(query, context, test_cases, max_new_tokens=1000, alt_text=alt_text)\n",
    "                        context_expansion = extract_sections(trajectories_and_context, \"context_generation\")\n",
    "                        context_expansion = f\"Proposed solution context: {context_expansion}{extract_sections(trajectories_and_context, 'solution')}\"\n",
    "                        \n",
    "                        hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "                        crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "                        \n",
    "                        final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "                        extracted_functions = extract_functions(final_output)\n",
    "                        \n",
    "                        # generation_successful = True\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Generation failed for layer {layer_idx}, instance {i}, attempt {attempt + 1}: {str(e)}\\nquery: {query}\")\n",
    "                        if attempt == max_retries - 1:\n",
    "                            logger.error(f\"All retries failed for layer {layer_idx}, instance {i}. Error: {traceback.format_exc()}\")\n",
    "            \n",
    "\n",
    "                # trajectories_and_context = framework.generate_thought_trajectories(query, context, test_cases, max_new_tokens=1000, alt_text=alt_text)\n",
    "                # context_expansion = extract_sections(trajectories_and_context, \"context_generation\")\n",
    "                # context_expansion = tmp + context_expansion + extract_sections(trajectories_and_context, \"solution\")\n",
    "                \n",
    "                # hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "                # crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "                \n",
    "                # final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "                # extracted_functions = extract_functions(final_output)\n",
    "                \n",
    "                current_instance = i + start_instance_index\n",
    "                if current_instance >= max_instances:\n",
    "                    results['instance_id'].append(current_instance)\n",
    "                    results['query'].append(query)\n",
    "                    results['context'].append(context)\n",
    "                    results['test_cases'].append(test_cases)\n",
    "                    max_instances = current_instance + 1\n",
    "\n",
    "                results[f'final_output_{layer_idx}'].append(final_output)\n",
    "                results[f'extracted_functions_{layer_idx}'].append(extracted_functions)\n",
    "                results[f'trajectories_and_context_{layer_idx}'].append(trajectories_and_context)\n",
    "                results[f'context_expansion_{layer_idx}'].append(context_expansion)\n",
    "\n",
    "                if not pr_flag:\n",
    "                    print(f\"results dict for instance i: {results}\")\n",
    "                    pr_flag = True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {start_instance_index + i}: {str(e)}\")\n",
    "                save_checkpoint(layer_idx, start_instance_index + i, dict(results), checkpoint_dir)\n",
    "                raise\n",
    "\n",
    "        start_instance_index = 0\n",
    "\n",
    "    # Ensure all arrays have the same length\n",
    "    max_length = max(len(v) for v in results.values())\n",
    "    for key in results:\n",
    "        results[key] = results[key] + [None] * (max_length - len(results[key]))\n",
    "\n",
    "    df = pd.DataFrame(dict(results))  # Convert defaultdict to regular dict\n",
    "    if 'instance_id' in df.columns:\n",
    "        df.set_index('instance_id', inplace=True)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    # df = pd.DataFrame(results)\n",
    "    # df.set_index('instance_id', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86716f98-f9ae-4a0d-bfe6-90aa7e6acebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from checkpoints/checkpoint_layer_1_instance_9.json\n",
      "Resuming from example 9\n",
      "Loaded checkpoint from checkpoints/checkpoint_layer_1_instance_9.json\n",
      "layer_indices:  [1, 10, 23, 'orig']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ebb66bd86b4901ab2117aa996eacf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing layer indices:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c0eeca70d94ac5a2702beae39acab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing instances for layer 1:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_layer_idx: 1\n",
      "start_instance_index: 10\n",
      "i: 0\n",
      "layer_idx: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results dict for instance i: defaultdict(<class 'list'>, {'instance_id': [0, 1, 2, 3, 5, 6, 7, 8, 10], 'query': ['<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to check if given tuple is distinct or not.\\nYour code should pass the following tests:\\nassert check_distinct((1, 4, 5, 6, 1, 4)) == False\\nassert check_distinct((1, 4, 5, 6)) == True\\nassert check_distinct((2, 3, 4, 5, 6)) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to find the first non-repeated character in a given string.\\nYour code should pass the following tests:\\nassert first_non_repeating_character(\"abcabc\") == None\\nassert first_non_repeating_character(\"abc\") == \"a\"\\nassert first_non_repeating_character(\"ababc\") == \"c\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to check whether the given string starts and ends with the same character or not using regex.\\nYour code should pass the following tests:\\nassert check_char(\"abba\") == \"Valid\"\\nassert check_char(\"a\") == \"Valid\"\\nassert check_char(\"abcd\") == \"Invalid\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to perform index wise addition of tuple elements in the given two nested tuples.\\nYour code should pass the following tests:\\nassert add_nested_tuples(((1, 3), (4, 5), (2, 9), (1, 10)), ((6, 7), (3, 9), (1, 1), (7, 3))) == ((7, 10), (7, 14), (3, 10), (8, 13))\\nassert add_nested_tuples(((2, 4), (5, 6), (3, 10), (2, 11)), ((7, 8), (4, 10), (2, 2), (8, 4))) == ((9, 12), (9, 16), (5, 12), (10, 15))\\nassert add_nested_tuples(((3, 5), (6, 7), (4, 11), (3, 12)), ((8, 9), (5, 11), (3, 3), (9, 5))) == ((11, 14), (11, 18), (7, 14), (12, 17))<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to check if a url is valid or not using regex.\\nYour code should pass the following tests:\\nassert is_valid_URL(\"https://www.google.com\") == True\\nassert is_valid_URL(\"https:/www.gmail.com\") == False\\nassert is_valid_URL(\"https:// www.redit.com\") == False<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to find the minimum of two numbers.\\nYour code should pass the following tests:\\nassert minimum(1,2) == 1\\nassert minimum(-5,-4) == -5\\nassert minimum(0,0) == 0<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to check whether an element exists within a tuple.\\nYour code should pass the following tests:\\nassert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\", \"e\"),\\'r\\')==True\\nassert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\", \"e\"),\\'5\\')==False\\nassert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\",\"e\"),3)==True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to find the parity of a given number.\\nYour code should pass the following tests:\\nassert find_Parity(12) == \"Even Parity\"\\nassert find_Parity(7) == \"Odd Parity\"\\nassert find_Parity(10) == \"Even Parity\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find k number of pairs which consist of one element from the first array and one element from the second array.\\nYour code should pass the following tests:\\nassert k_smallest_pairs([1,3,7],[2,4,6],2)==[[1, 2], [1, 4]]\\nassert k_smallest_pairs([1,3,7],[2,4,6],1)==[[1, 2]]\\nassert k_smallest_pairs([1,3,7],[2,4,6],7)==[[1, 2], [1, 4], [3, 2], [1, 6], [3, 4], [3, 6], [7, 2]]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python'], 'context': ['<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>', '<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n  res = tuple(set(test_tup1) & set(test_tup2))\\n  return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n    result = False\\n    for i in range(2,int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            result = True\\n    return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n  largest_nums = hq.nlargest(n, nums)\\n  return largest_nums\\n```<|eot_id|>'], 'test_cases': ['assert check_distinct((1, 4, 5, 6, 1, 4)) == False\\nassert check_distinct((1, 4, 5, 6)) == True\\nassert check_distinct((2, 3, 4, 5, 6)) == True', 'assert first_non_repeating_character(\"abcabc\") == None\\nassert first_non_repeating_character(\"abc\") == \"a\"\\nassert first_non_repeating_character(\"ababc\") == \"c\"', 'assert check_char(\"abba\") == \"Valid\"\\nassert check_char(\"a\") == \"Valid\"\\nassert check_char(\"abcd\") == \"Invalid\"', 'assert add_nested_tuples(((1, 3), (4, 5), (2, 9), (1, 10)), ((6, 7), (3, 9), (1, 1), (7, 3))) == ((7, 10), (7, 14), (3, 10), (8, 13))\\nassert add_nested_tuples(((2, 4), (5, 6), (3, 10), (2, 11)), ((7, 8), (4, 10), (2, 2), (8, 4))) == ((9, 12), (9, 16), (5, 12), (10, 15))\\nassert add_nested_tuples(((3, 5), (6, 7), (4, 11), (3, 12)), ((8, 9), (5, 11), (3, 3), (9, 5))) == ((11, 14), (11, 18), (7, 14), (12, 17))', 'assert is_valid_URL(\"https://www.google.com\") == True\\nassert is_valid_URL(\"https:/www.gmail.com\") == False\\nassert is_valid_URL(\"https:// www.redit.com\") == False', 'assert minimum(1,2) == 1\\nassert minimum(-5,-4) == -5\\nassert minimum(0,0) == 0', 'assert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\", \"e\"),\\'r\\')==True\\nassert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\", \"e\"),\\'5\\')==False\\nassert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\",\"e\"),3)==True', 'assert find_Parity(12) == \"Even Parity\"\\nassert find_Parity(7) == \"Odd Parity\"\\nassert find_Parity(10) == \"Even Parity\"', 'assert k_smallest_pairs([1,3,7],[2,4,6],2)==[[1, 2], [1, 4]]\\nassert k_smallest_pairs([1,3,7],[2,4,6],1)==[[1, 2]]\\nassert k_smallest_pairs([1,3,7],[2,4,6],7)==[[1, 2], [1, 4], [3, 2], [1, 6], [3, 4], [3, 6], [7, 2]]'], 'final_output_1': ['\\nHowever, for the purpose of this problem, you can convert it to a tuple and then check if the tuple is distinct or not.\\n```\\nThe proposed solution is already correct.\\n \\nThis solution is already correct.\\nThe proposed solution is correct.\\nThe proposed solution is correct.\\nThe proposed solution is correct.', \"\\n- Test case 4: (1, 4, 5)\\nThe proposed solution uses Python's set() function to remove duplicates. Therefore, we don't need to worry about the order of elements. We can simply convert the list to a set, you can use the set() function to remove duplicates.\", \"\\nTest case 3: (1, 2, 3, 2, 2, 3)\\nTest case 1 and 3.\\nIn this case 1 and 2 will always distinct and immutable. Therefore, we don't need to use a tuple. The function will work as expected.\", \"\\nYour code should pass these tests as it correctly identifies distinct tuples.\\nNote: Python tuples are inherently unordered, so it's not possible to have duplicate elements in a tuple in the classical sense. However, if you're working with a list and want to convert it to a tuple, you can use the tuple() function.\", '\\nis_valid_URL(\"www.redit.com\")\\n\\nHere is the corrected code with the same task:\\n\\n```python\\nimport re\\n\\ndef is_valid_URL(url):\\n    pattern = re.compile(r\\'^(http|https)://[a-zA-Z0-9.-.com] {\\n    return bool(pattern.match(url))\\nassert is_valid_URL(\"https://www.google.com\") == True\\nassert is_valid_URL(\"https:/www.gmail.com\") == False\\nassert is_valid_URL(\"https:// www.redit.com\") == False\\n\\n```\\n\\nHere is the corrected code with the same task:\\n\\n```python\\nimport re\\n\\ndef is_valid(url):\\n    pattern = re.compile(r\\'^(http|https)://[a-zA-Z0-9.-.[a-zA-Z0-.[a-zA-Z0-9]', '\\n    //www.google.com\") \\nassert not is_valid_URL(\"https://www.gmail.com\") \\nassert not is_valid_URL(\"https:// www.redit.com\")', '\\n    def is_valid_tuple_element(url):\\n        pattern = re.compile(r\\'^(http|https?)//[a-zA-Z0-9.-]+.[a-zA-Z2\\ndef is_valid_URL(url):\\n    pattern = re.compile(r\\'^(http|https?)//[a-zA-Z0-9.-]+.[a-zA-Z2, \"e\")\\n    return bool(pattern.match(url))', \"\\n    pattern = re.compile(r'^[a-zA-Z0-9/ and\\n```\\n\\nThere are some minor errors in your code. Here is the task is your code.\\n \\nThis code should pass the following tasks: \\nPlease correct your code.\", '\\n1, 4]]\\nassert k_smallest_pairs([1,3,2,4,6],2]==[[1, 2], [1, 4], [3, 2], [1, 6], [3, 4], [3, 6], [7, 2]]\\n```\\n\\nHere is the corrected code:\\nimport itertools\\n\\ndef k_smallest_pairs(arr1, arr2, k):\\n    return [list(pair) for pair in itertools.combinations(arr1, arr2)][:k]\\n\\nThis code defines a function called k_smallest_pairs that takes three parameters: arr1, arr2, and k. It uses the itertools.combinations function to generate all possible combinations of arr1 and arr2, and then selects the first k pairs. The return statement returns a list of lists, where each list represents a pair of elements, one from arr1 and one from arr2. The function is designed to find the k smallest pairs.'], 'extracted_functions_1': ['', '', '', '', 'import re\\nimport re\\n\\ndef is_valid_URL(url):\\n    pattern = re.compile(r\\'^(http|https)://[a-zA-Z0-9.-.com] {\\n    return bool(pattern.match(url))\\nassert is_valid_URL(\"https://www.google.com\") == True\\nassert is_valid_URL(\"https:/www.gmail.com\") == False\\nassert is_valid_URL(\"https:// www.redit.com\") == False\\n```\\nHere is the corrected code with the same task:\\n```python\\nimport re\\n\\ndef is_valid(url):\\n    pattern = re.compile(r\\'^(http|https)://[a-zA-Z0-9.-.[a-zA-Z0-.[a-zA-Z0-9]', '', 'def is_valid_tuple_element(url):\\n        pattern = re.compile(r\\'^(http|https?)//[a-zA-Z0-9.-]+.[a-zA-Z2\\n\\ndef is_valid_URL(url):\\n    pattern = re.compile(r\\'^(http|https?)//[a-zA-Z0-9.-]+.[a-zA-Z2, \"e\")\\n    return bool(pattern.match(url))', '', 'import itertools\\n\\ndef k_smallest_pairs(arr1, arr2, k):\\n    return [list(pair) for pair in itertools.combinations(arr1, arr2)][:k]\\nThis code defines a function called k_smallest_pairs that takes three parameters: arr1, arr2, and k. It uses the itertools.combinations function to generate all possible combinations of arr1 and arr2, and then selects the first k pairs. The return statement returns a list of lists, where each list represents a pair of elements, one from arr1 and one from arr2. The function is designed to find the k smallest pairs.'], 'trajectories_and_context_1': [\" Here's the Python solution for the given problem:\\n\\n```python\\ndef check_distinct(tup):\\n  return len(set(tup)) == len(tup)\\n```\\n\\nExplanation:\\n\\n1. The problem requires us to check if a given tuple is distinct or not.\\n2. A distinct tuple has no duplicate elements.\\n3. We can use Python's built-in set data type to remove duplicates from a tuple.\\n4. If the length of the original tuple is equal to the length of the set created from the tuple, then the tuple is distinct.\\n\\nEdge Cases:\\n\\n- Test case 1: (1, 4, 5, 6, 1, 4)\\n- Test case 2: (1, 4, 5, 6)\\n- Test case 3: (2, 3, 4, 5, 6)\\n\\nThe provided code will pass all these test cases as it correctly identifies distinct tuples.\\n\\nNote: Python tuples are inherently unordered, so it's not possible to have duplicate elements in a tuple in the classical sense. However, if you're working with a list and want to convert it to a tuple, you can use the tuple() function.\", \" You can use a dictionary to count the frequency of each character in the string, and then find the first character that has a count of 1.\\n\\nHere's a Python function to do this:\\n\\n```python\\ndef first_non_repeating_character(s):\\n    char_count = {}\\n    for char in s:\\n        if char in char_count:\\n            char_count[char] += 1\\n        else:\\n            char_count[char] = 1\\n\\n    for char in s:\\n        if char_count[char] == 1:\\n            return char\\n\\n    return None\\n```\\n\\nThis function first counts the frequency of each character in the string, and then iterates over the string again to find the first character with a count of 1. If no such character is found, it returns None.\\n\\nThis solution has a time complexity of O(n), where n is the length of the string, because we are doing two passes over the string. The space complexity is also O(n) because in the worst case, we need to store all characters in the dictionary.\", ' <chain_of_thoughts>\\n\\n  1. The problem asks to check if a given string starts and ends with the same character or not.\\n  2. The input string can be empty, in this case, it is considered as a valid string.\\n  3. If the string is not empty, we can use the regex module in Python to match the first and last characters of the string.\\n  4. We can use the re.match and re.search functions from the regex module to achieve this.\\n\\n  <thinking>\\n\\n  1. Import the regex module.\\n  2. Use the re.match function to match the first character of the string.\\n  3. Use the re.search function to match the last character of the string.\\n  4. Compare the two matches to determine if the string starts and ends with the same character.\\n\\n  <reflection>\\n  1. Review the reasoning: The approach seems correct, but we need to handle the case where the string is empty.\\n  2. Check for potential errors or oversights: We should add a check for an empty string.\\n  3. Confirm or adjust the conclusion: Yes, the approach is correct, but we need to add a check for an empty string.\\n\\n  </reflection>\\n\\n  </thinking>\\n\\n  <solution>\\n\\n  ```python\\nimport re\\n\\ndef check_char(s):\\n    if not s:  # Check if the string is empty\\n        return \"Valid\"\\n    if re.match(s[0], s) and re.search(s[0], s):  # Check if the string starts and ends with the same character\\n        return \"Valid\"\\n    else:\\n        return \"Invalid\"\\n  ```\\n\\n  </solution>\\n\\n  <reflection>\\n\\n  1. Review the reasoning: The solution seems correct, but we need to make sure it handles the case where the string is empty.\\n  2. Check for potential errors or oversights: We should make sure the solution is efficient and handles edge cases correctly.\\n  3. Confirm or adjust the conclusion: Yes, the solution is correct and efficient.\\n\\n  </reflection>\\n\\n  </chain_of_thoughts>\\n\\n  <output>\\n\\n  The function check_char takes a string as input and returns \"Valid\" if the string starts and ends with the same character, and \"Invalid\" otherwise.\\n\\n  </output>\\n\"', ' <chain_of_thoughts>\\n\\n        1. The problem asks to perform index-wise addition of tuple elements in the given two nested tuples.\\n        2. To solve this, we need to iterate over the two tuples simultaneously using the zip function in Python, which allows us to iterate over two lists in parallel.\\n        3. We then add the corresponding elements of the tuples and store the results in a new tuple.\\n        4. Finally, we return the resulting tuple.\\n\\n        <reflection>\\n        This approach is efficient and straightforward, as it uses the built-in zip function to iterate over the tuples in parallel.\\n        It also ensures that the resulting tuple has the same structure as the input tuples.\\n        </reflection>\\n\\n        </thinking>\\n\\n        <solution>\\n        def add_nested_tuples(tuple1, tuple2):\\n            return tuple(a + b for a, b in zip(tuple1, tuple2))\\n        </solution>\\n\\n        <reflection>\\n        This solution is concise and easy to understand, and it handles the edge case where the tuples are of different lengths by stopping at the end of the shorter tuple.\\n        It also handles the case where the tuples are of equal length.\\n        </reflection>\\n\\n        <context_generation>\\n        Problem: Perform index-wise addition of tuple elements in the given two nested tuples.\\n        Edge cases: Tuples of different lengths, tuples of equal length.\\n        Solution: Use the zip function to iterate over the tuples in parallel, add the corresponding elements, and store the results in a new tuple.\\n        </context_generation>\\n        ```', ' \"\"\"\\nThe problem is to write a function to check if a URL is valid or not using regex.\\n\"\"\"\\n\\nimport re\\n\\ndef is_valid_URL(url):\\n    # Define a regular expression pattern for a valid URL\\n    pattern = re.compile(r\\'^(http|https)://[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$\\')\\n    \\n    # Check if the URL matches the pattern\\n    return bool(pattern.match(url))\\n\\n# Test cases\\nassert is_valid_URL(\"https://www.google.com\") \\nassert not is_valid_URL(\"https:/www.gmail.com\") \\nassert not is_valid_URL(\"https:// www.redit.com\")', ' def minimum(a, b):\\n    if a <= b:\\n        return a\\n    else:\\n        return b\\n\"', ' <chain_of_thoughts>\\n        1. identify potential edge cases and tricky parts of the problem. For example, what if the tuple contains non-string elements, what if the tuple is empty, or what if the element to check is not found in the tuple?\\n        2. Write test cases for the edge cases and tricky parts.\\n        \\n        <thinking>\\n        1. Write a function to check whether an element exists within a tuple. This can be done using the \\'in\\' operator in Python.\\n        2. The \\'in\\' operator checks for membership in a sequence, such as a string, tuple, list, or other sequence types.\\n        3. The function should take two parameters: the tuple and the element to check.\\n        4. Inside the function, use the \\'in\\' operator to check if the element is in the tuple.\\n        5. Return True if the element is found, False otherwise.\\n        \\n        <reflection>\\n        1. Review the reasoning: the \\'in\\' operator is the most straightforward way to check for membership in a sequence in Python.\\n        2. Check for potential errors or oversights: the function assumes that the input is a tuple and an element. If the input is not a tuple or the element is not found, the function will raise an error.\\n        3. Confirm or adjust the conclusion: the function is correct and efficient.\\n        \\n        <solution>\\n        def check_tuplex(tup, element):\\n            return element in tup\\n        </solution>\\n        \\n        <context_generation>\\n        1. Rewrite the problem: check if an element is in a tuple.\\n        2. Rewrite the edge cases and tricky parts: what if the tuple is empty, what if the element is not found, what if the tuple contains non-string elements.\\n        3. Generate a very accurate and minimal Python code for the final solution: \\n        \"\"\"\\n        def check_tuplex(tup, element):\\n            return element in tup\\n        \"\"\"\\n        </context_generation>\\n        </chain_of_thoughts>\\n\\nassert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\", \"e\"),\\'r\\')\\nassert not check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\", \"e\"),\\'5\\')\\nassert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\",\"e\"),3)assistant\\n\\ndef check_tuplex(tup, element):\\n    return element in tup', ' function to find the parity of a number\\n         Given a number, determine if it is even or odd.\\n\\n         Test cases:\\n         - find_Parity(12) should return \"Even Parity\"\\n         - find_Parity(7) should return \"Odd Parity\"\\n         - find_Parity(10) should return \"Even Parity\"\\n\\n         <thinking>\\n\\n         1. We can use the modulus operator (%) to find the remainder of a number when divided by 2.\\n         2. If the remainder is 0, the number is even; otherwise, it is odd.\\n         3. We can create a function that takes an integer as input and returns \"Even Parity\" if the number is even and \"Odd Parity\" if the number is odd.\\n\\n         <solution>\\n\\n         def find_Parity(n):\\n             if n % 2 == 0:\\n                 return \"Even Parity\"\\n             else:\\n                 return \"Odd Parity\"\\n\\n         <reflection>\\n         - This function uses the modulus operator to determine the remainder of the input number when divided by 2.\\n         - If the remainder is 0, the number is even, and the function returns \"Even Parity\"; otherwise, it returns \"Odd Parity\".\\n         - This approach is simple and efficient, as it only requires a single line of code.\\n         </reflection>\\n         \"', ' import itertools\\ndef k_smallest_pairs(arr1, arr2, k):\\n    return [list(pair) for pair in itertools.product(arr1, arr2)][:k]\\n\"'], 'context_expansion_1': [\"Proposed solution context: def check_distinct(tup):\\n  return len(set(tup)) == len(tup)\\n```\\nExplanation:\\n1. The problem requires us to check if a given tuple is distinct or not.\\n2. A distinct tuple has no duplicate elements.\\n3. We can use Python's built-in set data type to remove duplicates from a tuple.\\n4. If the length of the original tuple is equal to the length of the set created from the tuple, then the tuple is distinct.\\nEdge Cases:\\n- Test case 1: (1, 4, 5, 6, 1, 4)\\n- Test case 2: (1, 4, 5, 6)\\n- Test case 3: (2, 3, 4, 5, 6)\\nThe provided code will pass all these test cases as it correctly identifies distinct tuples.\\nNote: Python tuples are inherently unordered, so it's not possible to have duplicate elements in a tuple in the classical sense. However, if you're working with a list and want to convert it to a tuple, you can use the tuple() function.def check_distinct(tup):\\n  return len(set(tup)) == len(tup)\\n```\\nExplanation:\\n1. The problem requires us to check if a given tuple is distinct or not.\\n2. A distinct tuple has no duplicate elements.\\n3. We can use Python's built-in set data type to remove duplicates from a tuple.\\n4. If the length of the original tuple is equal to the length of the set created from the tuple, then the tuple is distinct.\\nEdge Cases:\\n- Test case 1: (1, 4, 5, 6, 1, 4)\\n- Test case 2: (1, 4, 5, 6)\\n- Test case 3: (2, 3, 4, 5, 6)\\nThe provided code will pass all these test cases as it correctly identifies distinct tuples.\\nNote: Python tuples are inherently unordered, so it's not possible to have duplicate elements in a tuple in the classical sense. However, if you're working with a list and want to convert it to a tuple, you can use the tuple() function.\", 'Proposed solution context: def first_non_repeating_character(s):\\n    char_count = {}\\n    for char in s:\\n        if char in char_count:\\n            char_count[char] += 1\\n        else:\\n            char_count[char] = 1\\n    for char in s:\\n        if char_count[char] == 1:\\n            return char\\n    return None\\n```\\nThis function first counts the frequency of each character in the string, and then iterates over the string again to find the first character with a count of 1. If no such character is found, it returns None.\\nThis solution has a time complexity of O(n), where n is the length of the string, because we are doing two passes over the string. The space complexity is also O(n) because in the worst case, we need to store all characters in the dictionary.def first_non_repeating_character(s):\\n    char_count = {}\\n    for char in s:\\n        if char in char_count:\\n            char_count[char] += 1\\n        else:\\n            char_count[char] = 1\\n    for char in s:\\n        if char_count[char] == 1:\\n            return char\\n    return None\\n```\\nThis function first counts the frequency of each character in the string, and then iterates over the string again to find the first character with a count of 1. If no such character is found, it returns None.\\nThis solution has a time complexity of O(n), where n is the length of the string, because we are doing two passes over the string. The space complexity is also O(n) because in the worst case, we need to store all characters in the dictionary.', 'Proposed solution context: import re\\n\\ndef check_char(s):\\n    if not s:  \\n        return \"Valid\"\\n    if re.match(s[0], s) and re.search(s[0], s):  \\n        return \"Valid\"\\n    else:\\n        return \"Invalid\"\\n  ```\\n  </solution>\\n  <reflection>\\n  1. Review the reasoning: The solution seems correct, but we need to make sure it handles the case where the string is empty.\\n  2. Check for potential errors or oversights: We should make sure the solution is efficient and handles edge cases correctly.\\n  3. Confirm or adjust the conclusion: Yes, the solution is correct and efficient.\\n  </reflection>\\n  </chain_of_thoughts>\\n  <output>\\n  The function check_char takes a string as input and returns \"Valid\" if the string starts and ends with the same character, and \"Invalid\" otherwise.\\n  </output>\\n\"```python\\nimport re\\n\\ndef check_char(s):\\n    if not s:  # Check if the string is empty\\n        return \"Valid\"\\n    if re.match(s[0], s) and re.search(s[0], s):  # Check if the string starts and ends with the same character\\n        return \"Valid\"\\n    else:\\n        return \"Invalid\"\\n  ```', 'Proposed solution context: Problem: Perform index-wise addition of tuple elements in the given two nested tuples.\\n        Edge cases: Tuples of different lengths, tuples of equal length.\\n        Solution: Use the zip function to iterate over the tuples in parallel, add the corresponding elements, and store the results in a new tuple.def add_nested_tuples(tuple1, tuple2):\\n            return tuple(a + b for a, b in zip(tuple1, tuple2))', 'Proposed solution context: import re\\n\\ndef is_valid_URL(url):\\n    pattern = re.compile(r\\'^(http|https)://[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$\\')\\n    return bool(pattern.match(url))\\nassert is_valid_URL(\"https://www.google.com\") \\nassert not is_valid_URL(\"https:/www.gmail.com\") \\nassert not is_valid_URL(\"https:// www.redit.com\")import re\\n\\ndef is_valid_URL(url):\\n    pattern = re.compile(r\\'^(http|https)://[a-zA-Z0-9.-]+.[a-zA-Z]{2,}$\\')\\n    return bool(pattern.match(url))\\nassert is_valid_URL(\"https://www.google.com\") \\nassert not is_valid_URL(\"https:/www.gmail.com\") \\nassert not is_valid_URL(\"https:// www.redit.com\")', 'Proposed solution context: def minimum(a, b):\\n    if a <= b:\\n        return a\\n    else:\\n        return b\\n\"def minimum(a, b):\\n    if a <= b:\\n        return a\\n    else:\\n        return b\\n\"', 'Proposed solution context: 1. Rewrite the problem: check if an element is in a tuple.\\n        2. Rewrite the edge cases and tricky parts: what if the tuple is empty, what if the element is not found, what if the tuple contains non-string elements.\\n        3. Generate a very accurate and minimal Python code for the final solution: \\n        \"\"\"\\n        def check_tuplex(tup, element):\\n            return element in tup\\n        \"\"\"def check_tuplex(tup, element):\\n            return element in tup', 'Proposed solution context: def find_Parity(n):\\n             if n % 2 == 0:\\n                 return \"Even Parity\"\\n             else:\\n                 return \"Odd Parity\"\\n         <reflection>\\n         - This function uses the modulus operator to determine the remainder of the input number when divided by 2.\\n         - If the remainder is 0, the number is even, and the function returns \"Even Parity\"; otherwise, it returns \"Odd Parity\".\\n         - This approach is simple and efficient, as it only requires a single line of code.\\n         </reflection>\\n         \"def find_Parity(n):\\n             if n % 2 == 0:\\n                 return \"Even Parity\"\\n             else:\\n                 return \"Odd Parity\"\\n         <reflection>\\n         - This function uses the modulus operator to determine the remainder of the input number when divided by 2.\\n         - If the remainder is 0, the number is even, and the function returns \"Even Parity\"; otherwise, it returns \"Odd Parity\".\\n         - This approach is simple and efficient, as it only requires a single line of code.\\n         </reflection>\\n         \"', 'Proposed solution context: def k_smallest_pairs(arr1, arr2, k):\\n    return [list(pair) for pair in itertools.product(arr1, arr2)][:k]\\n\"def k_smallest_pairs(arr1, arr2, k):\\n    return [list(pair) for pair in itertools.product(arr1, arr2)][:k]\\n\"']})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_19.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_24.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_29.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_34.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_39.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_44.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_49.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9202326016 11282677760 25217466368 0.8123339385908604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_54.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_59.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoints/checkpoint_layer_1_instance_64.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "subset_name = \"processed_Meta-Llama-3.1-8B-Instruct-evals__mbpp__details\"\n",
    "loaded_dataset = load_from_disk(f\"data/{subset_name}\")\n",
    "\n",
    "num_examples=250\n",
    "if num_examples is not None:\n",
    "    loaded_dataset = loaded_dataset.select(range(num_examples))\n",
    "\n",
    "# Define layer indices to evaluate\n",
    "layer_indices = [1, 10, 23, 'orig']\n",
    "# layer_indices = [32, 'orig']\n",
    " \n",
    "checkpoint = load_checkpoint()\n",
    "start_index = 0\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "if checkpoint:\n",
    "    start_index = checkpoint[\"instance_index\"]\n",
    "    results_df = checkpoint[\"results\"]\n",
    "    print(f\"Resuming from example {start_index}\")\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "# new_results_df = evaluate_model(model, tokenizer, loaded_dataset, layer_indices)\n",
    "new_results_df = evaluate_model(model, tokenizer, loaded_dataset, layer_indices)\n",
    "\n",
    "print(\"new_results_df info:\")\n",
    "print(new_results_df.info())\n",
    "print(\"\\nnew_results_df shape:\", new_results_df.shape)\n",
    "print(\"\\nnew_results_df columns:\", new_results_df.columns)\n",
    "print(\"\\nnew_results_df head:\")\n",
    "print(new_results_df.head())\n",
    "\n",
    "# Combine previous results with new results\n",
    "results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "# Add parsed functions to the dataset\n",
    "updated_dataset = add_parsed_functions_to_dataset(loaded_dataset, results_df, layer_indices)\n",
    "\n",
    "print(f\"Type of updated_dataset: {type(updated_dataset)}\")\n",
    "print(f\"Number of rows in updated_dataset: {len(updated_dataset)}\")\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_dataset.save_to_disk(f\"data/{subset_name}_results\")\n",
    "\n",
    "# os.remove(os.path.join(\"checkpoints\", \"checkpoint.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a97c8d5-83bc-4538-9033-a16db8b03482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    subset_name = \"processed_Meta-Llama-3.1-8B-Instruct-evals__mbpp__details\"\n",
    "    loaded_dataset = load_from_disk(f\"data/{subset_name}\")\n",
    "    num_examples = None\n",
    "    if num_examples is not None:\n",
    "        loaded_dataset = loaded_dataset.select(range(num_examples))\n",
    "\n",
    "    # Define layer indices to evaluate\n",
    "    layer_indices = [1, 10, 15, 20, 32, 'orig']\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    checkpoint = load_checkpoint()\n",
    "    start_index = 0\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    if checkpoint:\n",
    "        start_index = checkpoint[\"current_index\"]\n",
    "        results_df = checkpoint[\"results_df\"]\n",
    "        print(f\"Resuming from example {start_index}\")\n",
    "\n",
    "    \n",
    "    while start_index < len(loaded_dataset):\n",
    "\n",
    "        clear_gpu_memory()\n",
    "        # Evaluate model\n",
    "        new_results_df, end_index = evaluate_model(model, tokenizer, loaded_dataset, layer_indices, num_examples=num_examples, start_index=start_index)\n",
    "\n",
    "        # Combine previous results with new results\n",
    "        results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "        # Update start_index for next iteration\n",
    "        start_index = end_index\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(start_index, results_df)\n",
    "\n",
    "    # Add parsed functions to the dataset\n",
    "    updated_dataset = add_parsed_functions_to_dataset(loaded_dataset, results_df, layer_indices)\n",
    "\n",
    "    print(f\"Type of updated_dataset: {type(updated_dataset)}\")\n",
    "    print(f\"Number of rows in updated_dataset: {len(updated_dataset)}\")\n",
    "\n",
    "    # Save the updated dataset\n",
    "    updated_dataset.save_to_disk(f\"data/{subset_name}_results\")\n",
    "\n",
    "    # Clear checkpoint after successful completion\n",
    "    os.remove(os.path.join(\"checkpoints\", \"checkpoint.pkl\"))\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred: {str(e)}\")\n",
    "    #     print(f\"Last processed example: {start_index}\")\n",
    "    #     print(\"The script will automatically resume from this point when restarted.\")\n",
    "        \n",
    "        # Checkpoint is already saved in the evaluate_model function, so we don't need to save it here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128aecd5-f94d-4fb9-917f-1fe901f028c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_name = \"processed_Meta-Llama-3.1-8B-Instruct-evals__mbpp__details\"\n",
    "loaded_dataset = load_from_disk(f\"data/{subset_name}\")\n",
    "num_examples = None\n",
    "if num_examples is not None:\n",
    "    loaded_dataset = loaded_dataset.select(range(num_examples))\n",
    "\n",
    "# Define layer indices to evaluate\n",
    "layer_indices = [1, 10, 15, 20, 32, 'orig']\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint = load_checkpoint()\n",
    "start_index = 0\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "if checkpoint:\n",
    "    start_index = checkpoint[\"current_index\"]\n",
    "    results_df = checkpoint[\"results_df\"]\n",
    "    print(f\"Resuming from example {start_index}\")\n",
    "\n",
    "# Evaluate model\n",
    "new_results_df, end_index = evaluate_model(model, tokenizer, loaded_dataset, layer_indices, num_examples=num_examples, start_index=start_index)\n",
    "\n",
    "# Combine previous results with new results\n",
    "results_df = pd.concat([results_df, new_results_df], ignore_index=True)\n",
    "\n",
    "# Add parsed functions to the dataset\n",
    "updated_dataset = add_parsed_functions_to_dataset(loaded_dataset, results_df, layer_indices)\n",
    "\n",
    "print(f\"Type of updated_dataset: {type(updated_dataset)}\")\n",
    "print(f\"Number of rows in updated_dataset: {len(updated_dataset)}\")\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_dataset.save_to_disk(f\"data/{subset_name}_results\")\n",
    "\n",
    "# Clear checkpoint after successful completion\n",
    "os.remove(os.path.join(\"checkpoints\", \"checkpoint.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60e384e-9f0d-46a9-b3da-17e344ff6202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 2 rows of the dataset:\n",
      "Row 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m()  \u001b[38;5;66;03m# Add a blank line between rows\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 2 rows of the dataset:\")\n",
    "for i, example in enumerate(results_df):\n",
    "    if i < 2:\n",
    "        print(f\"Row {i + 1}:\")\n",
    "        for key, value in example.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()  # Add a blank line between rows\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc304549-af46-4f55-8b9f-20b6f63edf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():    \n",
    "    # Print column names\n",
    "    print(\"\\nColumns in the updated dataset:\")\n",
    "    print(updated_dataset.column_names)\n",
    "    # loaded_dataset2 = load_from_disk(\"data/processed_meta_llama_dataset_with_results\")\n",
    "    loaded_dataset2 = load_dataset(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct-evals\",\n",
    "    name=\"Meta-Llama-3.1-8B-Instruct-evals__mbpp__details\",\n",
    "    split=\"latest\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Print selected columns from the first 5 rows of the updated dataset\n",
    "    print(\"First 5 rows of the updated dataset (selected columns):\")\n",
    "    for i, example in enumerate(loaded_dataset2.select(range(num_examples))):\n",
    "        print(f\"\\nExample {i + 1}:\")\n",
    "        # Print original columns\n",
    "        print(f\"input_correct_responses: {example['input_correct_responses']}...\")\n",
    "        # print(f\"extracted_functions_layer_15: {example['extracted_functions_layer_15']}...\")\n",
    "        \n",
    "        # Print new columns for each layer\n",
    "        for layer in layer_indices:\n",
    "            print(f\"\\nLayer {layer}:\")\n",
    "            final_output = example.get(f'final_output_layer_{layer}')\n",
    "            extracted_functions = example.get(f'extracted_functions_layer_{layer}')\n",
    "            \n",
    "            if final_output:\n",
    "                print(f\"Final Output: {final_output[:100]}...\")\n",
    "            else:\n",
    "                print(\"Final Output: None\")\n",
    "            \n",
    "            if extracted_functions:\n",
    "                print(f\"Extracted Functions: {extracted_functions[:100]}...\")\n",
    "            else:\n",
    "                print(\"Extracted Functions: None\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ee130-7498-4889-a337-2f6d119fc84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
