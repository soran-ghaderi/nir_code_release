{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import configs\n",
    "from experimental.controller.memory_manager import MemoryManager\n",
    "from generator.crv_generator import CRVGenerator\n",
    "from generator.text_generator import TextGenerator\n",
    "\n",
    "from utils import set_seed, logger\n",
    "from utils.loading_model import CustomTransformerLoader\n",
    "\n",
    "# from rich import print\n",
    "from rich.console import Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "230bae0c-9bde-4ff6-9657-aa26969d36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and console\n",
    "console = Console()\n",
    "logger = logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aff6ac92-f30c-436c-a039-4009fe621fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "model_urls = {\n",
    "    \"llama31\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}\n",
    "model_path = model_urls[\"llama31\"]\n",
    "tokenizer_path = model_path\n",
    "hf_token = \"your token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca8834cd-3750-47bb-9446-40aeaabfb7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Loading the Model</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[92m──────────────────────────────────────────────── \u001B[0m\u001B[1;31mLoading the Model\u001B[0m\u001B[92m ────────────────────────────────────────────────\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, use_auth_token=hf_token)\n",
    "\n",
    "console.rule(\"[bold red]Loading the Model\")\n",
    "\n",
    "loader = CustomTransformerLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cdaf49da-287e-4088-bb45-18672902dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3220: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">config:  LlamaConfig <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"_name_or_path\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta-llama/Meta-Llama-3.1-8B-Instruct\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"architectures\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"LlamaForCausalLM\"</span>\n",
       "  <span style=\"font-weight: bold\">]</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"attention_bias\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"attention_dropout\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"bos_token_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128000</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"eos_token_id\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128001</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128008</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128009</span>\n",
       "  <span style=\"font-weight: bold\">]</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_act\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"silu\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"initializer_range\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"intermediate_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14336</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"max_position_embeddings\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">131072</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"mlp_bias\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"model_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"llama\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_attention_heads\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_hidden_layers\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_key_value_heads\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pretraining_tp\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"rms_norm_eps\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"rope_scaling\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"factor\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"high_freq_factor\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"low_freq_factor\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"original_max_position_embeddings\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"rope_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"llama3\"</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"rope_theta\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500000.0</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"tie_word_embeddings\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"torch_dtype\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"bfloat16\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"transformers_version\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"4.44.2\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"use_cache\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"vocab_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128256</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "config:  LlamaConfig \u001B[1m{\u001B[0m\n",
       "  \u001B[32m\"_name_or_path\"\u001B[0m: \u001B[32m\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\u001B[0m,\n",
       "  \u001B[32m\"architectures\"\u001B[0m: \u001B[1m[\u001B[0m\n",
       "    \u001B[32m\"LlamaForCausalLM\"\u001B[0m\n",
       "  \u001B[1m]\u001B[0m,\n",
       "  \u001B[32m\"attention_bias\"\u001B[0m: false,\n",
       "  \u001B[32m\"attention_dropout\"\u001B[0m: \u001B[1;36m0.0\u001B[0m,\n",
       "  \u001B[32m\"bos_token_id\"\u001B[0m: \u001B[1;36m128000\u001B[0m,\n",
       "  \u001B[32m\"eos_token_id\"\u001B[0m: \u001B[1m[\u001B[0m\n",
       "    \u001B[1;36m128001\u001B[0m,\n",
       "    \u001B[1;36m128008\u001B[0m,\n",
       "    \u001B[1;36m128009\u001B[0m\n",
       "  \u001B[1m]\u001B[0m,\n",
       "  \u001B[32m\"hidden_act\"\u001B[0m: \u001B[32m\"silu\"\u001B[0m,\n",
       "  \u001B[32m\"hidden_size\"\u001B[0m: \u001B[1;36m4096\u001B[0m,\n",
       "  \u001B[32m\"initializer_range\"\u001B[0m: \u001B[1;36m0.02\u001B[0m,\n",
       "  \u001B[32m\"intermediate_size\"\u001B[0m: \u001B[1;36m14336\u001B[0m,\n",
       "  \u001B[32m\"max_position_embeddings\"\u001B[0m: \u001B[1;36m131072\u001B[0m,\n",
       "  \u001B[32m\"mlp_bias\"\u001B[0m: false,\n",
       "  \u001B[32m\"model_type\"\u001B[0m: \u001B[32m\"llama\"\u001B[0m,\n",
       "  \u001B[32m\"num_attention_heads\"\u001B[0m: \u001B[1;36m32\u001B[0m,\n",
       "  \u001B[32m\"num_hidden_layers\"\u001B[0m: \u001B[1;36m32\u001B[0m,\n",
       "  \u001B[32m\"num_key_value_heads\"\u001B[0m: \u001B[1;36m8\u001B[0m,\n",
       "  \u001B[32m\"pretraining_tp\"\u001B[0m: \u001B[1;36m1\u001B[0m,\n",
       "  \u001B[32m\"rms_norm_eps\"\u001B[0m: \u001B[1;36m1e-05\u001B[0m,\n",
       "  \u001B[32m\"rope_scaling\"\u001B[0m: \u001B[1m{\u001B[0m\n",
       "    \u001B[32m\"factor\"\u001B[0m: \u001B[1;36m8.0\u001B[0m,\n",
       "    \u001B[32m\"high_freq_factor\"\u001B[0m: \u001B[1;36m4.0\u001B[0m,\n",
       "    \u001B[32m\"low_freq_factor\"\u001B[0m: \u001B[1;36m1.0\u001B[0m,\n",
       "    \u001B[32m\"original_max_position_embeddings\"\u001B[0m: \u001B[1;36m8192\u001B[0m,\n",
       "    \u001B[32m\"rope_type\"\u001B[0m: \u001B[32m\"llama3\"\u001B[0m\n",
       "  \u001B[1m}\u001B[0m,\n",
       "  \u001B[32m\"rope_theta\"\u001B[0m: \u001B[1;36m500000.0\u001B[0m,\n",
       "  \u001B[32m\"tie_word_embeddings\"\u001B[0m: false,\n",
       "  \u001B[32m\"torch_dtype\"\u001B[0m: \u001B[32m\"bfloat16\"\u001B[0m,\n",
       "  \u001B[32m\"transformers_version\"\u001B[0m: \u001B[32m\"4.44.2\"\u001B[0m,\n",
       "  \u001B[32m\"use_cache\"\u001B[0m: true,\n",
       "  \u001B[32m\"vocab_size\"\u001B[0m: \u001B[1;36m128256\u001B[0m\n",
       "\u001B[1m}\u001B[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42514f1381e43538aee61ec50a11780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":warning: model type:  <class 'moc_layers.LlamaForCausalLM'>\n",
      "config.hidden_size:  32\n",
      "config._attn_implementation:  eager\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = loader.load_model(\n",
    "    model_path=model_path, tokenizer_path=tokenizer_path, hf_token=hf_token\n",
    ")\n",
    "\n",
    "crv_layers = configs.CRV_LAYERS\n",
    "\n",
    "print(\":warning: model type: \", type(model))\n",
    "print(\"config.hidden_size: \", config.num_hidden_layers)\n",
    "print(\"config._attn_implementation: \", config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "453bb3fd-6791-42f7-8d32-432e86e62914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_context_expansion(text):\n",
    "    pattern = r'<context_generation>(.*?)</context_generation>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return f\"Context expansion section not found. The original text: {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bda16c02-52d9-43da-81e9-0193cd2470b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cases len:  4\n",
      "['assert rearrange_bigger(12)==21', 'assert rearrange_bigger(10)==False', 'assert rearrange_bigger(102)==120']\n"
     ]
    }
   ],
   "source": [
    "def extract_test_cases(text):\n",
    "    # Pattern to match assert statements\n",
    "    pattern = r'assert\\s+[\\w_]+\\(.*?\\).*?(?=[\\n<]|$)'\n",
    "    \n",
    "    # Find all matches\n",
    "    test_cases = re.findall(pattern, text)\n",
    "    \n",
    "    # Group test cases by task\n",
    "    grouped_tests = []\n",
    "    current_group = []\n",
    "    \n",
    "    for test in test_cases:\n",
    "        if current_group and not test.startswith(current_group[-1].split('(')[0]):\n",
    "            grouped_tests.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(test)\n",
    "    \n",
    "    if current_group:\n",
    "        grouped_tests.append(current_group)\n",
    "        print(\"test cases len: \", len(grouped_tests))\n",
    "    \n",
    "    return grouped_tests\n",
    "\n",
    "text = '''<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n res = tuple(set(test_tup1) & set(test_tup2))\\n return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n result = False\\n for i in range(2,int(math.sqrt(n)) + 1):\\n if n % i == 0:\\n result = True\\n return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n largest_nums = hq.nlargest(n, nums)\\n return largest_nums\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to create the next bigger number by rearranging the digits of a given number.\\nYour code should pass the following tests:\\nassert rearrange_bigger(12)==21\\nassert rearrange_bigger(10)==False\\nassert rearrange_bigger(102)==120<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\"'''\n",
    "out = extract_test_cases(text)\n",
    "print(out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f5569a92-06d5-47d4-8e61-bdfa151e8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_functions2(text):\n",
    "    # Regex pattern to match function definitions\n",
    "    # pattern = r\"def\\s+\\w+\\s*\\(.*?\\):\\s*(?:\\n\\s*['\\\"].*?['\\\"])?(?:\\n(?:(?!def\\s).)*?)*\"\n",
    "    # pattern = r\"(def\\s+\\w+\\s*\\(.*?\\):(?:\\s*['\\\"][\\s\\S]*?['\\\"])?\\s*(?:(?!def\\s)[\\s\\S])*?(?=\\ndef|\\Z))\"\n",
    "    function_pattern = r\"(def\\s+\\w+\\s*\\(.*?\\):(?:\\s*['\\\"][\\s\\S]*?['\\\"])?\\s*(?:(?!def\\s)[\\s\\S])*?(?=\\ndef|\\Z))\"\n",
    "    \n",
    "    functions = re.findall(function_pattern, text, re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    def clean_function(func):\n",
    "        # Remove docstrings\n",
    "        func = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'', '', func)\n",
    "        # Remove comments\n",
    "        func = re.sub(r'#.*', '', func)\n",
    "        # Remove empty lines and trailing whitespace\n",
    "        func = '\\n'.join(line for line in func.splitlines() if line.strip())\n",
    "        return func\n",
    "    \n",
    "    cleaned_functions = [clean_function(func) for func in functions]\n",
    "    \n",
    "    # Join functions with two newlines for readability\n",
    "    return '\\n\\n'.join(cleaned_functions)\n",
    "\n",
    "\n",
    "    \n",
    "    # # Find all matches in the text\n",
    "    # functions = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # # Strip leading/trailing whitespace and return the list\n",
    "    # return [func.strip() for func in functions]\n",
    "\n",
    "def extract_functions(text):\n",
    "    # Extract imports\n",
    "    import_pattern = r'^(?:from\\s+[\\w.]+\\s+import\\s+(?:[\\w.]+(?:\\s*,\\s*[\\w.]+)*|\\*)|import\\s+(?:[\\w.]+(?:\\s*,\\s*[\\w.]+)*))(?:\\s+as\\s+[\\w.]+)?'\n",
    "    imports = re.findall(import_pattern, text, re.MULTILINE)\n",
    "    \n",
    "    # Extract functions\n",
    "    function_pattern = r\"(def\\s+\\w+\\s*\\(.*?\\):(?:\\s*['\\\"][\\s\\S]*?['\\\"])?\\s*(?:(?!def\\s)[\\s\\S])*?(?=\\ndef|\\Z))\"\n",
    "    functions = re.findall(function_pattern, text, re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    def clean_code(code):\n",
    "        # Remove docstrings\n",
    "        code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'', '', code)\n",
    "        # Remove comments\n",
    "        code = re.sub(r'#.*', '', code)\n",
    "        # Remove empty lines and trailing whitespace\n",
    "        code = '\\n'.join(line for line in code.splitlines() if line.strip())\n",
    "        return code\n",
    "    \n",
    "    cleaned_imports = [clean_code(imp) for imp in imports]\n",
    "    cleaned_functions = [clean_code(func) for func in functions]\n",
    "    \n",
    "    # Combine imports and functions\n",
    "    cleaned_code = '\\n'.join(cleaned_imports)\n",
    "    if cleaned_imports and cleaned_functions:\n",
    "        cleaned_code += '\\n\\n'\n",
    "    cleaned_code += '\\n\\n'.join(cleaned_functions)\n",
    "    \n",
    "    return cleaned_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c1d545c9-6fe3-47e3-aaf0-32deb2973b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLLaMACRVFramework:\n",
    "    def __init__(self, model, tokenizer, layer_idx = 10):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_generator = TextGenerator(model, tokenizer)\n",
    "        self.crv_generator = CRVGenerator(model, tokenizer, max_length=configs.MAX_LENGTH)\n",
    "        self.memory_manager = MemoryManager(model, max_memories=5)\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "\n",
    "    def generate_thought_trajectories(self, input_query, test_cases=None, max_new_tokens=1000):\n",
    "        prompt_template = f\"\"\"\n",
    "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        \n",
    "        \\n\\nYou are an expert Python programmer designed to provide standard, accurate,and fully working codes, and here is your task:\\n\n",
    "        \\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n res = tuple(set(test_tup1) & set(test_tup2))\\n return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n result = False\\n for i in range(2,int(math.sqrt(n)) + 1):\\n if n % i == 0:\\n result = True\\n return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n largest_nums = hq.nlargest(n, nums)\\n return largest_nums\\n```<|eot_id|>\n",
    "        Enable code_interpreter tool.\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        \n",
    "        \\n\\nYou are an expert Python programmer, and here is your task:\\n{input_query}.\\nYour code must pass these test cases:{test_cases}\n",
    "        \n",
    "        \\n\\nYour outputs must follow this structure:\n",
    "\n",
    "        Identify the core components of this problem.\n",
    "        1. Identify potential edge cases and tricky parts.\n",
    "        2. Write 2 short test cases for the edge cases and tricky parts.\n",
    "        \n",
    "        <chain_of_thoughts>\n",
    "        1. you must consider the edge cases according to the problem statement.\n",
    "        2. Begin with a <thinking> section.\n",
    "        3. Inside the thinking section:\n",
    "           a. Write the topic name of the query, the name of the algorithm if necessary.\n",
    "           b. Draft an answer as an expert.\n",
    "           b. Briefly analyze the question and outline your approach.\n",
    "           c. Present a clear plan of steps to solve the problem.\n",
    "           d. Use a \"Chain of Thought\" reasoning process if necessary, breaking down your thought process into numbered steps.\n",
    "        4. Include a <reflection> section for each idea where you:\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        5. Be sure to close all reflection sections.\n",
    "        6. Close the thinking section with </thinking>.\n",
    "        7. Provide your final answer in an <output> section.        \n",
    "        </chain_of_thoughts>\n",
    "\n",
    "        <chain_of_thought_selection>\n",
    "        you must consider the edge cases according to the problem statement and select the most promising chain of thought that solves the edge cases (not necessarily the simplest nor the standard approach).\n",
    "        </chain_of_thought_selection>\n",
    "\n",
    "        <solution>\n",
    "        1. As a Python expert, generate the Python code and make sure it solves the edge cases while keeping it efficient.\n",
    "        2. the internal steps must produce the required output.\n",
    "        </solution>\n",
    "\n",
    "        Include a <reflection> section for the selected solution where if it is not correct, modify or if necessary, rewrite the solution and pay attention to the input problem.\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights according to the problem. you must consider the edge cases according to the problem. Make sure it is not overcomplicated.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        4. Be sure to close all reflection sections.\n",
    "        \n",
    "        <context_generation>\n",
    "        1. Rewrite the problem.\n",
    "        2. Rewrite the edge cases and tricky parts in one short sentence\n",
    "        2. Generate a very accurate and minimal Python code/pseudocode for the final solution. Ensure that the final solution is minimal and accurate.\n",
    "        </context_generation>\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"\"\"\n",
    "        # <|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\n",
    "        # <|start_header_id|>system<|end_header_id|>\n",
    "        \n",
    "        # <|eot_id|>\n",
    "        # ```python\n",
    "        # print(\"prompt: \", prompt)\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            prompt_template,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        return generated_text\n",
    "    \n",
    "    def extract_hidden_states(self, context):\n",
    "        best_crv, seq_length = self.crv_generator.generate_crvs(\n",
    "            context, crv_layers=crv_layers, max_length=configs.MAX_LENGTH\n",
    "        )\n",
    "        return best_crv, seq_length  # Return the hidden state and its len\n",
    "\n",
    "    def generate_crv(self, hidden_states, seq_length):\n",
    "        # return torch.mean(hidden_states, dim=1)\n",
    "        return hidden_states, seq_length\n",
    "        \n",
    "    def final_generation(self, original_query, test_cases, crv, seq_length, max_new_tokens=250):\n",
    "\n",
    "        query=f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\n{original_query}.\\nYour code should pass the following tests:{test_cases}\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\"\"\"\n",
    "        # Combine original query and CRV\n",
    "        self.memory_manager.add_memory(\n",
    "        crv, seq_length, layer_idx=self.layer_idx, crv_layers=crv_layers\n",
    "    )\n",
    "\n",
    "        # model.model.set_post_concat_crv(True)\n",
    "        self.memory_manager.set_concat_positions(0, start_pos=0, end_pos=seq_length)\n",
    "        self.memory_manager.apply_memory_to_model(0)\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            query,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        # print(generated_text)\n",
    "        print('==' * 50)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3128aaa-6a4a-47f7-8a99-4d3c60cf0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the dataset\n",
    "\n",
    "\n",
    "framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx = 15)\n",
    "\n",
    "loaded_dataset = load_from_disk(\"data/processed_meta_llama_dataset\")\n",
    "# layer_idx = 5\n",
    "i = 0\n",
    "buff = \"\"\n",
    "types = []\n",
    "num_examples = 3\n",
    "for instance in loaded_dataset:\n",
    "    if i> num_examples-1:\n",
    "        break\n",
    "    query = instance['query'][0]\n",
    "    context = instance['context'][0]\n",
    "    test_cases = '\\n'.join(extract_test_cases(instance['input_final_prompts'][0])[-1])\n",
    "    # print(len(query), query)\n",
    "    # print(len(context))\n",
    "    print(\"test_cases: \", test_cases)\n",
    "    print('====' * 15)\n",
    "\n",
    "    trajectories_and_context = framework.generate_thought_trajectories(query, test_cases, max_new_tokens=1000)\n",
    "    print(\"End of Trajectories and Context: \", trajectories_and_context)\n",
    "    context_expansion = extract_context_expansion(trajectories_and_context)\n",
    "    print('====' * 15)\n",
    "\n",
    "    print(\"the extracted context: \", context_expansion)\n",
    "    print('----' * 15)\n",
    "    \n",
    "    # Instance 2: Extract hidden states from generated context\n",
    "    hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "    \n",
    "    # Generate CRV from hidden states\n",
    "    crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "    print(\"Generated CRV:\")\n",
    "\n",
    "    # Instance 3: Final generation using original query and CRV\n",
    "    final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250) #todo: change query to the instructed query\n",
    "    print(\"Final Output:\", final_output)\n",
    "    print('test cases: ', test_cases)\n",
    "    print(\"extracted functions: \", extract_functions(final_output))\n",
    "    print('****|' * 15)\n",
    "    \n",
    "\n",
    "\n",
    "    # model.model.set_post_concat_crv(True)\n",
    "    # memory_manager.set_concat_positions(0, start_pos=0, end_pos=best_seq_length)\n",
    "    # memory_manager.apply_memory_to_model(0)\n",
    "    # generated_text = text_generator.generate_text(\n",
    "    # query,\n",
    "    # max_new_tokens=400,\n",
    "    # num_return_sequences = 1,\n",
    "    # output_file=\"data/results.csv\",\n",
    "    # # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "    # )\n",
    "    # print(generated_text)\n",
    "    # print('==' * 50)\n",
    "\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "538429bc-63ae-47ce-bb07-3977f65c8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset: Dataset, layer_indices: List[int], num_examples: int = -1) -> pd.DataFrame:\n",
    "    results = []\n",
    "    \n",
    "    for layer_idx in tqdm(layer_indices, desc=\"Processing layer indices\"):\n",
    "        framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx=layer_idx)\n",
    "        \n",
    "        for i, instance in enumerate(tqdm(dataset, desc=f\"Processing instances for layer {layer_idx}\")):\n",
    "            if num_examples != -1 and i >= num_examples:\n",
    "                break\n",
    "            \n",
    "            query = instance['query'][0]\n",
    "            context = instance['context'][0]\n",
    "            test_cases = '\\n'.join(extract_test_cases(instance['input_final_prompts'][0])[-1])\n",
    "            \n",
    "            trajectories_and_context = framework.generate_thought_trajectories(query, test_cases, max_new_tokens=1000)\n",
    "            context_expansion = extract_context_expansion(trajectories_and_context)\n",
    "            \n",
    "            hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "            crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "            \n",
    "            final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "            extracted_functions = extract_functions(final_output)\n",
    "            \n",
    "            result = {\n",
    "                'layer_idx': layer_idx,\n",
    "                'instance_id': i,\n",
    "                'query': query,\n",
    "                'context': context,\n",
    "                'test_cases': test_cases,\n",
    "                'final_output': final_output,\n",
    "                'extracted_functions': extracted_functions\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "585e3fe8-645b-44f4-9924-8097f7882a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parsed_functions_to_dataset(dataset: Dataset, results_df: pd.DataFrame, layer_indices=[5, 10, 15, 20]) -> Dataset:\n",
    "    # Convert results DataFrame to a dictionary\n",
    "    results_dict = results_df.to_dict('records')\n",
    "    \n",
    "    # Create a dictionary to store new columns\n",
    "    new_columns = {\n",
    "        'final_output_layer_5': [],\n",
    "        'final_output_layer_10': [],\n",
    "        'final_output_layer_15': [],\n",
    "        'final_output_layer_20': [],\n",
    "        'extracted_functions_layer_5': [],\n",
    "        'extracted_functions_layer_10': [],\n",
    "        'extracted_functions_layer_15': [],\n",
    "        'extracted_functions_layer_20': [],\n",
    "    }\n",
    "    \n",
    "    # Populate new columns\n",
    "    for i in range(len(dataset)):\n",
    "        instance_results = [r for r in results_dict if r['instance_id'] == i]\n",
    "        for layer in layer_indices:\n",
    "            layer_result = next((r for r in instance_results if r['layer_idx'] == layer), None)\n",
    "            if layer_result:\n",
    "                new_columns[f'final_output_layer_{layer}'].append(layer_result['final_output'])\n",
    "                new_columns[f'extracted_functions_layer_{layer}'].append(layer_result['extracted_functions'])\n",
    "            else:\n",
    "                new_columns[f'final_output_layer_{layer}'].append(None)\n",
    "                new_columns[f'extracted_functions_layer_{layer}'].append(None)\n",
    "    \n",
    "    # Add new columns to the dataset\n",
    "    for column_name, column_data in new_columns.items():\n",
    "        dataset = dataset.add_column(column_name, column_data)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "62fb4037-cc97-428f-aecf-043d5166beef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layer indices:   0%|                           | 0/1 [00:00<?, ?it/s]\n",
      "Processing instances for layer 15:   0%|                | 0/500 [00:00<?, ?it/s]\u001B[AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Processing instances for layer 15:   0%|                | 0/500 [00:00<?, ?it/s]\n",
      "Processing layer indices:   0%|                           | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cases len:  4\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 72.00 MiB. GPU \u0002 has a total capacity of 23.49 GiB of which 8.25 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 52.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[108], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;66;03m# You can now use this updated dataset for further metric calculations\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 24\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[108], line 9\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m layer_indices \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m15\u001B[39m]\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Evaluate model\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m results_df \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloaded_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_indices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Add parsed functions to the dataset\u001B[39;00m\n\u001B[1;32m     12\u001B[0m updated_dataset \u001B[38;5;241m=\u001B[39m add_parsed_functions_to_dataset(loaded_dataset, results_df, layer_indices)\n",
      "Cell \u001B[0;32mIn[104], line 20\u001B[0m, in \u001B[0;36mevaluate_model\u001B[0;34m(model, tokenizer, dataset, layer_indices, num_examples)\u001B[0m\n\u001B[1;32m     17\u001B[0m context \u001B[38;5;241m=\u001B[39m instance[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     18\u001B[0m test_cases \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(extract_test_cases(instance[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_final_prompts\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m---> 20\u001B[0m trajectories_and_context \u001B[38;5;241m=\u001B[39m \u001B[43mframework\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_thought_trajectories\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_cases\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m context_expansion \u001B[38;5;241m=\u001B[39m extract_context_expansion(trajectories_and_context)\n\u001B[1;32m     23\u001B[0m hidden_states, seq_len \u001B[38;5;241m=\u001B[39m framework\u001B[38;5;241m.\u001B[39mextract_hidden_states(context_expansion)\n",
      "Cell \u001B[0;32mIn[103], line 75\u001B[0m, in \u001B[0;36mAdvancedLLaMACRVFramework.generate_thought_trajectories\u001B[0;34m(self, input_query, test_cases, max_new_tokens)\u001B[0m\n\u001B[1;32m     12\u001B[0m prompt_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124m<|begin_of_text|><|start_header_id|>system<|end_header_id|>\u001B[39m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124m\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;124m<|start_header_id|>assistant<|end_header_id|>\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# <|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# <|start_header_id|>system<|end_header_id|>\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# <|eot_id|>\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# ```python\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# print(\"prompt: \", prompt)\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_text\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt_template\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/results.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# stop_sequences=[\"The end\", \".\\n\\n\"],\u001B[39;49;00m\n\u001B[1;32m     81\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m generated_text\n",
      "File \u001B[0;32m~/PycharmProjects/moc/generator/text_generator.py:135\u001B[0m, in \u001B[0;36mTextGenerator.generate_text\u001B[0;34m(self, prompt, max_length, max_new_tokens, num_return_sequences, temperature, top_k, top_p, min_p, dry_run, dry_multiplier, dry_base, dry_allowed_length, dry_sequence_breakers, repetition_penalty, no_repeat_ngram_size, crv_layer_idx, output_file, use_eos_token, stop_sequences)\u001B[0m\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;66;03m# Uncomment these lines if you need to set CRV concatenation\u001B[39;00m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;66;03m# self.model.model.set_layers_to_concat(layer_idx=crv_layer_idx)\u001B[39;00m\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;66;03m# self.model.model.set_is_crv_concatenated(is_crv_concatenated=False)\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 135\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m generated_texts \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mdecode(\n\u001B[1;32m    139\u001B[0m         output[input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] :], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    140\u001B[0m     )\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs\n\u001B[1;32m    142\u001B[0m ]\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_file:\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2024\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2016\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2017\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2018\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[1;32m   2019\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2020\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2021\u001B[0m     )\n\u001B[1;32m   2023\u001B[0m     \u001B[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[0;32m-> 2024\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2025\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2026\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2027\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2028\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2029\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2030\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2032\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2033\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2035\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[1;32m   2036\u001B[0m     \u001B[38;5;66;03m# 11. prepare logits warper\u001B[39;00m\n\u001B[1;32m   2037\u001B[0m     prepared_logits_warper \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2038\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(generation_config, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2039\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mdo_sample\n\u001B[1;32m   2040\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2041\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2982\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001B[0m\n\u001B[1;32m   2979\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[1;32m   2981\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 2982\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   2984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[1;32m   2985\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    167\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 169\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/PycharmProjects/moc/moc_layers.py:1725\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[1;32m   1720\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1721\u001B[0m     return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m   1722\u001B[0m )\n\u001B[1;32m   1724\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m-> 1725\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1726\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1729\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1730\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1731\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1732\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1733\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1734\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1735\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1736\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1738\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpretraining_tp \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/moc_layers.py:1487\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[1;32m   1483\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintegrate_crv(hidden_states, layer_idx)\n\u001B[1;32m   1485\u001B[0m     \u001B[38;5;66;03m# print(f\"hidden_states shape at {layer_idx}\", hidden_states.shape)\u001B[39;00m\n\u001B[1;32m   1486\u001B[0m     \u001B[38;5;66;03m# end modify\u001B[39;00m\n\u001B[0;32m-> 1487\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1488\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1492\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1493\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1496\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1498\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    167\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 169\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/PycharmProjects/moc/moc_layers.py:1114\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m   1112\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m   1113\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n\u001B[0;32m-> 1114\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1115\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m   1117\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/moc/venv/lib/python3.12/site-packages/accelerate/hooks.py:169\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    167\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 169\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/PycharmProjects/moc/moc_layers.py:472\u001B[0m, in \u001B[0;36mLlamaMLP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    470\u001B[0m     down_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(down_proj)\n\u001B[1;32m    471\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 472\u001B[0m     down_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_proj(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgate_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mup_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m down_proj\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 72.00 MiB. GPU \u0002 has a total capacity of 23.49 GiB of which 8.25 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 52.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load dataset\n",
    "    loaded_dataset = load_from_disk(\"data/processed_meta_llama_dataset\")\n",
    "\n",
    "    # Define layer indices to evaluate\n",
    "    layer_indices = [15]\n",
    "\n",
    "    # Evaluate model\n",
    "    results_df = evaluate_model(model, tokenizer, loaded_dataset, layer_indices, num_examples=3)\n",
    "\n",
    "    # Add parsed functions to the dataset\n",
    "    updated_dataset = add_parsed_functions_to_dataset(loaded_dataset, results_df, layer_indices)\n",
    "\n",
    "    # Save the updated dataset\n",
    "    updated_dataset.save_to_disk(\"data/processed_meta_llama_dataset_with_results\")\n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"Dataset size:\", len(updated_dataset))\n",
    "    print(\"Columns:\", updated_dataset.column_names)\n",
    "\n",
    "    # You can now use this updated dataset for further metric calculations\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a74d5-bf15-4434-8e1a-8699388784b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
