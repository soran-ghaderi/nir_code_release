{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdf8500-f6cf-491a-ad71-7a2c0da0d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Callable\n",
    "from radon.metrics import h_visit\n",
    "from radon.complexity import cc_visit\n",
    "\n",
    "layer_indices = [1, 10, 23, 'orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_name = \"processed_Meta-Llama-3.1-8B-Instruct-evals__mbpp__details_results\"\n",
    "\n",
    "# dataset = load_from_disk(f\"data/{subset_name}\")\n",
    "df = pd.read_csv(\"checkpoints/latest_results_clone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd786af8-7260-497c-97e8-0eaf307189a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_name</th>\n",
       "      <th>subtask_name</th>\n",
       "      <th>input_question</th>\n",
       "      <th>input_choice_list</th>\n",
       "      <th>input_final_prompts</th>\n",
       "      <th>input_correct_responses</th>\n",
       "      <th>output_prediction_text</th>\n",
       "      <th>output_parsed_answer</th>\n",
       "      <th>output_choice_completions</th>\n",
       "      <th>...</th>\n",
       "      <th>trajectories_and_context_10</th>\n",
       "      <th>context_expansion_10</th>\n",
       "      <th>final_output_23</th>\n",
       "      <th>extracted_functions_23</th>\n",
       "      <th>trajectories_and_context_23</th>\n",
       "      <th>context_expansion_23</th>\n",
       "      <th>final_output_orig</th>\n",
       "      <th>extracted_functions_orig</th>\n",
       "      <th>trajectories_and_context_orig</th>\n",
       "      <th>context_expansion_orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mbpp_chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write a function to check if given tuple is di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['\\ndef check_distinct(tup):\\n  return len(tup...</td>\n",
       "      <td>\\ndef check_distinct(tup):\\n  return len(tup) ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n        &lt;thinking&gt;\\n    ...</td>\n",
       "      <td>Proposed solution context: Given a tuple, chec...</td>\n",
       "      <td>\\ndef check_distinct(numbers):\\n    \"\"\"Check i...</td>\n",
       "      <td>def check_distinct(numbers):\\n    return len(n...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: Problem: Write a fu...</td>\n",
       "      <td>\\ndef check_distinct(numbers):\\n    \"\"\"\\n    C...</td>\n",
       "      <td>def check_distinct(numbers):\\n    return len(n...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n    &lt;top...</td>\n",
       "      <td>Proposed solution context: Problem: Check if a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mbpp_chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write a python function to find the first non-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['\\ndef first_non_repeating_character(s):\\n   ...</td>\n",
       "      <td>\\ndef first_non_repeating_character(s):\\n    c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n        &lt;thinking&gt;\\n    ...</td>\n",
       "      <td>Proposed solution context: def first_non_repea...</td>\n",
       "      <td>\\ndef first_non_repeating_character(s):\\n    c...</td>\n",
       "      <td>def first_non_repeating_character(s):\\n    cha...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: The problem is to f...</td>\n",
       "      <td>\\ndef first_non_repeating_character(s):\\n    \"...</td>\n",
       "      <td>def first_non_repeating_character(s):\\n    cha...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n        &lt;thinking&gt;\\n    ...</td>\n",
       "      <td>Proposed solution context: The problem is to f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mbpp_chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write a function to check whether the given st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['\\nimport re\\ndef check_char(s):\\n    if re.m...</td>\n",
       "      <td>\\nimport re\\ndef check_char(s):\\n    if re.mat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n        &lt;thinking&gt;\\n    ...</td>\n",
       "      <td>Proposed solution context: Problem: Check if a...</td>\n",
       "      <td>\\ndef check_char(s):\\n    \"\"\"Returns 'Valid' i...</td>\n",
       "      <td>def check_char(s):\\n    if len(set(s)) == 1:\\n...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: def check_char(s):\\...</td>\n",
       "      <td>\\ndef check_char(s):\\n    \"\"\"\\n    This functi...</td>\n",
       "      <td>def check_char(s):\\n    s = s.lower()\\n    if ...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: The problem stateme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mbpp_chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write a function to perform index wise additio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['\\ndef add_nested_tuples(tup1, tup2):\\n  retu...</td>\n",
       "      <td>\\ndef add_nested_tuples(tup1, tup2):\\n  return...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: Problem: Write a fu...</td>\n",
       "      <td>\\ndef add_nested_tuples(tup1, tup2):\\n    if l...</td>\n",
       "      <td>def add_nested_tuples(tup1, tup2):\\n    if len...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: Problem: Write a fu...</td>\n",
       "      <td>\\ndef add_nested_tuples(tup1, tup2):\\n    \"\"\"\\...</td>\n",
       "      <td>def add_nested_tuples(tup1, tup2):\\n    return...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: Problem: Write a fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mbpp_chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write a function to compute the value of ncr%p.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['\\ndef ncr_modp(n, r, p):\\n    if n &lt; r:\\n   ...</td>\n",
       "      <td>\\ndef ncr_modp(n, r, p):\\n    if n &lt; r:\\n     ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: Problem: Calculate ...</td>\n",
       "      <td>\\nimport math\\n\\ndef ncr_modp(n, r, p):\\n    i...</td>\n",
       "      <td>import math\\n\\ndef ncr_modp(n, r, p):\\n    if ...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: Problem: Calculate ...</td>\n",
       "      <td>\\ndef ncr_modp(n, r, p):\\n    if r &gt; n - r: \\n...</td>\n",
       "      <td>def ncr_modp(n, r, p):\\n    if r &gt; n - r: \\n  ...</td>\n",
       "      <td>&lt;chain_of_thoughts&gt;\\n    &lt;thinking&gt;\\n        ...</td>\n",
       "      <td>Proposed solution context: Problem: Calculate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    task_type  task_name  subtask_name  \\\n",
       "0  Generative  mbpp_chat           NaN   \n",
       "1  Generative  mbpp_chat           NaN   \n",
       "2  Generative  mbpp_chat           NaN   \n",
       "3  Generative  mbpp_chat           NaN   \n",
       "4  Generative  mbpp_chat           NaN   \n",
       "\n",
       "                                      input_question  input_choice_list  \\\n",
       "0  Write a function to check if given tuple is di...                NaN   \n",
       "1  Write a python function to find the first non-...                NaN   \n",
       "2  Write a function to check whether the given st...                NaN   \n",
       "3  Write a function to perform index wise additio...                NaN   \n",
       "4    Write a function to compute the value of ncr%p.                NaN   \n",
       "\n",
       "                                 input_final_prompts  input_correct_responses  \\\n",
       "0  ['<|start_header_id|>user<|end_header_id|>\\n\\n...                      NaN   \n",
       "1  ['<|start_header_id|>user<|end_header_id|>\\n\\n...                      NaN   \n",
       "2  ['<|start_header_id|>user<|end_header_id|>\\n\\n...                      NaN   \n",
       "3  ['<|start_header_id|>user<|end_header_id|>\\n\\n...                      NaN   \n",
       "4  ['<|start_header_id|>user<|end_header_id|>\\n\\n...                      NaN   \n",
       "\n",
       "                              output_prediction_text  \\\n",
       "0  ['\\ndef check_distinct(tup):\\n  return len(tup...   \n",
       "1  ['\\ndef first_non_repeating_character(s):\\n   ...   \n",
       "2  ['\\nimport re\\ndef check_char(s):\\n    if re.m...   \n",
       "3  ['\\ndef add_nested_tuples(tup1, tup2):\\n  retu...   \n",
       "4  ['\\ndef ncr_modp(n, r, p):\\n    if n < r:\\n   ...   \n",
       "\n",
       "                                output_parsed_answer  \\\n",
       "0  \\ndef check_distinct(tup):\\n  return len(tup) ...   \n",
       "1  \\ndef first_non_repeating_character(s):\\n    c...   \n",
       "2  \\nimport re\\ndef check_char(s):\\n    if re.mat...   \n",
       "3  \\ndef add_nested_tuples(tup1, tup2):\\n  return...   \n",
       "4  \\ndef ncr_modp(n, r, p):\\n    if n < r:\\n     ...   \n",
       "\n",
       "   output_choice_completions  ...  \\\n",
       "0                        NaN  ...   \n",
       "1                        NaN  ...   \n",
       "2                        NaN  ...   \n",
       "3                        NaN  ...   \n",
       "4                        NaN  ...   \n",
       "\n",
       "                         trajectories_and_context_10  \\\n",
       "0   <chain_of_thoughts>\\n        <thinking>\\n    ...   \n",
       "1   <chain_of_thoughts>\\n        <thinking>\\n    ...   \n",
       "2   <chain_of_thoughts>\\n        <thinking>\\n    ...   \n",
       "3   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "4   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "\n",
       "                                context_expansion_10  \\\n",
       "0  Proposed solution context: Given a tuple, chec...   \n",
       "1  Proposed solution context: def first_non_repea...   \n",
       "2  Proposed solution context: Problem: Check if a...   \n",
       "3  Proposed solution context: Problem: Write a fu...   \n",
       "4  Proposed solution context: Problem: Calculate ...   \n",
       "\n",
       "                                     final_output_23  \\\n",
       "0  \\ndef check_distinct(numbers):\\n    \"\"\"Check i...   \n",
       "1  \\ndef first_non_repeating_character(s):\\n    c...   \n",
       "2  \\ndef check_char(s):\\n    \"\"\"Returns 'Valid' i...   \n",
       "3  \\ndef add_nested_tuples(tup1, tup2):\\n    if l...   \n",
       "4  \\nimport math\\n\\ndef ncr_modp(n, r, p):\\n    i...   \n",
       "\n",
       "                              extracted_functions_23  \\\n",
       "0  def check_distinct(numbers):\\n    return len(n...   \n",
       "1  def first_non_repeating_character(s):\\n    cha...   \n",
       "2  def check_char(s):\\n    if len(set(s)) == 1:\\n...   \n",
       "3  def add_nested_tuples(tup1, tup2):\\n    if len...   \n",
       "4  import math\\n\\ndef ncr_modp(n, r, p):\\n    if ...   \n",
       "\n",
       "                         trajectories_and_context_23  \\\n",
       "0   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "1   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "2   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "3   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "4   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "\n",
       "                                context_expansion_23  \\\n",
       "0  Proposed solution context: Problem: Write a fu...   \n",
       "1  Proposed solution context: The problem is to f...   \n",
       "2  Proposed solution context: def check_char(s):\\...   \n",
       "3  Proposed solution context: Problem: Write a fu...   \n",
       "4  Proposed solution context: Problem: Calculate ...   \n",
       "\n",
       "                                   final_output_orig  \\\n",
       "0  \\ndef check_distinct(numbers):\\n    \"\"\"\\n    C...   \n",
       "1  \\ndef first_non_repeating_character(s):\\n    \"...   \n",
       "2  \\ndef check_char(s):\\n    \"\"\"\\n    This functi...   \n",
       "3  \\ndef add_nested_tuples(tup1, tup2):\\n    \"\"\"\\...   \n",
       "4  \\ndef ncr_modp(n, r, p):\\n    if r > n - r: \\n...   \n",
       "\n",
       "                            extracted_functions_orig  \\\n",
       "0  def check_distinct(numbers):\\n    return len(n...   \n",
       "1  def first_non_repeating_character(s):\\n    cha...   \n",
       "2  def check_char(s):\\n    s = s.lower()\\n    if ...   \n",
       "3  def add_nested_tuples(tup1, tup2):\\n    return...   \n",
       "4  def ncr_modp(n, r, p):\\n    if r > n - r: \\n  ...   \n",
       "\n",
       "                       trajectories_and_context_orig  \\\n",
       "0   <chain_of_thoughts>\\n    <thinking>\\n    <top...   \n",
       "1   <chain_of_thoughts>\\n        <thinking>\\n    ...   \n",
       "2   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "3   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "4   <chain_of_thoughts>\\n    <thinking>\\n        ...   \n",
       "\n",
       "                              context_expansion_orig  \n",
       "0  Proposed solution context: Problem: Check if a...  \n",
       "1  Proposed solution context: The problem is to f...  \n",
       "2  Proposed solution context: The problem stateme...  \n",
       "3  Proposed solution context: Problem: Write a fu...  \n",
       "4  Proposed solution context: Problem: Calculate ...  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cb8fd3-eacc-441a-a4f7-964d048a0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   task_type                               250 non-null    object \n",
      " 1   task_name                               250 non-null    object \n",
      " 2   subtask_name                            0 non-null      float64\n",
      " 3   input_question                          250 non-null    object \n",
      " 4   input_choice_list                       0 non-null      float64\n",
      " 5   input_final_prompts                     250 non-null    object \n",
      " 6   input_correct_responses                 0 non-null      float64\n",
      " 7   output_prediction_text                  250 non-null    object \n",
      " 8   output_parsed_answer                    250 non-null    object \n",
      " 9   output_choice_completions               0 non-null      float64\n",
      " 10  output_choice_negative_log_likelihoods  0 non-null      float64\n",
      " 11  output_metrics                          250 non-null    object \n",
      " 12  is_correct                              250 non-null    bool   \n",
      " 13  input_question_hash                     250 non-null    object \n",
      " 14  input_final_prompts_hash                250 non-null    object \n",
      " 15  benchmark_label                         250 non-null    object \n",
      " 16  eval_config                             250 non-null    object \n",
      " 17  context                                 250 non-null    object \n",
      " 18  query                                   250 non-null    object \n",
      " 19  extracted_test_cases                    250 non-null    object \n",
      " 20  final_output_1                          169 non-null    object \n",
      " 21  extracted_functions_1                   75 non-null     object \n",
      " 22  trajectories_and_context_1              169 non-null    object \n",
      " 23  context_expansion_1                     169 non-null    object \n",
      " 24  final_output_10                         168 non-null    object \n",
      " 25  extracted_functions_10                  167 non-null    object \n",
      " 26  trajectories_and_context_10             168 non-null    object \n",
      " 27  context_expansion_10                    168 non-null    object \n",
      " 28  final_output_23                         168 non-null    object \n",
      " 29  extracted_functions_23                  168 non-null    object \n",
      " 30  trajectories_and_context_23             168 non-null    object \n",
      " 31  context_expansion_23                    168 non-null    object \n",
      " 32  final_output_orig                       168 non-null    object \n",
      " 33  extracted_functions_orig                168 non-null    object \n",
      " 34  trajectories_and_context_orig           168 non-null    object \n",
      " 35  context_expansion_orig                  168 non-null    object \n",
      "dtypes: bool(1), float64(5), object(30)\n",
      "memory usage: 68.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5b3119-8f03-45f1-94f6-ddf53a413924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['task_type', 'task_name', 'subtask_name', 'input_question',\n",
      "       'input_choice_list', 'input_final_prompts', 'input_correct_responses',\n",
      "       'output_prediction_text', 'output_parsed_answer',\n",
      "       'output_choice_completions',\n",
      "       ...\n",
      "       'comment_ratio_orig', 'h1_orig', 'h2_orig', 'N1_orig', 'N2_orig',\n",
      "       'vocabulary_orig', 'length_orig', 'volume_orig', 'difficulty_orig',\n",
      "       'effort_orig'],\n",
      "      dtype='object', length=104)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee7d2da3-86ff-48bb-b1ee-705a5e213ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 104 columns):\n",
      " #    Column                                  Non-Null Count  Dtype  \n",
      "---   ------                                  --------------  -----  \n",
      " 0    task_type                               250 non-null    object \n",
      " 1    task_name                               250 non-null    object \n",
      " 2    subtask_name                            0 non-null      float64\n",
      " 3    input_question                          250 non-null    object \n",
      " 4    input_choice_list                       0 non-null      float64\n",
      " 5    input_final_prompts                     250 non-null    object \n",
      " 6    input_correct_responses                 0 non-null      float64\n",
      " 7    output_prediction_text                  250 non-null    object \n",
      " 8    output_parsed_answer                    250 non-null    object \n",
      " 9    output_choice_completions               0 non-null      float64\n",
      " 10   output_choice_negative_log_likelihoods  0 non-null      float64\n",
      " 11   output_metrics                          250 non-null    object \n",
      " 12   is_correct                              250 non-null    bool   \n",
      " 13   input_question_hash                     250 non-null    object \n",
      " 14   input_final_prompts_hash                250 non-null    object \n",
      " 15   benchmark_label                         250 non-null    object \n",
      " 16   eval_config                             250 non-null    object \n",
      " 17   context                                 250 non-null    object \n",
      " 18   query                                   250 non-null    object \n",
      " 19   extracted_test_cases                    250 non-null    object \n",
      " 20   final_output_1                          169 non-null    object \n",
      " 21   extracted_functions_1                   75 non-null     object \n",
      " 22   trajectories_and_context_1              169 non-null    object \n",
      " 23   context_expansion_1                     169 non-null    object \n",
      " 24   final_output_10                         168 non-null    object \n",
      " 25   extracted_functions_10                  167 non-null    object \n",
      " 26   trajectories_and_context_10             168 non-null    object \n",
      " 27   context_expansion_10                    168 non-null    object \n",
      " 28   final_output_23                         168 non-null    object \n",
      " 29   extracted_functions_23                  168 non-null    object \n",
      " 30   trajectories_and_context_23             168 non-null    object \n",
      " 31   context_expansion_23                    168 non-null    object \n",
      " 32   final_output_orig                       168 non-null    object \n",
      " 33   extracted_functions_orig                168 non-null    object \n",
      " 34   trajectories_and_context_orig           168 non-null    object \n",
      " 35   context_expansion_orig                  168 non-null    object \n",
      " 36   response_rate_1                         250 non-null    int64  \n",
      " 37   syntactic_correctness_1                 169 non-null    float64\n",
      " 38   cyclomatic_complexity_1                 169 non-null    float64\n",
      " 39   function_name_consistency_1             169 non-null    float64\n",
      " 40   lines_1                                 169 non-null    float64\n",
      " 41   characters_1                            169 non-null    float64\n",
      " 42   comment_lines_1                         169 non-null    float64\n",
      " 43   comment_ratio_1                         169 non-null    float64\n",
      " 44   h1_1                                    0 non-null      float64\n",
      " 45   h2_1                                    0 non-null      float64\n",
      " 46   N1_1                                    0 non-null      float64\n",
      " 47   N2_1                                    0 non-null      float64\n",
      " 48   vocabulary_1                            0 non-null      float64\n",
      " 49   length_1                                0 non-null      float64\n",
      " 50   volume_1                                0 non-null      float64\n",
      " 51   difficulty_1                            0 non-null      float64\n",
      " 52   effort_1                                0 non-null      float64\n",
      " 53   response_rate_10                        250 non-null    int64  \n",
      " 54   syntactic_correctness_10                168 non-null    float64\n",
      " 55   cyclomatic_complexity_10                168 non-null    float64\n",
      " 56   function_name_consistency_10            168 non-null    float64\n",
      " 57   lines_10                                168 non-null    float64\n",
      " 58   characters_10                           168 non-null    float64\n",
      " 59   comment_lines_10                        168 non-null    float64\n",
      " 60   comment_ratio_10                        168 non-null    float64\n",
      " 61   h1_10                                   0 non-null      float64\n",
      " 62   h2_10                                   0 non-null      float64\n",
      " 63   N1_10                                   0 non-null      float64\n",
      " 64   N2_10                                   0 non-null      float64\n",
      " 65   vocabulary_10                           0 non-null      float64\n",
      " 66   length_10                               0 non-null      float64\n",
      " 67   volume_10                               0 non-null      float64\n",
      " 68   difficulty_10                           0 non-null      float64\n",
      " 69   effort_10                               0 non-null      float64\n",
      " 70   response_rate_23                        250 non-null    int64  \n",
      " 71   syntactic_correctness_23                168 non-null    float64\n",
      " 72   cyclomatic_complexity_23                168 non-null    float64\n",
      " 73   function_name_consistency_23            168 non-null    float64\n",
      " 74   lines_23                                168 non-null    float64\n",
      " 75   characters_23                           168 non-null    float64\n",
      " 76   comment_lines_23                        168 non-null    float64\n",
      " 77   comment_ratio_23                        168 non-null    float64\n",
      " 78   h1_23                                   0 non-null      float64\n",
      " 79   h2_23                                   0 non-null      float64\n",
      " 80   N1_23                                   0 non-null      float64\n",
      " 81   N2_23                                   0 non-null      float64\n",
      " 82   vocabulary_23                           0 non-null      float64\n",
      " 83   length_23                               0 non-null      float64\n",
      " 84   volume_23                               0 non-null      float64\n",
      " 85   difficulty_23                           0 non-null      float64\n",
      " 86   effort_23                               0 non-null      float64\n",
      " 87   response_rate_orig                      250 non-null    int64  \n",
      " 88   syntactic_correctness_orig              168 non-null    float64\n",
      " 89   cyclomatic_complexity_orig              168 non-null    float64\n",
      " 90   function_name_consistency_orig          168 non-null    float64\n",
      " 91   lines_orig                              168 non-null    float64\n",
      " 92   characters_orig                         168 non-null    float64\n",
      " 93   comment_lines_orig                      168 non-null    float64\n",
      " 94   comment_ratio_orig                      168 non-null    float64\n",
      " 95   h1_orig                                 0 non-null      float64\n",
      " 96   h2_orig                                 0 non-null      float64\n",
      " 97   N1_orig                                 0 non-null      float64\n",
      " 98   N2_orig                                 0 non-null      float64\n",
      " 99   vocabulary_orig                         0 non-null      float64\n",
      " 100  length_orig                             0 non-null      float64\n",
      " 101  volume_orig                             0 non-null      float64\n",
      " 102  difficulty_orig                         0 non-null      float64\n",
      " 103  effort_orig                             0 non-null      float64\n",
      "dtypes: bool(1), float64(69), int64(4), object(30)\n",
      "memory usage: 201.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8beebd-237b-4cef-ad59-1d3f2f9ccb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27b163dc-045d-4e9c-a6ae-92fa4c04e5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting code with comments for layer 1...\n",
      "Extracting code with comments for layer 10...\n",
      "Extracting code with comments for layer 23...\n",
      "Extracting code with comments for layer orig...\n",
      "                                code_with_comments_1\n",
      "0                                               None\n",
      "1                                               None\n",
      "2                                               None\n",
      "3  [    def add_nested_tuples(list1, list2):, def...\n",
      "4  [    def ncr_modp(10,2,13)== \\n            res...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 40 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   task_type                               250 non-null    object \n",
      " 1   task_name                               250 non-null    object \n",
      " 2   subtask_name                            0 non-null      float64\n",
      " 3   input_question                          250 non-null    object \n",
      " 4   input_choice_list                       0 non-null      float64\n",
      " 5   input_final_prompts                     250 non-null    object \n",
      " 6   input_correct_responses                 0 non-null      float64\n",
      " 7   output_prediction_text                  250 non-null    object \n",
      " 8   output_parsed_answer                    250 non-null    object \n",
      " 9   output_choice_completions               0 non-null      float64\n",
      " 10  output_choice_negative_log_likelihoods  0 non-null      float64\n",
      " 11  output_metrics                          250 non-null    object \n",
      " 12  is_correct                              250 non-null    bool   \n",
      " 13  input_question_hash                     250 non-null    object \n",
      " 14  input_final_prompts_hash                250 non-null    object \n",
      " 15  benchmark_label                         250 non-null    object \n",
      " 16  eval_config                             250 non-null    object \n",
      " 17  context                                 250 non-null    object \n",
      " 18  query                                   250 non-null    object \n",
      " 19  extracted_test_cases                    250 non-null    object \n",
      " 20  final_output_1                          169 non-null    object \n",
      " 21  extracted_functions_1                   75 non-null     object \n",
      " 22  trajectories_and_context_1              169 non-null    object \n",
      " 23  context_expansion_1                     169 non-null    object \n",
      " 24  final_output_10                         168 non-null    object \n",
      " 25  extracted_functions_10                  167 non-null    object \n",
      " 26  trajectories_and_context_10             168 non-null    object \n",
      " 27  context_expansion_10                    168 non-null    object \n",
      " 28  final_output_23                         168 non-null    object \n",
      " 29  extracted_functions_23                  168 non-null    object \n",
      " 30  trajectories_and_context_23             168 non-null    object \n",
      " 31  context_expansion_23                    168 non-null    object \n",
      " 32  final_output_orig                       168 non-null    object \n",
      " 33  extracted_functions_orig                168 non-null    object \n",
      " 34  trajectories_and_context_orig           168 non-null    object \n",
      " 35  context_expansion_orig                  168 non-null    object \n",
      " 36  code_with_comments_1                    98 non-null     object \n",
      " 37  code_with_comments_10                   168 non-null    object \n",
      " 38  code_with_comments_23                   168 non-null    object \n",
      " 39  code_with_comments_orig                 168 non-null    object \n",
      "dtypes: bool(1), float64(5), object(34)\n",
      "memory usage: 76.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# def extract_pure_python_code(text):\n",
    "#     # Extract imports\n",
    "#     import_pattern = r\"^(?:from\\s+[\\w.]+\\s+import\\s+(?:[\\w.]+(?:\\s*,\\s*[\\w.]+)*|\\*)|import\\s+(?:[\\w.]+(?:\\s*,\\s*[\\w.]+)*))(?:\\s+as\\s+[\\w.]+)?\"\n",
    "#     imports = re.findall(import_pattern, text, re.MULTILINE)\n",
    "\n",
    "#     # Extract functions\n",
    "#     function_pattern = r\"(def\\s+\\w+\\s*\\(.*?\\):(?:\\s*['\\\"][\\s\\S]*?['\\\"])?\\s*(?:(?!def\\s)[\\s\\S])*?(?=\\ndef|\\Z))\"\n",
    "#     function_pattern = r\"(def\\s+\\w+\\s*\\([^)]*\\):\\s*(?:(?:'''[\\s\\S]*?'''|\\\"\\\"\\\"[\\s\\S]*?\\\"\\\"\\\")?\\s*[\\s\\S]*?)(?=\\ndef|\\Z))\"\n",
    "#     function_pattern = r\"(def\\s+\\w+\\s*\\([^)]*\\):(?:\\s*(?:\\\"\\\"\\\"[\\s\\S]*?\\\"\\\"\\\"|'''.*?''')?\\s*)?[\\s\\S]*?(?=\\ndef\\s|\\Z))\"\n",
    "#     function_pattern = '# MODE:\\n(.*?)\\}'\n",
    "#     # function_pattern = r\"def\\s+\\w+\\s*\\([^)]*\\):\\s*(?:(?:'''[\\s\\S]*?'''|\\\"\\\"\\\"[\\s\\S]*?\\\"\\\"\\\")?\\s*[\\s\\S]*?(?=\\ndef|\\Z))\"\n",
    "\n",
    "#     functions = re.findall(function_pattern, text, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "#     def clean_code(code):\n",
    "#         # Remove docstrings\n",
    "#         # code = re.sub(r'\"\"\"[\\\\s\\\\S]*?\"\"\"|\\'\\'\\'[\\\\\\s\\\\S]*?\\'\\'\\'|\\'''[\\\\s\\\\S]*?\\'''', \"\", code)\n",
    "#         code = re.sub(r'\"\"\"[\\\\s\\\\S]*?\"\"\"|\\'\\'\\'[\\\\s\\\\S]*?\\'\\'\\'', \"\", code)\n",
    "\n",
    "#         # Remove comments\n",
    "#         code = re.sub(r\"#.*\", \"\", code)\n",
    "#         # Remove empty lines and trailing whitespace\n",
    "#         code = \"\\n\".join(line for line in code.splitlines() if line.strip())\n",
    "#         return code\n",
    "\n",
    "#     cleaned_imports = [clean_code(imp) for imp in imports]\n",
    "#     cleaned_functions = [clean_code(func) for func in functions]\n",
    "\n",
    "#     # Combine imports and functions\n",
    "#     cleaned_code = \"\\n\".join(cleaned_imports)\n",
    "#     if cleaned_imports and cleaned_functions:\n",
    "#         cleaned_code += \"\\n\\n\"\n",
    "#     cleaned_code += \"\\n\\n\".join(cleaned_functions)\n",
    "\n",
    "#     return cleaned_code\n",
    "\n",
    "def extract_pure_python_code(text):\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    functions = []\n",
    "    current_function = []\n",
    "    indent_level = 0\n",
    "    in_function = False\n",
    "    if not text.__contains__('def'):\n",
    "        return None\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        # Check for function or class definition\n",
    "        if re.match(r'^(def|class)\\s', stripped_line):\n",
    "            if in_function:\n",
    "                # Save the previous function if we're starting a new one\n",
    "                functions.append('\\n'.join(current_function))\n",
    "                current_function = []\n",
    "            in_function = True\n",
    "            indent_level = len(line) - len(line.lstrip())\n",
    "            current_function.append(line)\n",
    "        elif in_function:\n",
    "            # Check if this line is part of the current function\n",
    "            if line.strip() == '' or len(line) - len(line.lstrip()) > indent_level:\n",
    "                current_function.append(line)\n",
    "            else:\n",
    "                # This line is not part of the function, so the function has ended\n",
    "                functions.append('\\n'.join(current_function))\n",
    "                current_function = []\n",
    "                in_function = False\n",
    "                if re.match(r'^(def|class)\\s', stripped_line):\n",
    "                    # If this is a new function definition, start a new function\n",
    "                    in_function = True\n",
    "                    indent_level = len(line) - len(line.lstrip())\n",
    "                    current_function.append(line)\n",
    "\n",
    "    \n",
    "    # Add the last function if there is one\n",
    "    if current_function:\n",
    "        functions.append('\\n'.join(current_function))\n",
    "\n",
    "    return functions\n",
    "\n",
    "# def extract_python_code_with_comments(text):\n",
    "#     if pd.isna(text):\n",
    "#         return None\n",
    "    \n",
    "#     # Regex pattern to match Python code blocks\n",
    "#     pattern = r'```(?:python)?\\s*\\n((?:(?!```).|\\n)+)\\s*```'\n",
    "#     matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "#     if not matches:\n",
    "#         print(\"not matches with comments\")\n",
    "#         # Fallback pattern for code not wrapped in backticks\n",
    "#         pattern = r'(?:^|\\n)((?:import\\s+\\w+|from\\s+\\w+\\s+import\\s+[\\w,\\s]+|def\\s+\\w+\\s*\\(|class\\s+\\w+[^\\n]*:)(?:\\n(?!```|import\\s+\\w+|from\\s+\\w+\\s+import|def\\s+\\w+\\s*\\(|class\\s+\\w+).*)*)'\n",
    "#         matches = re.findall(pattern, text, re.MULTILINE)\n",
    "    \n",
    "#     if not matches:\n",
    "#         return None\n",
    "    \n",
    "#     return '\\n\\n'.join(matches)\n",
    "\n",
    "\n",
    "def process_dataframe(df):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    layer_indices = ['1', '10', '23', 'orig']\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        column_name = f'final_output_{layer_idx}'\n",
    "        if column_name in new_df.columns:\n",
    "            print(f\"Extracting code with comments for layer {layer_idx}...\")\n",
    "            new_df[f'code_with_comments_{layer_idx}'] = new_df[column_name].apply(\n",
    "                lambda x: extract_pure_python_code(x) if pd.notna(x) else None\n",
    "            )\n",
    "            \n",
    "            # print(f\"Extracting pure code for layer {layer_idx}...\")\n",
    "            # new_df[f'pure_code_{layer_idx}'] = new_df[column_name].apply(\n",
    "            #     lambda x: extract_pure_python_code(x, clean=True) if pd.notna(x) else []\n",
    "            # )\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "code_comments_df = process_dataframe(df)\n",
    "print(code_comments_df[['code_with_comments_1']].head())\n",
    "print(code_comments_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ba59a8bc-fd56-4a3c-8fad-b585ccbd9293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['task_type', 'task_name', 'subtask_name', 'input_question',\n",
      "       'input_choice_list', 'input_final_prompts', 'input_correct_responses',\n",
      "       'output_prediction_text', 'output_parsed_answer',\n",
      "       'output_choice_completions',\n",
      "       ...\n",
      "       'comment_ratio_orig', 'h1_orig', 'h2_orig', 'N1_orig', 'N2_orig',\n",
      "       'vocabulary_orig', 'length_orig', 'volume_orig', 'difficulty_orig',\n",
      "       'effort_orig'],\n",
      "      dtype='object', length=108)\n"
     ]
    }
   ],
   "source": [
    "# def code_length(func: str) -> dict:\n",
    "#     lines = func.split(\"\\n\")\n",
    "#     return {\"lines\": len(lines), \"characters\": len(func)}\n",
    "\n",
    "def code_length(func):\n",
    "    if isinstance(func, list):\n",
    "        # Combine all functions into a single string\n",
    "        combined_func = '\\n'.join(func)\n",
    "        lines = [line.strip() for line in combined_func.split('\\n') if line.strip()]\n",
    "        return {\n",
    "            \"lines\": len(lines),\n",
    "            \"characters\": sum(len(line) for line in lines)\n",
    "        }\n",
    "    elif isinstance(func, str):\n",
    "        lines = [line.strip() for line in func.split('\\n') if line.strip()]\n",
    "        return {\n",
    "            \"lines\": len(lines),\n",
    "            \"characters\": sum(len(line) for line in lines)\n",
    "        }\n",
    "    else:\n",
    "        return {\"lines\": 0, \"characters\": 0}\n",
    "\n",
    "\n",
    "def comment_analysis(func: str) -> dict:\n",
    "    lines = func.split(\"\\n\")\n",
    "    comment_lines = sum(1 for line in lines if line.strip().startswith(\"#\"))\n",
    "    return {\n",
    "        \"comment_lines\": comment_lines,\n",
    "        \"comment_ratio\": comment_lines / len(lines) if lines else 0,\n",
    "    }\n",
    "\n",
    "def cyclomatic_complexity(func: str) -> int:\n",
    "    try:\n",
    "        return max((cc.complexity for cc in cc_visit(func)), default=0)\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "def syntactic_correctness(func: str) -> bool:\n",
    "    try:\n",
    "        ast.parse(func)\n",
    "        return True\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "        \n",
    "def extract_function_name(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    # pattern = r\"(?:def|class|assert)\\s+([\\w\\d_]+)\\s*\\(\"\n",
    "    pattern = r\"(?:def|class|assert)\\s+([\\w\\d_]+)\\s*(?:\\(|:)\"\n",
    "\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group(1) if match else None\n",
    "def check_function_name_consistency(test_cases: str, generated_function: str) -> float:\n",
    "    test_func_name = extract_function_name(test_cases)\n",
    "    generated_func_name = extract_function_name(generated_function)\n",
    "\n",
    "    if test_func_name and generated_func_name:\n",
    "        if test_func_name == generated_func_name:\n",
    "            return float(test_func_name == generated_func_name)\n",
    "            # print(\"yes equal: \", test_func_name, generated_func_name)\n",
    "        else:\n",
    "            if generated_function.lower().__contains__(test_func_name):\n",
    "                return float(generated_function.lower().__contains__(test_func_name))\n",
    "            # print(f\"not equal: tests; {test_func_name}\\ngens: {generated_func_name}, \\nbut is contained? {float(generated_function.lower().__contains__(test_func_name))}\")\n",
    "        \n",
    "        return float(test_func_name == generated_func_name)\n",
    "\n",
    "    if generated_function.lower().__contains__(test_func_name.lower()):\n",
    "        return float(generated_function.lower().__contains__(test_func_name))\n",
    "        # print(f\"not equal: tests; {test_func_name}\\ngens: {generated_func_name}, \\nbut is contained? {float(generated_function.lower().__contains__(test_func_name))}\")\n",
    "        \n",
    "    # print(\"were not equal: \", test_func_name, generated_func_name)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "\n",
    "import textwrap\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "\n",
    "class RobustFunctionParser:\n",
    "    def __init__(self, code: str):\n",
    "        self.original_code = code\n",
    "        self.cleaned_code = self.preprocess(code)\n",
    "        self.tokens = self.tokenize(self.cleaned_code)\n",
    "        self.metrics = self.calculate_metrics()\n",
    "\n",
    "    def preprocess(self, code: str) -> str:\n",
    "        lines = code.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # Remove comments\n",
    "            line = re.sub(r'#.*$', '', line)\n",
    "            # Remove leading/trailing whitespace\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                cleaned_lines.append(line)\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "\n",
    "    def tokenize(self, code: str) -> List[str]:\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', code)\n",
    "\n",
    "    def calculate_metrics(self) -> Dict[str, Any]:\n",
    "        metrics = {\n",
    "            'h1': 0,  # number of distinct operators\n",
    "            'h2': 0,  # number of distinct operands\n",
    "            'N1': 0,  # total number of operators\n",
    "            'N2': 0,  # total number of operands\n",
    "            'vocabulary': 0,\n",
    "            'length': len(self.tokens),\n",
    "            'volume': 0,\n",
    "            'difficulty': 0,\n",
    "            'effort': 0\n",
    "        }\n",
    "\n",
    "        operators = set()\n",
    "        operands = set()\n",
    "\n",
    "        for token in self.tokens:\n",
    "            if token in {'+', '-', '*', '/', '%', '=', '==', '!=', '<', '>', '<=', '>=', 'and', 'or', 'not', 'in', 'is'}:\n",
    "                operators.add(token)\n",
    "                metrics['N1'] += 1\n",
    "            elif not token.isdigit() and token not in {'def', 'class', 'return', 'if', 'else', 'elif', 'while', 'for', 'in', 'try', 'except', 'finally'}:\n",
    "                operands.add(token)\n",
    "                metrics['N2'] += 1\n",
    "\n",
    "        metrics['h1'] = len(operators)\n",
    "        metrics['h2'] = len(operands)\n",
    "        metrics['vocabulary'] = metrics['h1'] + metrics['h2']\n",
    "        \n",
    "        if metrics['vocabulary'] > 0:\n",
    "            metrics['volume'] = metrics['length'] * np.log2(metrics['vocabulary'])\n",
    "        \n",
    "        if metrics['h2'] > 0:\n",
    "            metrics['difficulty'] = (metrics['h1'] / 2) * (metrics['N2'] / metrics['h2'])\n",
    "        \n",
    "        metrics['effort'] = metrics['difficulty'] * metrics['volume']\n",
    "\n",
    "        return metrics\n",
    "\n",
    "def safe_calculate_metrics(func: str) -> Dict[str, float]:\n",
    "    try:\n",
    "        parser = RobustFunctionParser(func)\n",
    "        return parser.metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error in metric calculation: {e}\")\n",
    "        return {k: np.nan for k in ['h1', 'h2', 'N1', 'N2', 'vocabulary', 'length', 'volume', 'difficulty', 'effort']}\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(row: pd.Series, layer_idx: str) -> dict:\n",
    "    # func = row[f\"extracted_functions_{layer_idx}\"]\n",
    "    func = row[f\"final_output_{layer_idx}\"]\n",
    "    func = row[f\"code_with_comments_{layer_idx}\"]\n",
    "    # if func:\n",
    "        # print(\"func: \", func)\n",
    "        # print(\"func: \", row[\"extracted_test_cases\"])\n",
    "        # print(\"=\" * 100)\n",
    "    # if isinstance(func, list):\n",
    "    #     print(\"func is a list\", func)\n",
    "    if func is None or (isinstance(func, list) and len(func) == 0):\n",
    "        return {\n",
    "            f\"{metric}_{layer_idx}\": np.nan\n",
    "            for metric in [\n",
    "                \"syntactic_correctness\",\n",
    "                \"cyclomatic_complexity\",\n",
    "                \"function_name_consistency\",\n",
    "                \"lines\",\n",
    "                \"characters\",\n",
    "                \"comment_lines\",\n",
    "                \"comment_ratio\",\n",
    "                \"h1\",\n",
    "                \"h2\",\n",
    "                \"N1\",\n",
    "                \"N2\",\n",
    "                \"vocabulary\",\n",
    "                \"length\",\n",
    "                \"volume\",\n",
    "                \"difficulty\",\n",
    "                \"effort\",\n",
    "            ]\n",
    "        }\n",
    "    # print(\n",
    "    #     f\"for layer idx {layer_idx}: \",\n",
    "    #     check_function_name_consistency(row[\"extracted_test_cases\"], func),\n",
    "    # )\n",
    "    # def cyclomatic_list_check(func):\n",
    "        \n",
    "    \n",
    "    metrics = {\n",
    "        # f\"syntactic_correctness_{layer_idx}\": int(syntactic_correctness(func)),\n",
    "        f\"syntactic_correctness_{layer_idx}\": int(any(syntactic_correctness(f) for f in func) if isinstance(func, list) else syntactic_correctness(func)) if func else np.nan,\n",
    "        # f\"cyclomatic_complexity_{layer_idx}\": cyclomatic_complexity(func) if func is not None else np.nan,\n",
    "        # f\"cyclomatic_complexity_{layer_idx}\": func.apply(lambda x: cyclomatic_complexity(x) if x is not None else np.nan),\n",
    "        f\"cyclomatic_complexity_{layer_idx}\": max((cyclomatic_complexity(f) for f in func), default=np.nan) if isinstance(func, list) else cyclomatic_complexity(func) if func is not None else np.nan,\n",
    "\n",
    "        # f\"function_name_consistency_{layer_idx}\": check_function_name_consistency(\n",
    "        #     row[\"extracted_test_cases\"], func\n",
    "        # ),\n",
    "        f\"function_name_consistency_{layer_idx}\": max((check_function_name_consistency(row[\"extracted_test_cases\"], f) for f in func), default=np.nan) if isinstance(func, list) else check_function_name_consistency(row[\"extracted_test_cases\"], func) if func is not None else np.nan,\n",
    "\n",
    "    }\n",
    "\n",
    "    # length_metrics = code_length(func)\n",
    "    # if isinstance(func, list):\n",
    "    #     length_metrics = {k: sum(v for f in func for k, v in code_length(f).items()) for k in ['lines', 'characters']}\n",
    "    # else:\n",
    "    #     length_metrics = code_length(func)\n",
    "    # metrics.update({f\"{k}_{layer_idx}\": v for k, v in length_metrics.items()})\n",
    "    length_metrics = code_length(func)\n",
    "    metrics.update({f\"{k}_{layer_idx}\": v for k, v in length_metrics.items()})\n",
    "\n",
    "    # comment_metrics = comment_analysis(func)\n",
    "    if isinstance(func, list):\n",
    "        comment_metrics = {\n",
    "            'comment_lines': sum(comment_analysis(f)['comment_lines'] for f in func),\n",
    "            'comment_ratio': max((comment_analysis(f)['comment_ratio'] for f in func), default=np.nan)\n",
    "        }\n",
    "    else:\n",
    "        comment_metrics = comment_analysis(func)\n",
    "\n",
    "    metrics.update({f\"{k}_{layer_idx}\": v for k, v in comment_metrics.items()})\n",
    "\n",
    "    \n",
    "    # Handle metrics calculation\n",
    "    h_keys = ['h1', 'h2', 'N1', 'N2', 'vocabulary', 'length', 'volume', 'difficulty', 'effort']\n",
    "    if isinstance(func, list):\n",
    "        h_metrics_list = [safe_calculate_metrics(f) for f in func if f]\n",
    "        \n",
    "        if h_metrics_list:\n",
    "            h_metrics = {k: max(hm.get(k, np.nan) for hm in h_metrics_list) for k in h_keys}\n",
    "        else:\n",
    "            h_metrics = {k: np.nan for k in h_keys}\n",
    "    else:\n",
    "        h_metrics = safe_calculate_metrics(func)\n",
    "\n",
    "    metrics.update({f\"{k}_{layer_idx}\": v for k, v in h_metrics.items()})\n",
    "\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_df = df.copy()\n",
    "\n",
    "    for layer_idx in [\"1\", \"10\", \"23\", \"orig\"]:\n",
    "        # new_df[f\"response_rate_{layer_idx}\"] = (\n",
    "        #     new_df[f\"code_with_comments_{layer_idx}\"].notna()\n",
    "        # )\n",
    "        new_df[f\"response_rate_{layer_idx}\"] = new_df[f\"code_with_comments_{layer_idx}\"].apply(lambda x: len(x) > 0 if isinstance(x, list) else None)\n",
    "\n",
    "        metrics = new_df.apply(lambda row: calculate_metrics(row, layer_idx), axis=1)\n",
    "        # metrics = pd.DataFrame(\n",
    "        #     [calculate_metrics(row, layer_idx) for _, row in new_df.iterrows()]\n",
    "        # )\n",
    "        for metric in metrics.iloc[0].keys():\n",
    "            new_df[metric] = metrics.apply(lambda x: x[metric])\n",
    "\n",
    "    return new_df\n",
    "metrics_df = process_dataframe(code_comments_df)\n",
    "print(str(metrics_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "63d331e4-d546-44e8-b1a7-ed2c196bc462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 108 columns):\n",
      " #    Column                                  Non-Null Count  Dtype  \n",
      "---   ------                                  --------------  -----  \n",
      " 0    task_type                               250 non-null    object \n",
      " 1    task_name                               250 non-null    object \n",
      " 2    subtask_name                            0 non-null      float64\n",
      " 3    input_question                          250 non-null    object \n",
      " 4    input_choice_list                       0 non-null      float64\n",
      " 5    input_final_prompts                     250 non-null    object \n",
      " 6    input_correct_responses                 0 non-null      float64\n",
      " 7    output_prediction_text                  250 non-null    object \n",
      " 8    output_parsed_answer                    250 non-null    object \n",
      " 9    output_choice_completions               0 non-null      float64\n",
      " 10   output_choice_negative_log_likelihoods  0 non-null      float64\n",
      " 11   output_metrics                          250 non-null    object \n",
      " 12   is_correct                              250 non-null    bool   \n",
      " 13   input_question_hash                     250 non-null    object \n",
      " 14   input_final_prompts_hash                250 non-null    object \n",
      " 15   benchmark_label                         250 non-null    object \n",
      " 16   eval_config                             250 non-null    object \n",
      " 17   context                                 250 non-null    object \n",
      " 18   query                                   250 non-null    object \n",
      " 19   extracted_test_cases                    250 non-null    object \n",
      " 20   final_output_1                          169 non-null    object \n",
      " 21   extracted_functions_1                   75 non-null     object \n",
      " 22   trajectories_and_context_1              169 non-null    object \n",
      " 23   context_expansion_1                     169 non-null    object \n",
      " 24   final_output_10                         168 non-null    object \n",
      " 25   extracted_functions_10                  167 non-null    object \n",
      " 26   trajectories_and_context_10             168 non-null    object \n",
      " 27   context_expansion_10                    168 non-null    object \n",
      " 28   final_output_23                         168 non-null    object \n",
      " 29   extracted_functions_23                  168 non-null    object \n",
      " 30   trajectories_and_context_23             168 non-null    object \n",
      " 31   context_expansion_23                    168 non-null    object \n",
      " 32   final_output_orig                       168 non-null    object \n",
      " 33   extracted_functions_orig                168 non-null    object \n",
      " 34   trajectories_and_context_orig           168 non-null    object \n",
      " 35   context_expansion_orig                  168 non-null    object \n",
      " 36   code_with_comments_1                    98 non-null     object \n",
      " 37   code_with_comments_10                   168 non-null    object \n",
      " 38   code_with_comments_23                   168 non-null    object \n",
      " 39   code_with_comments_orig                 168 non-null    object \n",
      " 40   response_rate_1                         98 non-null     object \n",
      " 41   syntactic_correctness_1                 94 non-null     float64\n",
      " 42   cyclomatic_complexity_1                 94 non-null     float64\n",
      " 43   function_name_consistency_1             94 non-null     float64\n",
      " 44   lines_1                                 94 non-null     float64\n",
      " 45   characters_1                            94 non-null     float64\n",
      " 46   comment_lines_1                         94 non-null     float64\n",
      " 47   comment_ratio_1                         94 non-null     float64\n",
      " 48   h1_1                                    94 non-null     float64\n",
      " 49   h2_1                                    94 non-null     float64\n",
      " 50   N1_1                                    94 non-null     float64\n",
      " 51   N2_1                                    94 non-null     float64\n",
      " 52   vocabulary_1                            94 non-null     float64\n",
      " 53   length_1                                94 non-null     float64\n",
      " 54   volume_1                                94 non-null     float64\n",
      " 55   difficulty_1                            94 non-null     float64\n",
      " 56   effort_1                                94 non-null     float64\n",
      " 57   response_rate_10                        168 non-null    object \n",
      " 58   syntactic_correctness_10                168 non-null    float64\n",
      " 59   cyclomatic_complexity_10                168 non-null    float64\n",
      " 60   function_name_consistency_10            168 non-null    float64\n",
      " 61   lines_10                                168 non-null    float64\n",
      " 62   characters_10                           168 non-null    float64\n",
      " 63   comment_lines_10                        168 non-null    float64\n",
      " 64   comment_ratio_10                        168 non-null    float64\n",
      " 65   h1_10                                   168 non-null    float64\n",
      " 66   h2_10                                   168 non-null    float64\n",
      " 67   N1_10                                   168 non-null    float64\n",
      " 68   N2_10                                   168 non-null    float64\n",
      " 69   vocabulary_10                           168 non-null    float64\n",
      " 70   length_10                               168 non-null    float64\n",
      " 71   volume_10                               168 non-null    float64\n",
      " 72   difficulty_10                           168 non-null    float64\n",
      " 73   effort_10                               168 non-null    float64\n",
      " 74   response_rate_23                        168 non-null    object \n",
      " 75   syntactic_correctness_23                168 non-null    float64\n",
      " 76   cyclomatic_complexity_23                168 non-null    float64\n",
      " 77   function_name_consistency_23            168 non-null    float64\n",
      " 78   lines_23                                168 non-null    float64\n",
      " 79   characters_23                           168 non-null    float64\n",
      " 80   comment_lines_23                        168 non-null    float64\n",
      " 81   comment_ratio_23                        168 non-null    float64\n",
      " 82   h1_23                                   168 non-null    float64\n",
      " 83   h2_23                                   168 non-null    float64\n",
      " 84   N1_23                                   168 non-null    float64\n",
      " 85   N2_23                                   168 non-null    float64\n",
      " 86   vocabulary_23                           168 non-null    float64\n",
      " 87   length_23                               168 non-null    float64\n",
      " 88   volume_23                               168 non-null    float64\n",
      " 89   difficulty_23                           168 non-null    float64\n",
      " 90   effort_23                               168 non-null    float64\n",
      " 91   response_rate_orig                      168 non-null    object \n",
      " 92   syntactic_correctness_orig              168 non-null    float64\n",
      " 93   cyclomatic_complexity_orig              168 non-null    float64\n",
      " 94   function_name_consistency_orig          168 non-null    float64\n",
      " 95   lines_orig                              168 non-null    float64\n",
      " 96   characters_orig                         168 non-null    float64\n",
      " 97   comment_lines_orig                      168 non-null    float64\n",
      " 98   comment_ratio_orig                      168 non-null    float64\n",
      " 99   h1_orig                                 168 non-null    float64\n",
      " 100  h2_orig                                 168 non-null    float64\n",
      " 101  N1_orig                                 168 non-null    float64\n",
      " 102  N2_orig                                 168 non-null    float64\n",
      " 103  vocabulary_orig                         168 non-null    float64\n",
      " 104  length_orig                             168 non-null    float64\n",
      " 105  volume_orig                             168 non-null    float64\n",
      " 106  difficulty_orig                         168 non-null    float64\n",
      " 107  effort_orig                             168 non-null    float64\n",
      "dtypes: bool(1), float64(69), object(38)\n",
      "memory usage: 209.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"structured_checkpoint_data.csv\")\n",
    "# dfnew = pd.read_csv(\"codellm_metrics.csv\")\n",
    "\n",
    "# Save the new DataFrame\n",
    "metrics_df.to_csv(\"codellm_metrics_notebook_v.csv\", index=False)\n",
    "print(metrics_df.info(max_cols=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0de682cb-f151-424e-973f-43710139c5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Columns: 108 entries, task_type to effort_orig\n",
      "dtypes: bool(1), float64(69), object(38)\n",
      "memory usage: 209.4+ KB\n",
      "None\n",
      "98\n",
      "Total entries in response_rate_1: 98\n",
      "168\n",
      "Total entries in response_rate_10: 168\n",
      "168\n",
      "Total entries in response_rate_23: 168\n",
      "168\n",
      "Total entries in response_rate_orig: 168\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Response Rates Across Different Layers}\n",
      "\\label{tab:response_rates}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Metric & Layer 1 & Layer 10 & Layer 23 & Original \\\\\n",
      "\\hline\n",
      "Response Rate & 0.5799 & 0.9941 & 0.9941 & 0.9941 \\\\\n",
      "Sample Size & 169.0000 & 169.0000 & 169.0000 & 169.0000 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrame\n",
    "# metrics_df = pd.read_csv(\"codellm_metrics_notebook_v.csv\")\n",
    "print(metrics_df.info())\n",
    "gen_list_len = 169\n",
    "\n",
    "def generate_latex_table(data, caption, label):\n",
    "    latex = f\"\\\\begin{{table}}[h]\\n\\\\centering\\n\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\\n\"\n",
    "    latex += \"\\\\begin{tabular}{l\" + \"c\" * len(data.columns) + \"}\\n\\\\hline\\n\"\n",
    "    latex += \"Metric & \" + \" & \".join(data.columns) + \" \\\\\\\\\\n\\\\hline\\n\"\n",
    "    for index, row in data.iterrows():\n",
    "        latex += (\n",
    "            f\"{index} & \"\n",
    "            + \" & \".join([f\"{x:.4f}\" if isinstance(x, float) else str(x) for x in row])\n",
    "            + \" \\\\\\\\\\n\"\n",
    "        )\n",
    "    latex += \"\\\\hline\\n\\\\end{tabular}\\n\\\\end{table}\\n\"\n",
    "    return latex\n",
    "\n",
    "response_rates = {}\n",
    "\n",
    "for layer_idx in [\"1\", \"10\", \"23\", \"orig\"]:\n",
    "    column_name = f\"response_rate_{layer_idx}\"\n",
    "    print(metrics_df[column_name].notna().sum())\n",
    "    # Count non-null values that are lists with at least one element\n",
    "    non_null_count = metrics_df[column_name].notna().sum()\n",
    "    \n",
    "    # Print the total number of entries in the column\n",
    "    print(f\"Total entries in {column_name}: {non_null_count}\")\n",
    "    \n",
    "    # Calculate the response rate\n",
    "    response_rates[column_name] = non_null_count / gen_list_len if gen_list_len > 0 else 0\n",
    "\n",
    "\n",
    "response_rates_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Layer 1\": [response_rates[\"response_rate_1\"], gen_list_len],\n",
    "        \"Layer 10\": [response_rates[\"response_rate_10\"], gen_list_len],\n",
    "        \"Layer 23\": [response_rates[\"response_rate_23\"], gen_list_len],\n",
    "        \"Original\": [response_rates[\"response_rate_orig\"], gen_list_len],\n",
    "    },\n",
    "    index=[\"Response Rate\", \"Sample Size\"],\n",
    ")\n",
    "response_rates_latex = generate_latex_table(\n",
    "    response_rates_data, \"Response Rates Across Different Layers\", \"tab:response_rates\"\n",
    ")\n",
    "\n",
    "print(response_rates_latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7e2d4af5-7684-40c8-87c2-3bb8ad1761ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Code Quality Metrics Across Different Layers}\n",
      "\\label{tab:code_quality}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Metric & Layer 1 & Layer 10 & Layer 23 & Original \\\\\n",
      "\\hline\n",
      "Syntactic Correctness & 0.1915 & 0.8690 & 0.9762 & 0.9762 \\\\\n",
      "Function Name Consistency & 0.9574 & 1.0000 & 0.9821 & 0.9940 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code Quality Metrics\n",
    "code_quality_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Layer 1\": [\n",
    "            metrics_df[\"syntactic_correctness_1\"].mean(),\n",
    "            metrics_df[\"function_name_consistency_1\"].mean(),\n",
    "        ],\n",
    "        \"Layer 10\": [\n",
    "            metrics_df[\"syntactic_correctness_10\"].mean(),\n",
    "            metrics_df[\"function_name_consistency_10\"].mean(),\n",
    "        ],\n",
    "        \"Layer 23\": [\n",
    "            metrics_df[\"syntactic_correctness_23\"].mean(),\n",
    "            metrics_df[\"function_name_consistency_23\"].mean(),\n",
    "        ],\n",
    "        \"Original\": [\n",
    "            metrics_df[\"syntactic_correctness_orig\"].mean(),\n",
    "            metrics_df[\"function_name_consistency_orig\"].mean(),\n",
    "        ],\n",
    "    },\n",
    "    index=[\"Syntactic Correctness\", \"Function Name Consistency\"],\n",
    ")\n",
    "code_quality_latex = generate_latex_table(\n",
    "    code_quality_data,\n",
    "    \"Code Quality Metrics Across Different Layers\",\n",
    "    \"tab:code_quality\",\n",
    ")\n",
    "\n",
    "print(code_quality_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0ce3413a-f6c8-47d7-8566-20827c738f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Code Complexity Metrics Across Different Layers}\n",
      "\\label{tab:code_complexity}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Metric & Layer 1 & Layer 10 & Layer 23 & Original \\\\\n",
      "\\hline\n",
      "Cyclomatic Complexity & 0.4468 & 2.0714 & 2.5952 & 2.3571 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Code Structure Metrics Across Different Layers}\n",
      "\\label{tab:code_structure}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Metric & Layer 1 & Layer 10 & Layer 23 & Original \\\\\n",
      "\\hline\n",
      "Avg Lines of Code & 8.2234 & 11.5893 & 6.0952 & 15.7262 \\\\\n",
      "Avg No. Characters & 217.1596 & 335.9464 & 198.0119 & 535.0417 \\\\\n",
      "Avg Comment Lines & 0.0745 & 1.2262 & 0.3750 & 3.0714 \\\\\n",
      "Avg Comment Ratio & 0.0140 & 0.0852 & 0.0256 & 0.1295 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Code Complexity Metrics\n",
    "code_complexity_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Layer 1\": [metrics_df[\"cyclomatic_complexity_1\"].mean()],\n",
    "        \"Layer 10\": [metrics_df[\"cyclomatic_complexity_10\"].mean()],\n",
    "        \"Layer 23\": [metrics_df[\"cyclomatic_complexity_23\"].mean()],\n",
    "        \"Original\": [metrics_df[\"cyclomatic_complexity_orig\"].mean()],\n",
    "    },\n",
    "    index=[\"Cyclomatic Complexity\"],\n",
    ")\n",
    "code_complexity_latex = generate_latex_table(\n",
    "    code_complexity_data,\n",
    "    \"Code Complexity Metrics Across Different Layers\",\n",
    "    \"tab:code_complexity\",\n",
    ")\n",
    "\n",
    "# Code Structure Metrics\n",
    "code_structure_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Layer 1\": [\n",
    "            metrics_df[\"lines_1\"].mean(),\n",
    "            metrics_df[\"characters_1\"].mean(),\n",
    "            metrics_df[\"comment_lines_1\"].mean(),\n",
    "            metrics_df[\"comment_ratio_1\"].mean(),\n",
    "        ],\n",
    "        \"Layer 10\": [\n",
    "            metrics_df[\"lines_10\"].mean(),\n",
    "            metrics_df[\"characters_10\"].mean(),\n",
    "            metrics_df[\"comment_lines_10\"].mean(),\n",
    "            metrics_df[\"comment_ratio_10\"].mean(),\n",
    "        ],\n",
    "        \"Layer 23\": [\n",
    "            metrics_df[\"lines_23\"].mean(),\n",
    "            metrics_df[\"characters_23\"].mean(),\n",
    "            metrics_df[\"comment_lines_23\"].mean(),\n",
    "            metrics_df[\"comment_ratio_23\"].mean(),\n",
    "        ],\n",
    "        \"Original\": [\n",
    "            metrics_df[\"lines_orig\"].mean(),\n",
    "            metrics_df[\"characters_orig\"].mean(),\n",
    "            metrics_df[\"comment_lines_orig\"].mean(),\n",
    "            metrics_df[\"comment_ratio_orig\"].mean(),\n",
    "        ],\n",
    "    },\n",
    "    index=[\"Avg Lines of Code\", \"Avg No. Characters\", \"Avg Comment Lines\", \"Avg Comment Ratio\"],\n",
    ")\n",
    "code_structure_latex = generate_latex_table(\n",
    "    code_structure_data,\n",
    "    \"Code Structure Metrics Across Different Layers\",\n",
    "    \"tab:code_structure\",\n",
    ")\n",
    "\n",
    "# Print LaTeX code for all tables\n",
    "\n",
    "print(code_complexity_latex)\n",
    "print(code_structure_latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "49c7192d-9f9d-413a-8835-9556656e0a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Basic Halstead Metrics Across Different Layers}\n",
      "\\label{tab:basic_halstead}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Metric & Layer 1 & Layer 10 & Layer 23 & Layer orig \\\\\n",
      "\\hline\n",
      "h1 (Distinct Operators) & 2.0319 & 3.0357 & 2.8036 & 3.7143 \\\\\n",
      "h2 (Distinct Operands) & 11.3511 & 20.6726 & 17.1250 & 32.7917 \\\\\n",
      "N1 (Total Operators) & 4.9894 & 6.5774 & 6.2500 & 7.6786 \\\\\n",
      "N2 (Total Operands) & 30.2128 & 49.1786 & 42.2321 & 83.3274 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Derived Halstead Metrics Across Different Layers}\n",
      "\\label{tab:derived_halstead}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Metric & Layer 1 & Layer 10 & Layer 23 & Layer orig \\\\\n",
      "\\hline\n",
      "Vocabulary & 13.3511 & 23.5655 & 19.9286 & 36.5000 \\\\\n",
      "Length & 43.7660 & 61.7381 & 54.9464 & 97.1905 \\\\\n",
      "Volume & 170.4025 & 291.6987 & 252.0961 & 511.6619 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Complexity Halstead Metrics Across Different Layers}\n",
      "\\label{tab:complexity_halstead}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Metric & Layer 1 & Layer 10 & Layer 23 & Layer orig \\\\\n",
      "\\hline\n",
      "Difficulty & 2.6944 & 3.8036 & 3.7718 & 4.8205 \\\\\n",
      "Effort & 732.6550 & 1431.2053 & 1712.3707 & 2925.4783 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Summary of Key Halstead Metrics Across Different Layers}\n",
      "\\label{tab:summary_halstead}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\hline\n",
      "Metric & Layer 1 & Layer 10 & Layer 23 & Layer orig \\\\\n",
      "\\hline\n",
      "Vocabulary & 13.3511 & 23.5655 & 19.9286 & 36.5000 \\\\\n",
      "Volume & 170.4025 & 291.6987 & 252.0961 & 511.6619 \\\\\n",
      "Difficulty & 2.6944 & 3.8036 & 3.7718 & 4.8205 \\\\\n",
      "Effort & 732.6550 & 1431.2053 & 1712.3707 & 2925.4783 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [\"1\", \"10\", \"23\", \"orig\"]\n",
    "halstead_metrics = ['h1', 'h2', 'N1', 'N2', 'vocabulary', 'length', 'volume', 'difficulty', 'effort']\n",
    "\n",
    "# Create DataFrames for different groups of metrics\n",
    "basic_halstead_df = pd.DataFrame({\n",
    "    f\"Layer {layer}\": [metrics_df[f\"{metric}_{layer}\"].mean() for metric in ['h1', 'h2', 'N1', 'N2']]\n",
    "    for layer in layers\n",
    "}, index=['h1 (Distinct Operators)', 'h2 (Distinct Operands)', 'N1 (Total Operators)', 'N2 (Total Operands)'])\n",
    "\n",
    "derived_halstead_df = pd.DataFrame({\n",
    "    f\"Layer {layer}\": [metrics_df[f\"{metric}_{layer}\"].mean() for metric in ['vocabulary', 'length', 'volume']]\n",
    "    for layer in layers\n",
    "}, index=['Vocabulary', 'Length', 'Volume'])\n",
    "\n",
    "complexity_halstead_df = pd.DataFrame({\n",
    "    f\"Layer {layer}\": [metrics_df[f\"{metric}_{layer}\"].mean() for metric in ['difficulty', 'effort']]\n",
    "    for layer in layers\n",
    "}, index=['Difficulty', 'Effort'])\n",
    "\n",
    "# Generate LaTeX tables\n",
    "basic_halstead_latex = generate_latex_table(\n",
    "    basic_halstead_df, \"Basic Halstead Metrics Across Different Layers\", \"tab:basic_halstead\"\n",
    ")\n",
    "\n",
    "derived_halstead_latex = generate_latex_table(\n",
    "    derived_halstead_df, \"Derived Halstead Metrics Across Different Layers\", \"tab:derived_halstead\"\n",
    ")\n",
    "\n",
    "complexity_halstead_latex = generate_latex_table(\n",
    "    complexity_halstead_df, \"Complexity Halstead Metrics Across Different Layers\", \"tab:complexity_halstead\"\n",
    ")\n",
    "\n",
    "# Print the LaTeX tables\n",
    "print(basic_halstead_latex)\n",
    "print(derived_halstead_latex)\n",
    "print(complexity_halstead_latex)\n",
    "\n",
    "# Optional: Create a summary table with key metrics\n",
    "summary_df = pd.DataFrame({\n",
    "    f\"Layer {layer}\": [\n",
    "        metrics_df[f\"vocabulary_{layer}\"].mean(),\n",
    "        metrics_df[f\"volume_{layer}\"].mean(),\n",
    "        metrics_df[f\"difficulty_{layer}\"].mean(),\n",
    "        metrics_df[f\"effort_{layer}\"].mean()\n",
    "    ] for layer in layers\n",
    "}, index=['Vocabulary', 'Volume', 'Difficulty', 'Effort'])\n",
    "\n",
    "summary_latex = generate_latex_table(\n",
    "    summary_df, \"Summary of Key Halstead Metrics Across Different Layers\", \"tab:summary_halstead\"\n",
    ")\n",
    "\n",
    "print(summary_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1666b98-7c38-48f6-b444-5ce5475cb2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
