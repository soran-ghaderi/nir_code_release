{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa2af50060d9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import configs\n",
    "from controller.memory_manager import MemoryManager\n",
    "from data_processor.data_loader import GSM8KDataset\n",
    "from generator.crv_generator import CRVGenerator\n",
    "from generator.text_generator import TextGenerator\n",
    "\n",
    "from retrieve.cosine_similarity import CRVRetriever\n",
    "from retrieve.dnc import DNMemory\n",
    "from utils import set_seed, logger\n",
    "from utils.loading_model import CustomTransformerLoader\n",
    "\n",
    "# from rich import print\n",
    "from rich.console import Console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696654b1-279c-46ec-ac35-841112a788ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and console\n",
    "console = Console()\n",
    "logger = logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55dd0d2-6866-42bb-b9dc-02ab954746f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "model_urls = {\n",
    "    \"llama31\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}\n",
    "model_path = model_urls[\"llama31\"]\n",
    "tokenizer_path = model_path\n",
    "hf_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de6a1f3-a3fb-40a1-9d97-ce339294e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Loading the Model</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────────────────────────────────────── \u001b[0m\u001b[1;31mLoading the Model\u001b[0m\u001b[92m ────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, use_auth_token=hf_token)\n",
    "\n",
    "console.rule(\"[bold red]Loading the Model\")\n",
    "\n",
    "loader = CustomTransformerLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5913df29-187a-4fc9-b839-5beb932b7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/sg23454/PycharmProjects/moc/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3220: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">config:  LlamaConfig <span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"_name_or_path\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta-llama/Meta-Llama-3.1-8B-Instruct\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"architectures\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"LlamaForCausalLM\"</span>\n",
       "  <span style=\"font-weight: bold\">]</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"attention_bias\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"attention_dropout\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"bos_token_id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128000</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"eos_token_id\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128001</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128008</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128009</span>\n",
       "  <span style=\"font-weight: bold\">]</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_act\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"silu\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"hidden_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"initializer_range\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"intermediate_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14336</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"max_position_embeddings\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">131072</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"mlp_bias\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"model_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"llama\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_attention_heads\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_hidden_layers\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"num_key_value_heads\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pretraining_tp\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"rms_norm_eps\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"rope_scaling\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"factor\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"high_freq_factor\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"low_freq_factor\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"original_max_position_embeddings\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"rope_type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"llama3\"</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"rope_theta\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500000.0</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"tie_word_embeddings\"</span>: false,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"torch_dtype\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"bfloat16\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"transformers_version\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"4.44.2\"</span>,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"use_cache\"</span>: true,\n",
       "  <span style=\"color: #008000; text-decoration-color: #008000\">\"vocab_size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128256</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "config:  LlamaConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"_name_or_path\"\u001b[0m: \u001b[32m\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\u001b[0m,\n",
       "  \u001b[32m\"architectures\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"LlamaForCausalLM\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"attention_bias\"\u001b[0m: false,\n",
       "  \u001b[32m\"attention_dropout\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"bos_token_id\"\u001b[0m: \u001b[1;36m128000\u001b[0m,\n",
       "  \u001b[32m\"eos_token_id\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m128001\u001b[0m,\n",
       "    \u001b[1;36m128008\u001b[0m,\n",
       "    \u001b[1;36m128009\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"silu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m4096\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m14336\u001b[0m,\n",
       "  \u001b[32m\"max_position_embeddings\"\u001b[0m: \u001b[1;36m131072\u001b[0m,\n",
       "  \u001b[32m\"mlp_bias\"\u001b[0m: false,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"llama\"\u001b[0m,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
       "  \u001b[32m\"num_key_value_heads\"\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "  \u001b[32m\"pretraining_tp\"\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "  \u001b[32m\"rms_norm_eps\"\u001b[0m: \u001b[1;36m1e-05\u001b[0m,\n",
       "  \u001b[32m\"rope_scaling\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"factor\"\u001b[0m: \u001b[1;36m8.0\u001b[0m,\n",
       "    \u001b[32m\"high_freq_factor\"\u001b[0m: \u001b[1;36m4.0\u001b[0m,\n",
       "    \u001b[32m\"low_freq_factor\"\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m\"original_max_position_embeddings\"\u001b[0m: \u001b[1;36m8192\u001b[0m,\n",
       "    \u001b[32m\"rope_type\"\u001b[0m: \u001b[32m\"llama3\"\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[32m\"rope_theta\"\u001b[0m: \u001b[1;36m500000.0\u001b[0m,\n",
       "  \u001b[32m\"tie_word_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"torch_dtype\"\u001b[0m: \u001b[32m\"bfloat16\"\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.44.2\"\u001b[0m,\n",
       "  \u001b[32m\"use_cache\"\u001b[0m: true,\n",
       "  \u001b[32m\"vocab_size\"\u001b[0m: \u001b[1;36m128256\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97966e025e91493daadb8177cc806c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":warning: model type:  <class 'moc_layers.LlamaForCausalLM'>\n",
      "config.hidden_size:  32\n",
      "config._attn_implementation:  eager\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = loader.load_model(\n",
    "    model_path=model_path, tokenizer_path=tokenizer_path, hf_token=hf_token\n",
    ")\n",
    "\n",
    "crv_layers = configs.CRV_LAYERS\n",
    "\n",
    "print(\":warning: model type: \", type(model))\n",
    "print(\"config.hidden_size: \", config.num_hidden_layers)\n",
    "print(\"config._attn_implementation: \", config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb9f9be-2b80-4e3b-9622-d43168b1d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_context_expansion(text):\n",
    "    pattern = r'<context_generation>(.*?)</context_generation>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return f\"Context expansion section not found. The original text: {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53db3c42-387d-422f-9d55-0e1192f50b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cases len:  4\n",
      "['assert rearrange_bigger(12)==21', 'assert rearrange_bigger(10)==False', 'assert rearrange_bigger(102)==120']\n"
     ]
    }
   ],
   "source": [
    "def extract_test_cases(text):\n",
    "    # Pattern to match assert statements\n",
    "    pattern = r'assert\\s+[\\w_]+\\(.*?\\).*?(?=[\\n<]|$)'\n",
    "    \n",
    "    # Find all matches\n",
    "    test_cases = re.findall(pattern, text)\n",
    "    \n",
    "    # Group test cases by task\n",
    "    grouped_tests = []\n",
    "    current_group = []\n",
    "    \n",
    "    for test in test_cases:\n",
    "        if current_group and not test.startswith(current_group[-1].split('(')[0]):\n",
    "            grouped_tests.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(test)\n",
    "    \n",
    "    if current_group:\n",
    "        grouped_tests.append(current_group)\n",
    "        print(\"test cases len: \", len(grouped_tests))\n",
    "    \n",
    "    return grouped_tests\n",
    "\n",
    "text = '''<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n res = tuple(set(test_tup1) & set(test_tup2))\\n return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n result = False\\n for i in range(2,int(math.sqrt(n)) + 1):\\n if n % i == 0:\\n result = True\\n return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n largest_nums = hq.nlargest(n, nums)\\n return largest_nums\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to create the next bigger number by rearranging the digits of a given number.\\nYour code should pass the following tests:\\nassert rearrange_bigger(12)==21\\nassert rearrange_bigger(10)==False\\nassert rearrange_bigger(102)==120<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\"'''\n",
    "out = extract_test_cases(text)\n",
    "print(out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9224d56-3694-4e28-90e8-da7697eb1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_functions(text):\n",
    "    # Extract imports\n",
    "    import_pattern = r'^(?:from\\s+[\\w.]+\\s+import\\s+(?:[\\w.]+(?:\\s*,\\s*[\\w.]+)*|\\*)|import\\s+(?:[\\w.]+(?:\\s*,\\s*[\\w.]+)*))(?:\\s+as\\s+[\\w.]+)?'\n",
    "    imports = re.findall(import_pattern, text, re.MULTILINE)\n",
    "    \n",
    "    # Extract functions\n",
    "    function_pattern = r\"(def\\s+\\w+\\s*\\(.*?\\):(?:\\s*['\\\"][\\s\\S]*?['\\\"])?\\s*(?:(?!def\\s)[\\s\\S])*?(?=\\ndef|\\Z))\"\n",
    "    functions = re.findall(function_pattern, text, re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    def clean_code(code):\n",
    "        # Remove docstrings\n",
    "        code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\'', '', code)\n",
    "        # Remove comments\n",
    "        code = re.sub(r'#.*', '', code)\n",
    "        # Remove empty lines and trailing whitespace\n",
    "        code = '\\n'.join(line for line in code.splitlines() if line.strip())\n",
    "        return code\n",
    "    \n",
    "    cleaned_imports = [clean_code(imp) for imp in imports]\n",
    "    cleaned_functions = [clean_code(func) for func in functions]\n",
    "    \n",
    "    # Combine imports and functions\n",
    "    cleaned_code = '\\n'.join(cleaned_imports)\n",
    "    if cleaned_imports and cleaned_functions:\n",
    "        cleaned_code += '\\n\\n'\n",
    "    cleaned_code += '\\n\\n'.join(cleaned_functions)\n",
    "    \n",
    "    return cleaned_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10f01675-9f5c-4d04-8e47-9269d3474364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLLaMACRVFramework:\n",
    "    def __init__(self, model, tokenizer, layer_idx = 10):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_generator = TextGenerator(model, tokenizer)\n",
    "        self.crv_generator = CRVGenerator(model, tokenizer, max_length=configs.MAX_LENGTH)\n",
    "        self.memory_manager = MemoryManager(model, max_memories=5)\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "\n",
    "    def generate_thought_trajectories(self, input_query, test_cases=None, max_new_tokens=1000):\n",
    "        prompt_template = f\"\"\"\n",
    "        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        \n",
    "        \\n\\nYou are an expert Python programmer designed to provide standard, accurate,and fully working codes, and here is your task:\\n\n",
    "        \\nWrite a function to find the similar elements from the given two tuple lists.\\nYour code should pass the following tests:\\nassert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)\\nassert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)\\nassert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\ndef similar_elements(test_tup1, test_tup2):\\n res = tuple(set(test_tup1) & set(test_tup2))\\n return (res) \\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a python function to identify non-prime numbers.\\nYour code should pass the following tests:\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport math\\ndef is_not_prime(n):\\n result = False\\n for i in range(2,int(math.sqrt(n)) + 1):\\n if n % i == 0:\\n result = True\\n return result\\n```<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\nWrite a function to find the largest integers from a given list of numbers using heap queue algorithm.\\nYour code should pass the following tests:\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] \\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\\nimport heapq as hq\\ndef heap_queue_largest(nums,n):\\n largest_nums = hq.nlargest(n, nums)\\n return largest_nums\\n```<|eot_id|>\n",
    "        Enable code_interpreter tool.\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        \n",
    "        \\n\\nYou are an expert Python programmer, and here is your task:\\n{input_query}.\\nYour code must pass these test cases:{test_cases}\n",
    "        \n",
    "        \\n\\nYour outputs must follow this structure:\n",
    "\n",
    "        Identify the core components of this problem.\n",
    "        1. Identify potential edge cases and tricky parts.\n",
    "        2. Write 2 short test cases for the edge cases and tricky parts.\n",
    "        \n",
    "        <chain_of_thoughts>\n",
    "        1. you must consider the edge cases according to the problem statement.\n",
    "        2. Begin with a <thinking> section.\n",
    "        3. Inside the thinking section:\n",
    "           a. Write the topic name of the query, the name of the algorithm if necessary.\n",
    "           b. Draft an answer as an expert.\n",
    "           b. Briefly analyze the question and outline your approach.\n",
    "           c. Present a clear plan of steps to solve the problem.\n",
    "           d. Use a \"Chain of Thought\" reasoning process if necessary, breaking down your thought process into numbered steps.\n",
    "        4. Include a <reflection> section for each idea where you:\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        5. Be sure to close all reflection sections.\n",
    "        6. Close the thinking section with </thinking>.\n",
    "        7. Provide your final answer in an <output> section.        \n",
    "        </chain_of_thoughts>\n",
    "\n",
    "        <chain_of_thought_selection>\n",
    "        you must consider the edge cases according to the problem statement and select the most promising chain of thought that solves the edge cases (not necessarily the simplest nor the standard approach).\n",
    "        </chain_of_thought_selection>\n",
    "\n",
    "        <solution>\n",
    "        1. As a Python expert, generate the Python code and make sure it solves the edge cases while keeping it efficient.\n",
    "        2. the internal steps must produce the required output.\n",
    "        </solution>\n",
    "\n",
    "        Include a <reflection> section for the selected solution where if it is not correct, modify or if necessary, rewrite the solution and pay attention to the input problem.\n",
    "           a. Review your reasoning.\n",
    "           b. Check for potential errors or oversights according to the problem. you must consider the edge cases according to the problem. Make sure it is not overcomplicated.\n",
    "           c. Confirm or adjust your conclusion if necessary.\n",
    "        4. Be sure to close all reflection sections.\n",
    "        \n",
    "        <context_generation>\n",
    "        1. Rewrite the problem.\n",
    "        2. Rewrite the edge cases and tricky parts in one short sentence\n",
    "        2. Generate a very accurate and minimal Python code/pseudocode for the final solution. Ensure that the final solution is minimal and accurate.\n",
    "        </context_generation>\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"\"\"\n",
    "        # <|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\n",
    "        # <|start_header_id|>system<|end_header_id|>\n",
    "        \n",
    "        # <|eot_id|>\n",
    "        # ```python\n",
    "        # print(\"prompt: \", prompt)\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            prompt_template,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        return generated_text\n",
    "    \n",
    "    def extract_hidden_states(self, context):\n",
    "        best_crv, seq_length = self.crv_generator.generate_crvs(\n",
    "            context, crv_layers=crv_layers, max_length=configs.MAX_LENGTH\n",
    "        )\n",
    "        return best_crv, seq_length  # Return the hidden state and its len\n",
    "\n",
    "    def generate_crv(self, hidden_states, seq_length):\n",
    "        # return torch.mean(hidden_states, dim=1)\n",
    "        return hidden_states, seq_length\n",
    "        \n",
    "    def final_generation(self, original_query, test_cases, crv, seq_length, max_new_tokens=250):\n",
    "\n",
    "        query=f\"\"\"<|start_header_id|>user<|end_header_id|>\\n\\nYou are an expert Python programmer, and here is your task:\\n{original_query}.\\nYour code should pass the following tests:{test_cases}\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n```python\"\"\"\n",
    "        # Combine original query and CRV\n",
    "        self.memory_manager.add_memory(\n",
    "        crv, seq_length, layer_idx=self.layer_idx, crv_layers=crv_layers\n",
    "    )\n",
    "\n",
    "        # model.model.set_post_concat_crv(True)\n",
    "        self.memory_manager.set_concat_positions(0, start_pos=0, end_pos=seq_length)\n",
    "        self.memory_manager.apply_memory_to_model(0)\n",
    "        generated_text = self.text_generator.generate_text(\n",
    "            query,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences = 1,\n",
    "            output_file=\"data/results.csv\",\n",
    "            # stop_sequences=[\"The end\", \".\\n\\n\"],\n",
    "        )\n",
    "        # print(generated_text)\n",
    "        print('==' * 50)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba3ff791-3d11-4830-b45b-9fd96cea0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset: Dataset, layer_indices: List[int], num_examples: int = -1) -> pd.DataFrame:\n",
    "    results = []\n",
    "    \n",
    "    for layer_idx in tqdm(layer_indices, desc=\"Processing layer indices\"):\n",
    "        framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx=layer_idx)\n",
    "        \n",
    "        for i, instance in enumerate(tqdm(dataset, desc=f\"Processing instances for layer {layer_idx}\")):\n",
    "            if num_examples != -1 and i >= num_examples:\n",
    "                break\n",
    "            \n",
    "            query = instance['query'][0]\n",
    "            context = instance['context'][0]\n",
    "            test_cases = '\\n'.join(extract_test_cases(instance['input_final_prompts'][0])[-1])\n",
    "            \n",
    "            trajectories_and_context = framework.generate_thought_trajectories(query, test_cases, max_new_tokens=1000)\n",
    "            context_expansion = extract_context_expansion(trajectories_and_context)\n",
    "            \n",
    "            hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "            crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "            \n",
    "            final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "            extracted_functions = extract_functions(final_output)\n",
    "            \n",
    "            result = {\n",
    "                'layer_idx': layer_idx,\n",
    "                'instance_id': i,  # This should be the index in the dataset\n",
    "                'query': query,\n",
    "                'context': context,\n",
    "                'test_cases': test_cases,\n",
    "                'final_output': final_output,\n",
    "                'extracted_functions': extracted_functions\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a67654fb-468c-4b95-9c70-45dcd4846cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "def add_parsed_functions_to_dataset(dataset: Dataset, results_df: pd.DataFrame, layer_indices: List[int]) -> Dataset:\n",
    "    # Convert results DataFrame to a dictionary\n",
    "    results_dict = results_df.to_dict('records')\n",
    "    \n",
    "    # Create a dictionary to store new columns\n",
    "    new_columns = {\n",
    "        f'final_output_layer_{layer}': [None] * len(dataset) for layer in layer_indices\n",
    "    }\n",
    "    new_columns.update({\n",
    "        f'extracted_functions_layer_{layer}': [None] * len(dataset) for layer in layer_indices\n",
    "    })\n",
    "    \n",
    "    # Populate new columns\n",
    "    for result in results_dict:\n",
    "        instance_id = result['instance_id']\n",
    "        layer = result['layer_idx']\n",
    "        if 0 <= instance_id < len(dataset):\n",
    "            new_columns[f'final_output_layer_{layer}'][instance_id] = result['final_output']\n",
    "            new_columns[f'extracted_functions_layer_{layer}'][instance_id] = result['extracted_functions']\n",
    "    \n",
    "    # Create a new dataset with only the new columns\n",
    "    new_dataset = Dataset.from_dict(new_columns)\n",
    "    \n",
    "    # Combine the original dataset with the new dataset\n",
    "    updated_dataset = concatenate_datasets([dataset, new_dataset], axis=1)\n",
    "    \n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86716f98-f9ae-4a0d-bfe6-90aa7e6acebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing layer indices:   0%|                           | 0/1 [00:00<?, ?it/s]\n",
      "Processing instances for layer 15:   0%|                | 0/500 [00:00<?, ?it/s]\u001b[AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=1000) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test cases len:  4\n",
      " # This function checks if a given tuple is distinct or not.\n",
      "def check_distinct(tup):\n",
      "    return len(tup) == len(set(tup))\n",
      "\"\n",
      "\n",
      "    assert check_distinct((1, 4, 5, 6, 1, 4)) == False\n",
      "assert check_distinct((1, 4, 5, 6)) == True\n",
      "assert check_distinct((2, 3, 4, 5, 6)) == True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.utils:The input received is a query\n",
      "INFO:controller.memory_manager:Added new memory. Current number of memories: 1\n",
      "INFO:controller.memory_manager:Set concat positions for memory 0: start=0, end=tensor([111], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Set CRV with shape <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">111</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"font-weight: bold\">])</span> at layer <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Set CRV with shape \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m6\u001b[0m, \u001b[1;36m111\u001b[0m, \u001b[1;36m4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m at layer \u001b[1;36m15\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CRV layers: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "CRV layers: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m15\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Concat positions: <span style=\"color: #808000; text-decoration-color: #808000\">start</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">end</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">111</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Concat positions: \u001b[33mstart\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mend\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:controller.memory_manager:Applied memory 0 to model\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Both `max_new_tokens` (=250) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">shape of the new layer_crv: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">111</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "shape of the new layer_crv: \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m111\u001b[0m, \u001b[1;36m4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">cat al layers <span style=\"font-weight: bold\">(</span>saved layers idx, and model idx<span style=\"font-weight: bold\">)</span>:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "cat al layers \u001b[1m(\u001b[0msaved layers idx, and model idx\u001b[1m)\u001b[0m:  \u001b[1;36m3\u001b[0m \u001b[1;36m15\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">hidden states before cat at layer <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">210</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "hidden states before cat at layer \u001b[1;36m15\u001b[0m: \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m210\u001b[0m, \u001b[1;36m4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">hidden states after cat: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">321</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "hidden states after cat: \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m321\u001b[0m, \u001b[1;36m4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def check_distinct(tup):\n",
      "    \"\"\"\n",
      "    This function checks if a given tuple is distinct or not.\n",
      "    \n",
      "    Args:\n",
      "        tup (tuple): The input tuple to be checked.\n",
      "    \n",
      "    Returns:\n",
      "        bool: True if the tuple is distinct, False otherwise.\n",
      "    \"\"\"\n",
      "    # Convert the tuple to a set. A set in Python is an unordered collection of unique elements.\n",
      "    # If the set has the same number of elements as the original tuple, it means all elements were unique.\n",
      "    return len(tup) == len(set(tup))\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing instances for layer 15:   0%|      | 1/500 [00:16<2:13:11, 16.01s/it]\u001b[A\n",
      "Processing layer indices: 100%|███████████████████| 1/1 [00:16<00:00, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Type of updated_dataset: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Number of rows in updated_dataset: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc09a0fb991c4322b0ac553eceb99628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_dataset = load_from_disk(\"data/processed_meta_llama_dataset\")\n",
    "\n",
    "# Define layer indices to evaluate\n",
    "layer_indices = [15]\n",
    "num_examples=1\n",
    "# Evaluate model\n",
    "results_df = evaluate_model(model, tokenizer, loaded_dataset, layer_indices, num_examples=num_examples)\n",
    "\n",
    "# Add parsed functions to the dataset\n",
    "updated_dataset = add_parsed_functions_to_dataset(loaded_dataset, results_df, layer_indices)\n",
    "\n",
    "print(f\"Type of updated_dataset: {type(updated_dataset)}\")\n",
    "print(f\"Number of rows in updated_dataset: {len(updated_dataset)}\")\n",
    "\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_dataset.save_to_disk(\"data/processed_meta_llama_dataset_with_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc304549-af46-4f55-8b9f-20b6f63edf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in the updated dataset:\n",
      "['task_type', 'task_name', 'subtask_name', 'input_question', 'input_choice_list', 'input_final_prompts', 'input_correct_responses', 'output_prediction_text', 'output_parsed_answer', 'output_choice_completions', 'output_choice_negative_log_likelihoods', 'output_metrics', 'is_correct', 'input_question_hash', 'input_final_prompts_hash', 'benchmark_label', 'eval_config', 'context', 'query', 'final_output_layer_15', 'extracted_functions_layer_15']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since meta-llama/Meta-Llama-3.1-8B-Instruct-evals couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since meta-llama/Meta-Llama-3.1-8B-Instruct-evals couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'Meta-Llama-3.1-8B-Instruct-evals__mbpp__details' at /home/sg23454/.cache/huggingface/datasets/meta-llama___meta-llama-3.1-8_b-instruct-evals/Meta-Llama-3.1-8B-Instruct-evals__mbpp__details/0.0.0/0f783b11d6240fc4f669dd95a842173b036e6799 (last modified on Sat Sep  7 05:25:57 2024).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'Meta-Llama-3.1-8B-Instruct-evals__mbpp__details' at /home/sg23454/.cache/huggingface/datasets/meta-llama___meta-llama-3.1-8_b-instruct-evals/Meta-Llama-3.1-8B-Instruct-evals__mbpp__details/0.0.0/0f783b11d6240fc4f669dd95a842173b036e6799 (last modified on Sat Sep  7 05:25:57 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the updated dataset (selected columns):\n",
      "\n",
      "Example 1:\n",
      "input_correct_responses: None...\n",
      "\n",
      "Layer 15:\n",
      "Final Output: None\n",
      "Extracted Functions: None\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def main():    \n",
    "    # Print column names\n",
    "    print(\"\\nColumns in the updated dataset:\")\n",
    "    print(updated_dataset.column_names)\n",
    "    # loaded_dataset2 = load_from_disk(\"data/processed_meta_llama_dataset_with_results\")\n",
    "    loaded_dataset2 = load_dataset(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct-evals\",\n",
    "    name=\"Meta-Llama-3.1-8B-Instruct-evals__mbpp__details\",\n",
    "    split=\"latest\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Print selected columns from the first 5 rows of the updated dataset\n",
    "    print(\"First 5 rows of the updated dataset (selected columns):\")\n",
    "    for i, example in enumerate(loaded_dataset2.select(range(num_examples))):\n",
    "        print(f\"\\nExample {i + 1}:\")\n",
    "        # Print original columns\n",
    "        print(f\"input_correct_responses: {example['input_correct_responses']}...\")\n",
    "        # print(f\"extracted_functions_layer_15: {example['extracted_functions_layer_15']}...\")\n",
    "        \n",
    "        # Print new columns for each layer\n",
    "        for layer in layer_indices:\n",
    "            print(f\"\\nLayer {layer}:\")\n",
    "            final_output = example.get(f'final_output_layer_{layer}')\n",
    "            extracted_functions = example.get(f'extracted_functions_layer_{layer}')\n",
    "            \n",
    "            if final_output:\n",
    "                print(f\"Final Output: {final_output[:100]}...\")\n",
    "            else:\n",
    "                print(\"Final Output: None\")\n",
    "            \n",
    "            if extracted_functions:\n",
    "                print(f\"Extracted Functions: {extracted_functions[:100]}...\")\n",
    "            else:\n",
    "                print(\"Extracted Functions: None\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ac4e0dd-259c-4b09-a1ab-36b7cd527417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def load_from_disk(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(path)  # Adjust if your data is in a different format\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset: pd.DataFrame, layer_indices: List[int], num_examples: int = -1) -> pd.DataFrame:\n",
    "    results = []\n",
    "    \n",
    "    for layer_idx in tqdm(layer_indices, desc=\"Processing layer indices\"):\n",
    "        framework = AdvancedLLaMACRVFramework(model, tokenizer, layer_idx=layer_idx)\n",
    "        \n",
    "        for i, row in dataset.iterrows():\n",
    "            if num_examples != -1 and i >= num_examples:\n",
    "                break\n",
    "            \n",
    "            query = row['query']\n",
    "            context = row['context']\n",
    "            test_cases = '\\n'.join(extract_test_cases(row['input_final_prompts'])[-1])\n",
    "            \n",
    "            trajectories_and_context = framework.generate_thought_trajectories(query, test_cases, max_new_tokens=1000)\n",
    "            context_expansion = extract_context_expansion(trajectories_and_context)\n",
    "            \n",
    "            hidden_states, seq_len = framework.extract_hidden_states(context_expansion)\n",
    "            crv, seq_len = framework.generate_crv(hidden_states, seq_len)\n",
    "            \n",
    "            final_output = framework.final_generation(query, test_cases, crv, seq_len, max_new_tokens=250)\n",
    "            extracted_functions = extract_functions(final_output)\n",
    "            \n",
    "            results.append({\n",
    "                'layer_idx': layer_idx,\n",
    "                'instance_id': i,\n",
    "                'final_output': final_output,\n",
    "                'extracted_functions': extracted_functions\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def add_parsed_functions_to_dataset(dataset: pd.DataFrame, results_df: pd.DataFrame, layer_indices: List[int]) -> pd.DataFrame:\n",
    "    for layer in layer_indices:\n",
    "        dataset[f'final_output_layer_{layer}'] = None\n",
    "        dataset[f'extracted_functions_layer_{layer}'] = None\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        instance_id = row['instance_id']\n",
    "        layer = row['layer_idx']\n",
    "        dataset.loc[instance_id, f'final_output_layer_{layer}'] = row['final_output']\n",
    "        dataset.loc[instance_id, f'extracted_functions_layer_{layer}'] = row['extracted_functions']\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aeee61e4-cbcb-41f8-968b-84d05ba94d41",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Functions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted_functions[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_functions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Functions: None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Evaluate model on a subset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m subset_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m(num_examples)\n\u001b[1;32m     11\u001b[0m results_df \u001b[38;5;241m=\u001b[39m evaluate_model(model, tokenizer, subset_dataset, layer_indices, num_examples\u001b[38;5;241m=\u001b[39mnum_examples)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Add parsed functions to the dataset\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load dataset\n",
    "    loaded_dataset = load_from_disk(\"data/processed_meta_llama_dataset\")\n",
    "    \n",
    "    # Define layer indices to evaluate\n",
    "    layer_indices = [15]\n",
    "    \n",
    "    # Evaluate model on a subset\n",
    "    num_examples = 2  # Adjust as needed\n",
    "    subset_dataset = loaded_dataset.head(num_examples)\n",
    "    results_df = evaluate_model(model, tokenizer, subset_dataset, layer_indices, num_examples=num_examples)\n",
    "    \n",
    "    # Add parsed functions to the dataset\n",
    "    updated_dataset = add_parsed_functions_to_dataset(subset_dataset, results_df, layer_indices)\n",
    "    \n",
    "    print(f\"Type of updated_dataset: {type(updated_dataset)}\")\n",
    "    print(f\"Number of rows in updated_dataset: {len(updated_dataset)}\")\n",
    "    \n",
    "    # Save the updated dataset\n",
    "    updated_dataset.to_csv(\"data/processed_meta_llama_dataset_with_results.csv\", index=False)\n",
    "    \n",
    "    # Print column names\n",
    "    print(\"\\nColumns in the updated dataset:\")\n",
    "    print(updated_dataset.columns.tolist())\n",
    "    \n",
    "    # Print selected columns from the first 5 rows of the updated dataset\n",
    "    print(\"First 5 rows of the updated dataset (selected columns):\")\n",
    "    for i, row in updated_dataset.iterrows():\n",
    "        print(f\"\\nExample {i + 1}:\")\n",
    "        print(f\"input_correct_responses: {row['input_correct_responses']}\")\n",
    "        print(f\"extracted_functions_layer_15: {row['extracted_functions_layer_15']}\")\n",
    "        \n",
    "        for layer in layer_indices:\n",
    "            print(f\"\\nLayer {layer}:\")\n",
    "            final_output = row[f'final_output_layer_{layer}']\n",
    "            extracted_functions = row[f'extracted_functions_layer_{layer}']\n",
    "            \n",
    "            print(f\"Final Output: {final_output[:100]}...\" if final_output else \"Final Output: None\")\n",
    "            print(f\"Extracted Functions: {extracted_functions[:100]}...\" if extracted_functions else \"Extracted Functions: None\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e384e-9f0d-46a9-b3da-17e344ff6202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
