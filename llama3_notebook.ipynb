{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f1356e7-4177-4eaa-9b49-41629e28fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "token_s = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1640eb03-e64e-4bb1-b82e-c3d52002bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/.conda/envs/myenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abbc5134a78429cad1fc5bb9efccabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sg23454/.conda/envs/myenv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/sg23454/.conda/envs/myenv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:919: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/sg23454/.conda/envs/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token_s)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token_s)\n",
    "config = AutoConfig.from_pretrained(model_name, use_auth_token=token_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c84b5a-c3fd-4be6-bb2c-9d0792408134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: [15339, 1917, 0]\n",
      "Decoded output: hello world!\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer.encode(\"hello world!\", add_special_tokens=False)\n",
    "decoded_output = tokenizer.decode(encoded_input)\n",
    "print(f\"Encoded input: {encoded_input}\")\n",
    "print(f\"Decoded output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27a6e91-8c49-4802-a415-33b9e40b0a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict:  [\n",
      "    \"model.embed_tokens.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.weight\",\n",
      "    \"model.layers.0.self_attn.k_proj.weight\",\n",
      "    \"model.layers.0.self_attn.v_proj.weight\",\n",
      "    \"model.layers.0.self_attn.o_proj.weight\",\n",
      "    \"model.layers.0.mlp.gate_proj.weight\",\n",
      "    \"model.layers.0.mlp.up_proj.weight\",\n",
      "    \"model.layers.0.mlp.down_proj.weight\",\n",
      "    \"model.layers.0.input_layernorm.weight\",\n",
      "    \"model.layers.0.post_attention_layernorm.weight\",\n",
      "    \"model.layers.1.self_attn.q_proj.weight\",\n",
      "    \"model.layers.1.self_attn.k_proj.weight\",\n",
      "    \"model.layers.1.self_attn.v_proj.weight\",\n",
      "    \"model.layers.1.self_attn.o_proj.weight\",\n",
      "    \"model.layers.1.mlp.gate_proj.weight\",\n",
      "    \"model.layers.1.mlp.up_proj.weight\",\n",
      "    \"model.layers.1.mlp.down_proj.weight\",\n",
      "    \"model.layers.1.input_layernorm.weight\",\n",
      "    \"model.layers.1.post_attention_layernorm.weight\",\n",
      "    \"model.layers.2.self_attn.q_proj.weight\"\n",
      "]\n",
      "config:  LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "\n",
    "# Print the first 20 layer names\n",
    "print(\"dict: \", json.dumps(list(state_dict.keys())[:20], indent=4))\n",
    "print(\"config: \", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c8eac98-de4e-4352-b2af-40cc141aea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.configuration_llama.LlamaConfig'>\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(type(config))\n",
    "print(config.num_attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf16877-784e-41d8-acdb-e0a03dd27ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0537772-5c36-4ee5-ac52-2c4d62f466c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>the answer to the ultimate question of life, the universe, and everything is \n"
     ]
    }
   ],
   "source": [
    "tokens = torch.tensor(tokens)\n",
    "prompt_split_as_tokens = tokenizer.decode([token.item() for token in tokens])\n",
    "print(prompt_split_as_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20ed2b37-b550-44ae-930f-cf57955b600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 4096])\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = torch.nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "embedding_layer.weight.data.copy_(model.state_dict()[\"model.embed_tokens.weight\"])\n",
    "token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)\n",
    "print(token_embedding_unnormalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "247f0549-c776-43b0-9a86-451566560e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install fairscale\n",
    "from main import RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b5b27ba-b146-4ebc-96ea-8dc9951e632a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 4096])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = RMSNorm(dim=config.hidden_size)(token_embeddings_unnormalized)\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cab9731b-311c-4be2-86f6-6f391b15984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(state_dict[\"model.layers.0.self_attn.q_proj.weight\"].shape,\n",
    "     state_dict[\"model.layers.0.self_attn.k_proj.weight\"].shape,\n",
    "     state_dict[\"model.layers.0.self_attn.v_proj.weight\"].shape,\n",
    "     state_dict[\"model.layers.0.self_attn.o_proj.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b61c9d0-3c00-4742-acdb-1e4209338f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 4096])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_layer0 = state_dict[\"model.layers.0.self_attn.q_proj.weight\"]\n",
    "head_dim = q_layer0.shape[0] // config.num_attention_heads\n",
    "q_layer0 = q_layer0.view(config.num_attention_heads, head_dim, config.hidden_size)\n",
    "q_layer0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03b88e9c-8ea2-4ea8-bdf0-94b53588d7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4096])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_layer0_head0 = q_layer0[0]\n",
    "q_layer0_head0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55440977-9352-4598-b8be-a9e9c2233bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 128])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)\n",
    "q_per_token.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67df9e-555e-4a9c-b8c4-acee15044fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
